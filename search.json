[{"title":"Bake loading performance into authoring time - reading note of a Google block","url":"/2023/01/01/Bake-loading-performance-into-authoring-time-reading-note-of-a-Google-block/","content":"\nSee [here](https://developer.chrome.com/blog/conformance/) for original blog\n\nKey takeaways\nSome very actionable optimization for loading performance:\n\n- Inlining critical CSS\n- Tag important images to be loaded early\n- Avoid fonts or synchronous scripts that delay early rendering\n- Reduce round trips required for loading\n\nMany error-checking and auditing tools that developers rely on (Lighthouse, Chrome DevTools Issues tab) are passive and require some form of user interaction to retrieve information. Developers are more likely to act **when errors are surfaced directly within their existing tooling**, and when they provide **concrete and specific actions** that should be taken to fix the problem.\n\nConformance rules:\n\n- Strong default\n- Actionable rules\n- Authoring time\n","tags":["Web Development","Performance"]},{"title":"Store UUID in space economical way - BINARY16","url":"/2022/11/25/Store-UUID-in-space-economical-way-BINARY16/","content":"\nThe easy way would be to store in raw string (CHAR(36)). If we have a bigger eye for performance, we could store it in byte level with hex. This means we want to understand how many sections are there in the UUID (could be company specific), and break them down.\n\n```\n-- +goose Up\n\n-- +goose StatementBegin\nCREATE FUNCTION bin_from_uuid(uuid CHAR(36))\nRETURNS BINARY(16) DETERMINISTIC\nBEGIN\n  RETURN UNHEX(CONCAT(MID(uuid, 25, 12), MID(uuid, 20, 4), MID(uuid, 15, 4), MID(uuid, 10, 4), MID(uuid, 1, 8)));\nEND\n-- +goose StatementEnd\n\n-- +goose StatementBegin\nCREATE FUNCTION uuid_from_bin(b BINARY(16))\nRETURNS CHAR(36) DETERMINISTIC\nBEGIN\n  DECLARE hex CHAR(32);\n  SET hex = HEX(b);\n  RETURN LOWER(CONCAT(MID(hex, 25, 8), '-', MID(hex, 21,4), '-', MID(hex, 17,4), '-', MID(hex, 13,4), '-', MID(hex, 1, 12)));\nEND\n-- +goose StatementEnd\n```\n","tags":["Database","MySQL"]},{"title":"How Go helps us to keep track of payments at Lightspeed","url":"/2022/10/22/How-Go-helps-us-to-keep-track-of-payments-at-Lightspeed/","content":"\nWhen we wait for a card payment from the customer, it might take quite some time. Longer than one API request could wait for.\n\nTo resolve this issue, the mechanism is:\n\n- The frontend sends the API request\n- When the backend receives the API request, write an empty database record and respond that the request has been received to frontend\n- The backend keeps going with the execution until the transaction has finished, and update the database record\n- In the meantime, the frontend polls for database update with an interval, and gives feedback to user when it detects the db status has been updated\n\n<https://docs.google.com/presentation/d/11JPSXHOiVddjghBWkNPR1uMPAWREigbehoU71AsiWP4/edit#slide=id.g1733f2c0e33_0_48>\n","tags":["Go","channels","async"]},{"title":"Summary of linting rules for testing library introduced","url":"/2022/10/16/Summary-of-linting-rules-for-testing-library-introduced/","content":"\nThere are 27 linting rules available in eslint-plugin-testing-library, here's a summary of the linting rules picked for our web register\n\n# To reduce flaky tests (that passes when they should)\n\n1. Disallow the use of the global RegExp flag (/g) in queries (`testing-library/no-global-regexp-flag-in-query`)\n\n# To create effective tests (that fails when they should)\n\n1. Enforce promises from async queries to be handled (`testing-library/await-async-query`)\n2. Enforce promises from async utils to be handled (`testing-library/await-async-utils`)\n\n# To create more helpful error messages\n\n# To best mimic user interaction\n\n1. Disallow `container` and its too-powerful methods like `querySelector` (`testing-library/no-container`)\n\n# To simplify tests\n\n1. Prefer `findBy` over `waitFor` + `getBy`\n\n# To use modern approach\n\n1. Use `waitFor` instead of `wait` -> deprecation\n2. use `view`, `utils`, or destructing for `render` results, instead of `wrapper`\n","tags":["Web Development","React","Test","Async"]},{"title":"Fix flaky React unit test - 2","url":"/2022/10/05/Fix-flaky-React-unit-test-2/","content":"\nI wrote about fixing flaky test in June, here's more discoveries over the last few months.\nThis time it's mostly async-related.\n\n## Mock implementation of a hook that takes a callback, and resolves the callback asynchronously\n\n### My original implementation\n\n```js\nconst mockUseTransaction = jest.fn()\n\njest.mock('./hooks/use-transaction/useTransaction', () => ({\n  useTransaction: (_: any, onComplete: (transaction: any) => void) =>\n    mockUseTransaction(onComplete),\n}))\n\ntest('reverses payment if user declines the signature', async () => {\n     mockUseTransaction.mockImplementationOnce(\n       async (onComplete: (transaction: any) => void) => {\n       // mock the behavior of query loading\n       await wait(10)\n       onComplete(APPROVED_WITH_SIGNATURE_TRANSACTION)\n       }\n     )\n```\n\nIf I do not `await wait(10)`, I will get a warning:\n\n```\nWarning: Cannot update a component (`SwiperPaymentModal`) while rendering a different component (`ProcessingTransaction`). To locate the bad setState() call inside `ProcessingTransaction`, follow the stack trace as described in https://reactjs.org/link/setstate-in-render\n```\n\nThis is because when component is first rendered, `SwiperPaymentModal` starts from processing status, and tries to render `ProcessingTransaction`; When `onComplete` callback is invoked synchronously, the parent (`SwiperPaymentModal`) will change state via `useReducer`.\n\nAdding a `wait(10)` is not exactly \"flaky\" as it should always pass, but the waiting time is not elegant.\n\n### Updated implementation\n\n```js\nmockUseTransaction.mockImplementationOnce(\n  async (onComplete: (transaction: any) => void) => {\n    onComplete(await Promise.resolve(APPROVED_TRANSACTION))\n  }\n)\n```\n\nI don't really need 10ms, all I need is that `onComplete` is not invoked synchronously. Here we go.\n","tags":["Web Development","React","Test","Async"]},{"title":"Polling gotchas using Apollo GraphQL client 😂","url":"/2022/09/14/Polling-using-GraphQL/","content":"\n## Auto with `pollingInterval` variable\n\nThe `useQuery` hook returns `startPolling` and `stopPolling` functions that can be used to control when to start polling.\n\nChallenge\n\n1. `useQuery` will immediately send a first query, before you start polling\n2. When specifying `skip` in the options, `startPolling` stops working in current version 😂\n3. `useLazyQuery` does not work well with polling either for current version\n\n## Manual\n\nIt's not too hard to implement manual polling leveraging `useLazyQuery` feature.\n\n```js\nconst [getXXX] =\n  useLazyQuery <XXXData>\n  (XXXQuery,\n  {\n    variables: {\n      XXX variables\n    },\n    fetchPolicy: 'network-only',\n  })\n\nconst startPolling = () => {\n  let attemptCounter = 0\n\n  const pollXXX = async () => {\n    attemptCounter = attemptCounter + 1\n    try {\n      const { data } = await geXXX()\n      if (gotFinishedXXX) {\n        onComplete(data.XXX)\n        return\n      }\n      await wait(getPollingTimeout(attemptCounter))\n      pollTransaction()\n    } catch {\n      // Handle error\n      return\n    }\n  }\n  pollXXX()\n}\n\nreturn startPolling\n```\n","tags":["GraphQL"]},{"title":"Decouple complex work like a Pro","url":"/2022/09/12/Decouple-complex-work-like-a-Pro/","content":"\nDecoupling complex work is always not easy, and sometimes I get too into the implementation and forget to take a step back to see how my PRs look like to reviewers. Noting down some feedback & tips I got from my awesome team.\n\nFirst conversation with Nathan on this topic:\n\n- Incremental approach is much more approachable for both author and reviewers\n- 264 lines is a good first PR to kick off the feedback & improvement cycle compared with 878 lines.\n- Smaller PRs will save us lots of back-and-forth over the feedback & improvement cycle.\n\nFirst conversation with Ema on this topic:\n\n- Take some steps back at the beginning to plan out what subtasks are out there\n- Include in the first PR what's the plan, so that reviewers have the full road map in mind\n- I started from easiest to hardest 😂 In most cases it's better to start from hardest\n","tags":["Maintainable Software","Team like a pro"]},{"title":"Migrate a payment flow modal from Angular to React","url":"/2022/09/08/Migrate-a-payment-flow-modal-from-Angular-to-React/","content":"\nI've been quite excited to do some Angular to React migration and finally! Possibly the hardest frontend ticket I've done, mostly because Angularjs was wired. Luckily Nathan had done some great examples to get ideas from.\n\n## How the existing Angular code works\n\nSo there's a controller, there's an html template, and there's a modal service that renders the modal via:\n\n```js\nreturn $q((resolve, reject) => {\n  this.Modal <\n    ApiTransaction >\n    {\n      controller: {the imported controller},\n       controllerAs: '$ctrl',\n      template: {the imported html template},\n      amount,\n      // some more arguments\n    }\n      .then((modal) => modal.closed)\n      .then((transaction) => {\n        if (transaction) {\n          resolve(transaction)\n        } else {\n          reject()\n        }\n      })\n})\n```\n\nIt took me some time to understand what the `Modal` is doing here. So it is a factory:\n\n> Factory is an angular function which is used to return the values. A value on demand is created by the factory, whenever a service or controller needs it. Once the value is created, it is reused for all services and controllers.\n\nIt is a reusable factory that creates a generic Modal that resolves to a value (typed `ApiTransaction`) which is the resolved value of `modal.closed`.\n\n## Make a \"bridge\" between the React component and the Angular site\n\n- [reactular](https://github.com/vend/reactular) is a package made by Vend to allow you to use React component in Angular. This solves the meat of the problem.\n- Then I need a new controller that instead of taking care of the full flow, just proxies the properties from the service to the Angularified React component (which is now a template too)\n\n## Move the flow's state management from Angular controller to a useEffectReducer hook\n\nThere are three main steps:\n\n- Create a transaction via an API call\n- Poll for the transaction until the transaction has reached a \"finished\" state e.g. Cancelled, declined, accepted, signature required\n- Reflect the transaction state to user and get follow-up actions if required e.g. validate signature\n\nBefore, it is done by a promise chain inside the controller, and the view change is done by a state owned by the controller, `this.workflow`.\nNow to achieve similar \"event-driven\" state management, I use a [useEffectReducer](https://www.npmjs.com/package/use-effect-reducer) hook:\n\n- The whole flow starts with an initial state and an initial effect\n- When the effect finishes (promise resolves), it dispatches an event\n- The reducer handles the event and 1) exec the next effect and 2) update the state -> update the UI\n\n## Angular services\n\n- `$q`: I'll understand it as similar to ES6 Promise for now, so that `$q((resolve, reject) => { // do things then resolve() // something go run then reject})`will map to`new Promise<ApiTransaction>((resolve, reject) => {// do things then resolve() // something go run then reject})`. This looks like a reasonable mapping for now, will come back if it turns out not to work.\n- `$timeout`: a wrapper for `window.setTimeout`, will replace `$timeout` with `setTimeout` and replace `$timeout.cancel` with `clearTimeout`\n","tags":["Web Development","Angular"]},{"title":"React re-rendering, when and how?","url":"/2022/08/30/React-re-rendering-when-and-how/","content":"\n## Background\n\nI want an interesting number input functionality:\n\n<image src=\"./input.gif\" width=\"600px\"/>\n\nI have multiple goals:\n\n1. The $ and % mode have different validation rules on the number value (e.g. maximum 999999.99 vs 100) and should be re-validated when either input or input type updated.\n2. For same invalid scenarios, $ and % have different error messages (e.g. `Please enter an amount` vs `Please enter a percentage`) and the error message should update accordingly when switching between $ and %.\n3. The `Save changes` button should be enabled/disabled based on whether the value is valid.\n4. The value updates should trigger state update in the parent.\n5. The $ mode rounds number to 2 fixed decimal places, and % mode rounds to integer, when switching between $ and % the rounding should be updated.\n\n## Challenge 1: stale error message\n\nIf I implement the input of two modes in one go:\n\n```js\n<NumberInput\nerrorMessages={inputType === InputType.Amount? AMOUNT_ERROR_MESSAGES : PERCENTAGE_ERROR_MESSAGES}\niconPosition={inputType === InputType.Amount? 'left' : 'right'}\n{...other props}\n>\n```\n\nThe error message will be stale like this when I switch between:\n\n<image src=\"./error.gif\" width=\"300px\"/>\n\nThis is interesting, from the position of the icon the `NumberInput` has been notified to re-render, but it did not re-render the error message 🤔\n\n### The quick solution\n\nIf I change to a switch case statement, the whole thing will be forced to re-render, and the problem is solved:\n\n```js\nswitch (inputType) {\n    case TippingOptionType.Amount: {\n      return (<NumberInput\n          errorMessages={ AMOUNT_ERROR_MESSAGES}\n          iconPosition={'left'}\n          {...other props}\n          >)\n    }\n    case TippingOptionType.Amount: {\n      return (<NumberInput\n          errorMessages={PERCENTAGE_ERROR_MESSAGES}\n          iconPosition={'right'}\n          {...other props}\n          >)\n    }\n    default: {\n      const _exhaustiveCheck: never = inputType\n      return _exhaustiveCheck\n    }\n}\n```\n\n### But why? Why did I get a \"Partial\" re-render?\n\nLooking into the implementation of `NumberInput`, the `errorMessage` state's setter is invoked when:\n\n- The `value` props passed to `NumberInput` has changed\n- The user has changed the input box's value or blurred the input\n- If `showInitialError` is true, when the element first renders\n\nNone of the three were triggered, this tells us:\n\n> A props update does not cause React to re-render the element.\n> It just triggers the `useEffect` hook that subscribes to this props change,\n> and causes React to update the DOM that is reflected in the returned element.\n\nBecause:\n\n- there is no effect that is triggered by `errorMessages` and updates `errorMessage` state\n- `<ErrorMessage>` part of the returned DOM is not changed because `errorMessage` state did not change\n\nReact does not think it needs to re-render the error message part, so it stays stale.\n\n### No effect that is triggered by `errorMessages` and updates `errorMessage` state??\n\nNow the part that seems off is that `errorMessages` props update does not trigger `errorMessage` state update\n\nThe dependency chain looks like this:\n\n- `useCallback` `validateInput` depends on `errorMessages`\n- `useCallback` `setValue` depends on the memorized callback `validateInput`\n- There is a `useEffect` that calls `validateInput` if `showInitialError` is true and `errorMessages` update, but when the value is `undefined` we want `showInitialError` to be `false` because we have an empty form.\n\n## Challenge 2: the need for boiler plate code to convert between % <-> $\n\nThis is the major reason I am very keen to replace the library component with a custom one. Any time user switches between %/$, there has to be a conversion like this \\* 3 on errors:\n\n```js\nconst convertPercentageErrorToAmountError = (\n  error: boolean | undefined,\n  value: number | string | undefined\n) => {\n  try {\n    // if the number value now falls in between amount valid range, clear the error\n    if (\n      vn(value).toNumber() <= AMOUNT_MAX &&\n      vn(value).toNumber() > PERCENTAGE_MAX\n    ) {\n      return false\n    }\n    return error\n  } catch {\n    return true\n  }\n}\n```\n\nAnd a conversion like this \\* 3 on values:\n\n```js\n// if the value is a number, round to 0 decimals for percentage\nconst convertAmountToPercentage = (value: number | string | undefined) => {\n  try {\n    return value && vn(value).toFixed(0)\n  } catch {\n    return value\n  }\n}\n```\n\nMy initial plan:\n\n> If when the tipping option changes, the number input gets re-rendered, then with `showInitialError={true}` (it will be true because the value will not be `undefined`), the values will be automatically validated, so the error will be automatically updated.\n\nHowever after digging into this plan, it looks that a custom component cannot really simplify the code to achieve our UI/UX need. Here's why:\n\nAs discussed in challenge 1, I need a useEffect that is explicitly triggered by the change. This will lead to an infinite loop because when the values are re-rounded and re-validated, the values are changed, then triggers the use effect again.\n","tags":["Web Development","React"]},{"title":"Apollo GraphQL adventures - cache and data type","url":"/2022/08/26/Apollo-GraphQL-adventures-cache-and-data-type/","content":"\nIn the last cycle I spent quite some time using Apollo client to interact with our GraphQL service from React app. Here are some gotchas and nice features I learnt.\n\n## Typing system in GraphQL\n\nI first got to know how custom [scalar types](https://graphql.org/learn/schema/#scalar-types) in GraphQL work when pairing with Nathan on submitting data to graphQL.\nThis discovery started when we were trying to submit data to GraphQL that has `Int64` type, and we keep getting `expected an Int64` while the data we had was of TypeScript `number` type. Turned out that `Int64` was a **custom scala type** that our GraphQL microservice defined.\n\nThen we discovered that there are only a limited set of default scalar types in GraphQL:\n\n- Int\n- Float\n- String\n- Boolean\n- ID (The ID type is serialized in the same way as a String; however, defining it as an ID signifies that it is not intended to be human‐readable.)\n\nAnd `Int64` is our custom scalar, and its serialization/deserialization rule is defined inside our GraphQL microservice.\nInside `schema.graphql`, these custom scala are defined by mapping to `model` files:\n\n```\n\"\"\"\nInt64 is a 64-bit, signed integer.\nGraphQL Int is 32-bit, so isn't suitable for some cases.\n\"\"\"\nscalar Int64 @goModel(model: \"github.com/vend/graphter/model.Int64\")\n```\n\nI just had a go with what the type really is when I submit a `number`:\n\n```go\nfunc UnmarshalInt64(v interface{}) (Int64, error) {\n\tswitch v := v.(type) {\n\tcase string:\n\t\ti, err := strconv.ParseInt(v, 10, 64)\n\t\tif err == nil {\n\t\t\treturn i, nil\n\t\t}\n\tcase Int64:\n\t\treturn v, nil\n\tcase int:\n\t\treturn Int64(v), nil\n\t}\n\tfmt.Printf(\"%v\", v)\n\tfmt.Printf(\"%v\", reflect.TypeOf(v))\n\treturn 0, Error(\"expected an Int64\")\n}\n```\n\nThe console output shows that the input `v` is of type `json.Number` 🤔 And we did not define how this type unmarshals to `Int64` scalar.\n\n### Solution\n\nIf we need to transfer an `Int64`, we need to transfer it as a `string`.\nAlthough here because we do not need this big of a number, we changed the expected input type on GraphQL side to be `Int`.\n\n### Lessons learnt\n\nGraphQL:\n\n- `scalar` is the \"minimum\"/\"leaf\" type for GraphQL that is used to build up object\n- there is a limited set of default scalar, and the others are customer defined. Keep in mind that we do not have 100% control of how the endpoint is called.\n- When choosing the type for a GraphQL schema, try to choose the simplest scalar data that does the job\n\nGraphQL Client:\n\n- Do not make assumptions of how the scalar types are used. The custom scalar is available via the GraphiQL docs, good to have a look. e.g.\n\n<img src=\"doc\" width=\"400px\">\n\n## Use the `InMemoryCache` feature of Apollo client to save network round trips on CRUD operations\n\n[docs](https://www.apollographql.com/docs/react/caching/overview/)\n\nThree options to make apollo update the cache:\n\n### 1. If there is a field that has a 1:1 relationship to the data, apollo can do the cache update itself\n\nGot this trick from Greg, the keys are:\n\n1. to help apollo find out who the `keyFields` is for this data type:\n\n```js\ncache: new InMemoryCache({\n    typePolicies: {\n      TippingConfig: {\n        keyFields: ['outletID'],\n      },\n    },\n  }),\n```\n\n2.  make sure the response that updates the data has the correct data structure of this type (i.e. when `delete`, returning `deleted: true` is not good enough, need to return the full data structure with `status` set to `OFF`)\n\nthen whenever there are this data type coming in from a response of a `useQuery` or `useMutation` call, apollo knows where to update it.\n\n### 2. use `cache.modify` to do a \"surgical\" update on the data\n\ne.g. To delete a field's value:\n\nhttps://www.apollographql.com/docs/react/caching/cache-interaction/#example-invalidating-fields-within-a-cached-object\n\n```js\ncache.modify({\n  id: cache.identify(myPost),\n  fields: {\n    comments(existingCommentRefs, { INVALIDATE }) {\n      return INVALIDATE\n    },\n  },\n})\n```\n\n### 3. use `cache.readQuery` and `cache.writeQuery` to operate on the cache just like we interact with gql server.\n\nMost expressive.\nUse standard GraphQL queries.\n","tags":["React","GraphQL","Apollo"]},{"title":"A continuous improvement development journey with Nathan","url":"/2022/07/27/A-continuous-improvement-development-journey-with-Nathan/","content":"\n# A continuous improvement development journey with Nathan\n\nThis week I had a very exciting development journey with Nathan's help. Nathan gave me some demonstrations on many good-practice patterns/principles that I knew existed but never put into practice, and I got on a journey that involved a lot of continuous improvement and reflections on how I structure and design my code.\n\n## My development goal\n\nI am creating a modal to allow people configure how they want to accept tips.\nThere are three types: fixed amounts, percentage, smart threshold [Stripe documentation here](https://stripe.com/docs/api/terminal/configuration/object#terminal_configuration_object-tipping). The view is already nicely designed so my goal is to implement the designed view and create the correct model to map from the view to the object expected by Stripe (our backend service actually, but similar thing).\n\n## How I approached it in the first iteration\n\n### Plan\n\nI was easily addicted to the data structure provided by Stripe 😂:\n\n```js\n{\n  fixed_amounts: [<integer in cents>, <integer in cents>, <integer in cents>],\n  percentages: [<integer>, <integer>, <integer>],\n  smart_tip_threshold: <integer>\n}\n```\n\nI wanted every component to use this data structure, and want the editing form component to do \"surgical\" operations on this object when user makes a change in the input.\n\n### Challenges\n\n#### 1- required fields/ array lengths 🤔\n\nBecause I want to map the array to three inputs, I immediately faced the issue of what happens if the array is undefined/has less than three values. So I made the first ugly thing (I thought it was cute back then 🐶)\n\n```js\n{\n  fixed_amounts: [undefined, undefined, undefined],\n  percentages: [undefined, undefined, undefined],\n  smart_tip_threshold: undefined\n}\n```\n\nThen as I mentioned, I planned a \"surgical\" approach to change the object, I heavily used this helper method to do some surgeries on the object while user is updating the input:\n\n```js\n/**\n * Set array's element at given index to the given value\n */\nexport const setValueAtIndex = <T>(\n  array: T[],\n  value: T,\n  index: number\n): T[] => [...array.slice(0, index), value, ...array.slice(index + 1)]\n```\n\n#### 2 - cent <-> dollars\n\nWe want to display dollars and let user edit in dollars, but the data structure I'm addicted to is in cents. The issue is in the conversion, rounding issues will happen. e.g. if user type 6.4444, input will become 6.444399999999999\nSo I created a second ugly thing to keep track of the object while user is editing.\n\n```js\nexport interface TippingConfigurationInEdit {\n  fixed_amounts_in_dollar: Array<number | undefined>\n  percentages: Array<number | undefined>\n  smart_tip_threshold_in_dollar?: number\n}\n\n```\n\nThis also leads to some complicated logic to convert `TippingConfigurationInEdit` to the object of the original structure in cents + clear the unexpected fields (e.g. clear percentages if user chose to use fixed amounts).\n\n#### 3 - flag errors for individual inputs, and disable/enable buttons correspondingly\n\nWell, once I have something like `TippingConfigurationInEdit`, there's nothing too wired for me to feel comfortable to make. So I made this to keep track of each fields' errors:\n\n```js\nexport interface TippingConfigurationErrors {\n  fixed_amounts_in_dollar_errors: Array<boolean | undefined>\n  percentages_errors: Array<boolean | undefined>\n  smart_tip_threshold_error: boolean | undefined\n}\n```\n\nThis is where I could have noticed some violation of DRY principle - the logic to check that no errors are present for the current configuration is almost exactly the same as the logic to check all required fields are filled (and equally cumbersome given that not all fields are useful for fixed amount/percentage configuration), yet there's no easy way to extract this logic.\n\n#### 4 - Organization of components\n\nThere are two steps when user are adding a tipping configuration: select outlet -> tipping configuration.\n\nThen there are some fine edge cases for design:\n\n1. if there is only one outlet, go directly to tipping configuration\n2. if user clicks \"edit\" button instead of \"add new\", go directly to tipping configuration too, but with different button text and headings for the modal\n3. allow user to go back to outlet selection if there are multiple outlets and user is creating a new one\n\nBecause my components are very affective with each other, the parent component controlling which model to open become very \"stateful\". I wound up having a state structure like this:\n\n```js\ntype TippingModalStep =\n  | { step: 'off' }\n  | {\n      step: 'selectOutlet'\n      outlets: Outlet[]\n    }\n  | {\n      step: 'editConfig'\n      outlet: Outlet\n      isNewConfig: boolean\n      allowBack: boolean\n      // TODO pass original tipping config information to edit modal\n    }\n  | {\n      step: 'deleteConfig'\n      outlet: Outlet\n    }\n```\n\nSee the `TODO`, this is not even all of it yet:)\n\n### My feelings after the 1st iteration\n\nI was super relieved when everything was working and the UI behaviours met the requirement 😌 I actually feel quite happy that I \"hacked🥷\" the way through it , like, `I knew it was not pretty here and there, but what can I do when Stripe gives me this cumbersome data structure?` and I even told hubby proudly, `I did so well surviving the mess 😂`\n\n#### My attempts to mitigate the ugly things\n\nI did some efforts to mitigate ugly parts. I'm quite happy I did as it was very helpful for the continuous improvement:\n\n1. I added comments on the data structure like `TippingConfigurationInEdit` to explain what was happening and to help people understanding. (I read recently that too much non-doc comment itself is a good indicator of unmaintainable code, but I wasn't relating this with what I was doing back then)\n2. I added lots of unit tests to the big fat util functions I created. These tests will not end up going into prod as the fat utils are not needed in the end, but having the tests in place helped a lot while I was refactoring and checking that only the \"expected\" tests are failing.\n\n## My discussion with Nathan to go through the 1st iteration\n\nI was expecting to receive some great feedbacks from Nathan to further mitigate the ugly things, but I was not expecting that Nathan helped me discover a whole new path of approaching this.\n\nTo start with summarizing some important thoughts I got from Nathan's feedback and questions:\n\n### Do authentic modularizing\n\nThe whole point of modularizing things is to decouple and to isolate things. So I should not base child/grandchild component design decisions on their parents. More specifically for my approach, I shouldn't have been so obsessed with the Stripe tipping config data structure that only the parent responsible for submitting it should know.\n\nNathan demoed how he would look at the design and discover the \"boundaries\" between components. By contrast I keep starting from a parent and take things out when parents become too big. Both approaches are necessary from time to time, but the \"boundary\" way of thinking is what I lack when doing frontend dev.\n\n> Don’t think about the child component in relationship to its parent, think about how you would write it itself! Then think about the data structure you need, and then put it to the parent.\n\n### The concept of \"Adapter\"!\n\nHeard the word adapter from blogs but thought it was for larger systems converting big fat data into another set of big fat data. But actually it could be for converting the very \"knowledgeable\" stripe data structure into something nice and simple for a child component that only cares about basic tipping configuration instead of the dynamic one.\n\nSome nice `onChange` function will adapt the child component's private, simplified data structure to the fat one.\n\n### Stop and track where the smell is baked\n\nI shouldn't have stopped at mitigating smells.\n\nI should track up where the smells come from.\n\nFor example, if I keep needing many `if` blocks to decide which field to operate, should these fields be put together from the very beginning? They are apparently not cohesive, why do they have to stay together?\n\n## My 2nd iteration after discussion with Nathan\n\n### Resolve challenge 1: required fields/ array lengths\n\nIf I know I always need three fields, if I start from the child component, I will immediately know I don't want an array. I want three fields, `option1` `option2` `option3`. Crystal clear all of a sudden 🔮\n\n### Resolve challenge 2: cent <-> dollars\n\nFollowing 1, if I create child's own component, I can create as I need it - the child DOES NOT EVEN NEED TO KNOW THE CONCEPT OF CENTS, except in the ADAPTER!\n\n### Resolve challenge 3: flag errors for individual inputs, and disable/enable buttons correspondingly\n\nSimilarly, I could have another child's private component, also with three fields instead of a boolean array, for `error1` `error2` `error3`. Again the child does not need to know anything else, so bye bye big data structure and many many `if` conditions for validation.\n\n### Resolve challenge 4: Organization of components\n\nThis relates back to the concept of modularization and decoupling. With well defined boundaries between components, a configuration modal will keep the knowledge about steps and going back and forward to itself.\n\n## Some general reflections at this stage\n\nIt is good to have a first iteration that works in an ugly way, as this will 1) prove things can work 2) set some base grounds to collect feedback and make step-by-step improvements. So I shouldn't feel bad that the first iteration is not pretty. It's just that I shouldn't stop there. Looking back at this journey with Sam he gave the same advice too and highlighted that this is a typical engineering journey 🐾 🐾\n\nI can be more proactive in looking for advice when things are not as nice as I want but I cannot think of a way to improve it. Instead of submitting a not-as-nice version for review and waiting for people to RE-discover the things I already smelled, I could have more proactive and ask for a discussion before I put the PR for review. This will make the conversation easier and save lots more RE-discover time.\n\n### 2nd Iteration\n\nThe biggest thing to be handled in 2nd iteration is where the conversion between the child and the parent components' data structure happens.\n\nA nice way Nathan and I settled on:\n\n```\nonChange: (formValues: BasicFormValues) => void\n```\n\nPassed from parent to child, so that child can call this `onChange` and parent can take care of the update of its more comprehensive data structure.\n","tags":["Web Development","Maintainable Software","WIP"]},{"title":"The this mystery - why fat arrow does not work for Angularjs controller","url":"/2022/07/08/The-this-mystery-why-fat-arrow-does-not-work-for-Angularjs-controller/","content":"\nI was learning some Angular.js for Reactify migration and noticed that when I define a component with template and controller like this:\n\n```js\nphonecatApp.component('phoneList', {\n  templateUrl: `<ul>\n  <li ng-repeat=\"phone in $ctrl.phones\">\n    <span> {{phone.name}} </span>\n    <span>{{phone.snippet}} </span>\n  </li>\n</ul>\n`,\n  controller: () => {\n    this.phones = [\n      {\n        snippet: 'Fast just got faster with Nexus S.',\n      },\n      {\n        name: 'Motorola XOOM™ with Wi-Fi',\n        snippet: 'The Next, Next Generation tablet.',\n      },\n      {\n        name: 'MOTOROLA XOOM™',\n        snippet: 'The Next, Next Generation tablet.',\n      },\n    ]\n  },\n})\n```\n\nThe phone list cannot be rendered. But this works:\n\n```js\nphonecatApp.component('phoneList', {\n  templateUrl: `<ul>\n  <li ng-repeat=\"phone in $ctrl.phones\">\n    <span> {{phone.name}} </span>\n    <span>{{phone.snippet}} </span>\n  </li>\n</ul>\n`,\n  controller: function () {\n    this.phones = [\n      {\n        snippet: 'Fast just got faster with Nexus S.',\n      },\n      {\n        name: 'Motorola XOOM™ with Wi-Fi',\n        snippet: 'The Next, Next Generation tablet.',\n      },\n      {\n        name: 'MOTOROLA XOOM™',\n        snippet: 'The Next, Next Generation tablet.',\n      },\n    ]\n  },\n})\n```\n\n## Here's why\n\n### There's no `this` binding with fat arrow function\n\nhttps://www.w3schools.com/js/js_arrow_function.asp#:~:text=In%20short%2C%20with%20arrow%20functions,that%20defined%20the%20arrow%20function.\n\n> In regular functions the this keyword represented the object that called the function, which could be the window, the document, a button or whatever.\n\n> With arrow functions the this keyword always represents the object that defined the arrow function.\n\n### If arrow function is called with `fn.apply(self, args)` the `self` arg will be ignored\n\nIf I define these two functions:\n\n```js\n> arrowFn = () => console.log(this)\n> fn = function(){console.log(this)}\n```\n\nIf I call them directly:\n\n```js\n> fn()\n<ref *1> Object [global] {\n  global: [Circular *1],\n  clearInterval: [Function: clearInterval],\n  clearTimeout: [Function: clearTimeout],\n  setInterval: [Function: setInterval],\n  setTimeout: [Function: setTimeout] {\n    [Symbol(nodejs.util.promisify.custom)]: [Getter]\n  },\n  queueMicrotask: [Function: queueMicrotask],\n  clearImmediate: [Function: clearImmediate],\n  setImmediate: [Function: setImmediate] {\n    [Symbol(nodejs.util.promisify.custom)]: [Getter]\n  },\n  arrowFn: [Function: arrowFn],\n  fn: [Function: fn]\n}\n> arrowFn()\n<ref *1> Object [global] {\n  global: [Circular *1],\n  clearInterval: [Function: clearInterval],\n  clearTimeout: [Function: clearTimeout],\n  setInterval: [Function: setInterval],\n  setTimeout: [Function: setTimeout] {\n    [Symbol(nodejs.util.promisify.custom)]: [Getter]\n  },\n  queueMicrotask: [Function: queueMicrotask],\n  clearImmediate: [Function: clearImmediate],\n  setImmediate: [Function: setImmediate] {\n    [Symbol(nodejs.util.promisify.custom)]: [Getter]\n  },\n  arrowFn: [Function: arrowFn],\n  fn: [Function: fn]\n}\n```\n\nI get the global scope for this.\n\nIf I call `fn` with `apply`, I can change `this`:\n\n```js\n> fn.apply(\"hello\", [])\n[String: 'hello']\n```\n\nIf I call `arrowFn` with `apply`, `this` is still `global`:\n\n```js\n> arrowFn.apply(\"hello\", [])\n<ref *1> Object [global] {\n  global: [Circular *1],\n  clearInterval: [Function: clearInterval],\n  clearTimeout: [Function: clearTimeout],\n  setInterval: [Function: setInterval],\n  setTimeout: [Function: setTimeout] {\n    [Symbol(nodejs.util.promisify.custom)]: [Getter]\n  },\n  queueMicrotask: [Function: queueMicrotask],\n  clearImmediate: [Function: clearImmediate],\n  setImmediate: [Function: setImmediate] {\n    [Symbol(nodejs.util.promisify.custom)]: [Getter]\n  },\n  arrowFn: [Function: arrowFn],\n  fn: [Function: fn]\n}\n```\n\n### Angular uses `apply` to define the scope for controller\n\n> Angular calls the controller function like this: fn.apply(self, args); where self (which becomes this in the invoked function) is an object that has the required fields\n\nhttps://stackoverflow.com/questions/35176469/passing-scope-variables-to-an-angularjs-controller-using-fat-arrows\n\n### Can I use arrow function with `.` definition\n\nShort answer: no.\n\nfunction definition works as expected:\n\n```js\n> const obj = {}\n> obj.fn = function(){console.log(this)}\n\n> obj.fn()\n{ fn: [Function (anonymous)] }\n```\n\narrow function still only remembers `global` for `this`:\n\n```js\n> obj.arrowFn = () => console.log(this)\n\n> obj.arrowFn()\n<ref *1> Object [global] {\n  global: [Circular *1],\n  clearInterval: [Function: clearInterval],\n  clearTimeout: [Function: clearTimeout],\n  setInterval: [Function: setInterval],\n  setTimeout: [Function: setTimeout] {\n    [Symbol(nodejs.util.promisify.custom)]: [Getter]\n  },\n  queueMicrotask: [Function: queueMicrotask],\n  clearImmediate: [Function: clearImmediate],\n  setImmediate: [Function: setImmediate] {\n    [Symbol(nodejs.util.promisify.custom)]: [Getter]\n  }\n}\n```\n\n## Key takeaways\n\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/Arrow_functions#Invoked_through_call_or_apply\n\n> Arrow functions don't have their own bindings to `this`, `arguments` or `super`, and should not be used as `methods`.\n\n> Arrow functions aren't suitable for `call`, `apply` and `bind` methods, which generally rely on establishing a `scope`.\n\n### Good user cases of arrow function\n\n- More consise promise chains\n- Easy array filter, mapping, reducing\n- Immediately invoked function expression\n- Parameterless functions as call back arguments that are easier for eyes\n","tags":["Web Development","Angular"]},{"title":"Flaky React unit test","url":"/2022/06/01/Flaky-React-unit-test/","content":"\n## ESLint extension: eslint-plugin-testing-library\n\nhttps://github.com/testing-library/eslint-plugin-testing-library\n\n## Await async components updates\n\nFlaky alert: `await wait(100)`\n\nUseful functions:\n\n`await waitForElementToBeRemoved(() => screen.getByTestId('loading-spinner'))`\n\n`await waitForNextUpdate()`\n\n`await waitFor(() => expect(queryByText('All')).toBeNull())`\n\n`await waitForValueToChange(() => result.current.comments)`\n\n## Responding to the `act` warning correctly\n\n> To prepare a component for assertions, wrap the code rendering it and performing updates inside an act() call. This makes your test run closer to how React works in the browser.\n\n`fireEvent` is already wrapped in `act` inside, you get the warning because an update after `fireEvent` is not awaited.\n\nTo fix act warning, make sure you properly handle asynchronous actions!\n\n## Resources\n\nhttps://kentcdodds.com/blog/common-mistakes-with-react-testing-library\n\n# Learnings from applying eslint plugin for testing library - whys\n\n## Avoid destructuring queries from `render` result, use `screen.getByText` instead\n\nSaves time and line\n\n## Forbidden usage of `render` within testing framework `beforeEach` setup\n\nMove rendering as close to assertion as possible\n\n## Prefer using queryBy\\* when waiting for disappearance\n\nMore specific error message\n","tags":["Web Development","React","Test"]},{"title":"Jest query normalization","url":"/2022/05/12/Jest-query-normalization/","content":"\nJest query (`get`, `query`, `find`) has some powerful normalization setting, and when not aware of them could cause trouble:\nhttps://testing-library.com/docs/queries/about/#normalization\n\n```tsx\n// component\n<<h2 className=\"vd-align-center\">\n`Tap, insert, or swipe the customer's card\\n used in the original sale.`\n</h2>\n```\n\n```ts\n// will not pass because collapseWhitespace defaults to true \\n is collapsed into a single space\nexpect(screen.getByText(`Tap, insert, or swipe the customer's card\\nused in the original sale.`)).toBeVisible()\n\n// will pass by querying the collapsed text\nexpect(screen.getByText(`Tap, insert, or swipe the customer's card used in the original sale.`)).toBeVisible()\n\n// will pass by disabling collapseWhitespace\nexpect(screen.getByText(`Tap, insert, or swipe the customer's card\\nused in the original sale.`{\n  normalizer: getDefaultNormalizer({ collapseWhitespace: false }),\n  }))\n.toBeVisible()\n\n```\n","tags":["Web Development","Jest"]},{"title":"JavaScript Security learning note","url":"/2022/05/06/JavaScript-Security-learning-note/","content":"\n## Top JavaScript security threats\n\nKey resource: https://owasp.org/www-project-top-ten/\n\n- Injection (XSS)\n- Broken authentication:\n  - Use proper Session library, no session Id in browser\n  - HTTPS\n  - crypto/bcrypt the credential data\n- Sensitive data\n  - crypto/bcrypt the sensitive data\n  - tools like Jscrambler - scramble your code and data to make it hard to understand\n  - HTTPS\n- XML external entities\n  - when XML is malformed -> takes longer to process due to its structure -> taken advantage by hackers to make the processing unit useless -> DoS attach\n- Security misconfiguration\n  - Debugging left on or console.log in the client\n  - Development version in production\n  - Default credentials\n  - Improper access control - roles with minimum privilege for each part of the system; server variables never in repo\n- Insecure de-serialization\n  - Key: Ensure your de-serialization library does not use the JS's `eval()` method. Choose the most active de/serialization package in npm\n- Component/dependency that is insecure\n  - github dependabot, snyk, npm audit\n- Insufficient logging and monitoring, e.g. not knowing it when you have a spike of request (DoS)\n  - Performance of server\n  - Server activity logs\n  - Network logs\n  - User logs\n  - Google analytics\n  - Application analytics\n","tags":["Web Development","Security","JavaScript"]},{"title":"TypeScript literal union type limitation - no propagation into object or array","url":"/2022/05/05/TypeScript-literal-union-type-limitation-no-propagation-into-object-or-array/","content":"\n## How I noticed it\n\nI was writing a unit test suite leveraging the `test.each` table approach of jest.\nWe have a string literal union type like this:\n\n```ts\nexport type UnrefRefundReason =\n  | 'ZERO_DOLLAR_REFUNDABLE'\n  | 'MULTIPLE_PAYMENTS'\n  | 'BLIND_REFUND'\n  | 'DIFFERENT_PAYMENT_TYPES'\n```\n\nAnd the test table layout looks like this:\n\n```ts\n  test.each([\n    {\n      someIds: undefined,\n      someBoolean: true,\n      reason: 'DIFFERENT_PAYMENT_TYPES',\n    },\n    {\n      someIds: ['1', '2'],\n      someBoolean: false,\n      reason: 'MULTIPLE_PAYMENTS',\n    },\n    {\n      someIds: ['1', '2'],\n      someBoolean: true,\n      reason: 'MULTIPLE_PAYMENTS',\n    },\n  ])(\n    'goes to the state of unreferencedPrompt with the correct reason $reason if unreferenced refund is needed',\n    async ({\n      someIds,\n      someBoolean,\n      reason,\n    }: {\n      someIds: string[]\n      someBoolean: boolean\n      reason: UnrefRefundReason,\n    }){\n       //... test script\n       })\n```\n\nThe error I got was:\n\n```\n Types of property 'reason' are incompatible.\n          Type 'string' is not assignable to type 'UnrefRefundReason'.\n```\n\nIf I go with the other mapping approach (array -> argument list):\n\n```ts\ntest.each([\n  [undefined, false, 'BLIND_REFUND'],\n  [[], false, 'BLIND_REFUND'],\n  [undefined, true, 'DIFFERENT_PAYMENT_TYPES'],\n  [[], true, 'DIFFERENT_PAYMENT_TYPES'],\n  [['1', '2'], false, 'MULTIPLE_PAYMENTS'],\n  [['1', '2'], true, 'MULTIPLE_PAYMENTS'],\n])(\n  'goes to the state of unreferencedPrompt with the correct reason $reason if unreferenced refund is needed',\n  async (\n    someIds: string[] | undefined,\n    someBoolean: boolean,\n    reason: UnrefRefundReason\n  ) => {\n    //...test script\n  }\n)\n```\n\nI got similar error:\n\n```\nType '[undefined, boolean, string]' is not assignable to type '[originalRegisterSalePaymentIds: string[] | undefined, originalSalePaidByOtherPT: boolean, reason: UnrefRefundReason]'.\n```\n\n## Findings\n\nFrom `string` to `UnrefRefundReason`, TypeScript needs to do a \"type narrowing\". But the type narrowing will not propagate into object. See explanation here:\n\nhttps://github.com/microsoft/TypeScript/issues/31755#issuecomment-498669080\n\n## Preference\n\nI am increasingly preferring enum instead of literal union:\n\n```ts\nexport enum UnrefRefundReason {\n  ZERO_DOLLAR_REFUNDABLE = 'ZERO_DOLLAR_REFUNDABLE',\n  MULTIPLE_PAYMENTS = 'MULTIPLE_PAYMENTS',\n  BLIND_REFUND = 'BLIND_REFUND',\n  DIFFERENT_PAYMENT_TYPES = 'DIFFERENT_PAYMENT_TYPES',\n}\n```\n","tags":["TypeScript"]},{"title":"Batch GraphQL requests","url":"/2022/04/27/Batch-GraphQL-requests/","content":"\nhttps://github.com/vend/monocle/pull/4463\nhttps://github.com/vend/monocle/pull/4464\nhttps://github.com/vend/monocle/pull/4484\n\nhttps://www.apollographql.com/blog/apollo-client/performance/batching-client-graphql-queries/\n\n## Pros\n\n> Avoid the cost of multiple round trips.\n\n## Biggest Con\n\n> If one of the underlying services is slower than the rest, the effects may be felt across the whole client app.\n","tags":["Web Development","GraphQL"]},{"title":"MySQL interpolate params","url":"/2022/04/22/MySQL-interpolate-params/","content":"\nA new learning at work.\n\n## How MySQL handles parameters in query\n\nWith `interpolateParams=false`, the driver has to do three roundtrips to the database:\n\n- prepare a statement,\n- execute it with given parameters and\n- close the statement again\n\nIf `interpolateParams` is true, placeholders (?) in calls to `db.Query()` and `db.Exec()` are interpolated into a single query string with given parameters in the [driver library](https://github.com/go-sql-driver/mysql#parameters). This reduces the number of roundtrips.\n\n## SQL Injection risk\n\nhttps://stackoverflow.com/questions/5741187/sql-injection-that-gets-around-mysql-real-escape-string/12118602#12118602\n","tags":["Go","Database"]},{"title":"Customise fmt.Format in Go","url":"/2022/04/20/Customise-fmt-Format-in-Go/","content":"\nhttps://pkg.go.dev/fmt#State\n\nhttps://pkg.go.dev/fmt#Formatter\n\n```go\n// Format formats the frame according to the fmt.Formatter interface.\n//\n//    %s    source file\n//    %d    source line\n//    %n    function name\n//    %v    equivalent to %s:%d\n//\n// Format accepts flags that alter the printing of some verbs, as follows:\n//\n//    %+s   function name and path of source file relative to the compile time\n//          GOPATH separated by \\n\\t (<funcname>\\n\\t<path>)\n//    %+v   equivalent to %+s:%d\nfunc (f Frame) Format(state fmt.State, verb rune) {\n\tswitch verb {\n\tcase 's':\n\t\tswitch {\n\t\tcase state.Flag('+'): // Flag reports whether the flag is present\n\t\t\tio.WriteString(state, f.name()) // state has the Write function -> Writer interface\n\t\t\tio.WriteString(state, \"\\n\\t\")\n\t\t\tio.WriteString(state, f.file())\n\t\tdefault:\n\t\t\tio.WriteString(state, path.Base(f.file()))\n\t\t}\n\tcase 'd':\n\t\tio.WriteString(state, strconv.Itoa(f.line()))\n\tcase 'n':\n\t\tio.WriteString(state, funcname(f.name()))\n\tcase 'v':\n\t\tf.Format(state, 's')\n\t\tio.WriteString(s, \":\")\n\t\tf.Format(state, 'd')\n\t}\n}\n```\n","tags":["Go"]},{"title":"Javascript generator to async","url":"/2022/04/20/Javascript-generator-to-async/","content":"\n## Understanding generator\n\n### Where it comes from\n\nFrom MDN:\n\n> The Generator object is returned by a generator function and it conforms to both the `iterable` protocol and the `iterator` protocol.\n\n> This object cannot be instantiated directly. Instead, a `Generator` instance can be returned from a `generator function`:\n\n```js\nfunction* generator() {\n  yield 1\n  yield 2\n  yield 3\n}\n\nconst gen = generator() // \"Generator { }\"\n```\n\n### How it runs\n\nIt pauses at `yield` keyword and can resume from there.\n\nIf there is:\n\n```js\ny = yield 1\nyield y\n```\n\nWhen calling `gen.next(2)`, generator will yield `1` and pause before the `y =` assigning. And then when calling `gen.next()`, generator will assign `y = 2` and yield 2.\n\n## `async` `await` are based on generator\n\n### Definition\n\n```js\nfunction _asyncToGenerator(fn) {\n  // fn is a generator function\n  return () => {\n    var self = this,\n      args = arguments\n\n    return new Promise(function (resolve, reject) {\n      // gen is a generator\n      var gen = fn.apply(self, args)\n      function _next(value) {\n        asyncGeneratorStep(gen, resolve, reject, _next, _throw, 'next', value)\n      }\n      function _throw(err) {\n        asyncGeneratorStep(gen, resolve, reject, _next, _throw, 'throw', err)\n      }\n      _next(undefined)\n    })\n  }\n}\n\nfunction asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) {\n  let info, value\n  try {\n    info = gen[key](arg) // invoke next or throw function on the generator object with arg\n    value = info.value // info = {value: _, done: _}. Note that value is a promise\n  } catch (error) {\n    reject(error)\n    return\n  }\n\n  if (info.done) {\n    resolve(value) // if generator has finished, resolve the promise returned by _asyncToGenerator\n  } else {\n    // if generator has not finished, recursively iterate to the next in gen\n    Promise.resolve(value).then(_next, _throw)\n  }\n}\n```\n\n### Usage\n\n```js\nconst asyncFunc = _asyncToGenerator(function* () {\n  console.log(1)\n  yield new Promise((resolve) => {\n    setTimeout(() => {\n      resolve()\n      console.log('sleep 1s')\n    }, 1000)\n  })\n  console.log(2)\n  const a = yield Promise.resolve('a')\n  console.log(3)\n  const b = yield Promise.resolve('b')\n  const c = yield Promise.resolve('c')\n  return [a, b, c]\n})\n\nasyncFunc().then((res) => {\n  console.log(res)\n})\n```\n\nSame output as:\n\n```js\nconst asyncFunc = async () => {\n  console.log(1)\n  await new Promise((resolve) => {\n    setTimeout(() => {\n      resolve()\n      console.log('sleep 1s')\n    }, 1000)\n  })\n  console.log(2)\n  // each await is a yield statement in the generator function\n  // with await, the async function resumes to execute to the next line when promise resolves\n  // with generator, the generator iterates to next when promise resolves\n  const a = await Promise.resolve('a')\n  console.log(3)\n  const b = await Promise.resolve('b')\n  const c = await Promise.resolve('c')\n  return [a, b, c]\n}\n\nfunc().then((res) => {\n  console.log(res)\n})\n```\n\n## An interesting observation on promise being flattened\n\nI noticed that if instead of returning `[a, b, c]` in the end, I return another promise that wraps multiple layers of promise, resolves to `[a,b,c]`, the `.then` will still resolve to `[a, b, c]`.\nThis is because `Promise.resolve()` flattens the promise:\n\n> This function flattens nested layers of promise-like objects (e.g. a promise that resolves to a promise that resolves to something) into a single layer.\n\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/resolve\n","tags":["Javascript"]},{"title":"Error handling in Go","url":"/2022/04/20/Error-handling-in-Go/","content":"\nSee also - [error wrapping in Go](https://niuniuanran.github.io/2022/04/08/Error-wrapping-in-Go/) notes\n\nhttps://www.digitalocean.com/community/tutorials/how-to-add-extra-information-to-errors-in-go\n","tags":["Go"]},{"title":"Error wrapping in Go","url":"/2022/04/20/Error-wrapping-in-Go/","content":"\nhttps://go.dev/blog/go1.13-errors\nhttps://pkg.go.dev/errors#As\n\n## Don't:\n\n```\nfunc isPairingNotFound(err error) bool {\n\treturn err.(errors.Error).ErrorCode() == errors.ErrCodeRegisterPairingNotFound.Code()\n}\n```\n\n## Do:\n\n```\nimport (\n\t\"errors\"\n)\n\nfunc isPairingNotFound(err error) bool {\n  var serviceErr service.Error\n  if errors.As(err, &serviceErr) {\n    return serviceErr.ErrorCode() == errors.ErrCodeRegisterPairingNotFound.Code()\n  }\n}\n```\n\n> The Unwrap, Is and As functions work on errors that may wrap other errors. An error wraps another error if its type has the method `Unwrap() error`\n> If `e.Unwrap()` returns a non-nil error w, then we say that e wraps w.\n","tags":["Go"]},{"title":"Transifex localization","url":"/2022/04/13/Transifex-localization/","content":"\n<img src=\"https://docs.transifex.com/transifex-native-sdk-overview/introduction\"/>\n\n## ESLint rule\n\ne.g. `@vend/eslint-config/i18n-strict` rule\nDisallow untranslatable rule\n\n## Suspense\n\nSuspense-like container component that hides UI until we fetched all translations for application.\n\n## Wrap and unwrap\n\n- @transifex/react\n\n  - `useT` hook will return a function to translate\n  - `T` component\n  - `UT` component, useful only when you want to wrap in html\n\n- @transifex/native\n  - `t` function, use out of component\n\n## Context?\n\nIs it a button or a title?\nCan not assume translation will be the same when they are the same in English\n\n- When string is very short\n- When string cannot be shared: t('Type') and t('Type') have same key and will share same translation; t('Type') and t('Type', {\\_context: 'Button'}) have different hash, so you can add a context to make it unique. We pay them by number of keys so try to reuse keys though.\n\n## Dynamic translations\n\nPass in variables as you need\n\n## Plural\n\nSupports ICU message format, but only supports plural for \"one\" and \"other\". If you need 0, use a ternary.\n\n## Do's and Don'ts\n\n- Dynamic variables must be translated text\n- No more plural shortcuts like `customer{count > 1 && 's'}` as it could be different in other languages\n","tags":["Web Development"]},{"title":"Should I just use the read replica","url":"/2022/04/11/Should-I-just-use-the-read-replica/","content":"\n## When to use read replica\n\nWe want to avoid overloading the primary by using read replica whenever possible:\n\n- Read queries\n- When we do not care about the latency of read replica update (1-2 seconds)\n\n## When to use primary instance\n\n### An example scenario where using the read replica for queries cause a problem\n\n> The retailer submits a new payment type form, and gets back to the payment type list page. Thee newly added payment type wasn't there, since the latency between primary<->secondary was longer than the time between the form submission API call and the subsequent list payment types API call.\n\nQuestion to ask:\n\n> Will the read call immediately come after a write?\n\n## The MySQL store maintains two connections - read-only and write\n\n```go\n// NewMySQL creates the MySQL DB.\nfunc NewMySQL() Store {\n\tvar replica *sqlx.DB\n\tvar primary *sqlx.DB\n\tif config.Bool(\"READ_REPLICA\", false) {\n\t\treplicaConnector, err := db.NewConnector(\"read-only\")\n\t\tif err != nil {\n\t\t\tlog.Global().WithError(err).Fatal(\"failed creating database replica connector\")\n\t\t}\n\t\treplica = replicaConnector.Db()\n\t\tgo reportStats(replica, \"read\")\n\n\t\tprimaryConnector, err := db.NewConnector(\"write\")\n\t\tif err != nil {\n\t\t\tlog.Global().WithError(err).Fatal(\"failed creating database primary connector\")\n\t\t}\n\t\tprimary = primaryConnector.Db()\n\t\tgo reportStats(primary, \"write\")\n\t} else {\n\t\tdbInit(false)\n\t\tclient := db.Client()\n\t\tgo reportStats(client, \"both\")\n\t\treplica = client\n\t\tprimary = client\n\t}\n\treturn &mySQL{\n\t\treplica:         replica,\n\t\tprimary:         primary,\n\t\tmySQLTimer:      vetrics.Metrics().Timer(\"mysql.timer\"),\n\t\tmySQLMeterError: vetrics.Metrics().Counter(\"mysql.meter.error\"),\n\t}\n}\n\n```\n","tags":["Database"]},{"title":"Non-break space in html","url":"/2022/04/06/Non-break-space-in-html/","content":"\n# When to use `{' '}` and when to use `&nbsp;`, and when to use neither\n\n## {' '}\n\nThis is used to put an explicit space in a text block, since leading and trailing spaces are ignored at compile/transformation time when there is another tag.\n\n## `&nbsp;`\n\nA non-breaking space is a space that will not break into a new line.\nTwo words separated by a non-breaking space will stick together (not break into a new line). This is handy when breaking the words might be disruptive.\n\nTip: The non-breaking hyphen (&#8209;) is used to define a hyphen character (‑) that does not break into a new line.\n\nExamples:\n\n§ 10\n10 km/h\n10 PM\n\n## If you want styling space\n\nUse margin/padding!\n","tags":["Web Development","React"]},{"title":"Race condition with index.html scripts","url":"/2022/03/30/Race-condition-with-index-html-scripts/","content":"\n## Background:\n\nFlaky cypress test that fail and it complains:\n\n```\n\"Uncaught ReferenceError: System is not defined\"\n```\n\n`system.js` is included to the project from `index.html`:\n\nhttps://www.angelfire.com/pq/jamesbarbetti/articles/javascript/001_AvoidingRaceConditions.htm\n\nhttps://eager.io/blog/everything-I-know-about-the-script-tag/\n\nhttps://javascript.info/onload-ondomcontentloaded#domcontentloaded\n","tags":["Web development"]},{"title":"TypeScript Utility Types","url":"/2022/03/24/TypeScript-Utility-Types/","content":"\nhttps://www.typescriptlang.org/docs/handbook/utility-types.html\n\n- Partial<Type>: useful for update react component state via setter\n- Required<Type>: all fields are set to required, useful to reinforce fields\n- Readonly<Type>: all fields are set to `readonly`\n- Record<Keys, Type>: map the properties of a type to another type\n- Pick<Type, Keys>, Omit<Type, Keys>: include/ not include some properties of a type\n- Exclude<UnionType, ExcludedMembers>: exclude all union members assignable to `ExcludedMembers`. e.g. `type T = Exclude<string | number | (() => void), Function>`, `type T = string } number`\n- Extract<Type, Union>, all types assignable to `Union`, e.g. `type T = Extract<string | number | (() => void), Function>`, `type T = () => void`\n- NonNullable<Type>: extract `null` and `undefined`\n- Parameters<Type>: `Type` needs to satisfy the constraint `(...args:any) => any`, and `Parameters<Type>` will be type of args\n- ConstructorParameters<Type>\n- ReturnType<Type>\n- InstanceType<Type>: type of a class's instance\n- ThisParameterType<Type>\n  OmitThisParameter<Type>\n  ThisType<Type>\n\n## Intrinsic String Manipulation Types\n\n- Uppercase<StringType>\n- Lowercase<StringType>\n- Capitalize<StringType>\n- Uncapitalize<StringType>\n\n## Interesting examples\n\n```ts\n// ThisParameterType\nfunction toHex(this: Number) {\n  return this.toString(16)\n}\n\nfunction numberToString(n: ThisParameterType<typeof toHex>) {\n  return toHex.apply(n)\n}\n```\n\n```ts\n// remove this parameter via bind\nfunction toHex(this: Number) {\n  return this.toString(16)\n}\n\nconst fiveToHex: OmitThisParameter<typeof toHex> = toHex.bind(5)\n```\n\n## Intersection type & ThisType unity\n\n```ts\n// Intersection type, ThisType unity\ntype ObjectDescriptor<D, M> = {\n  data?: D\n  methods?: M & ThisType<D & M> // Type of 'this' in methods is D & M\n}\n\nfunction makeObject<D, M>(desc: ObjectDescriptor<D, M>): D & M {\n  let data: object = desc.data || {}\n  let methods: object = desc.methods || {}\n  return { ...data, ...methods } as D & M\n}\n\nlet obj = makeObject({\n  data: { x: 0, y: 0 },\n  methods: {\n    moveBy(dx: number, dy: number) {\n      this.x += dx // Strongly typed this\n      this.y += dy // Strongly typed this\n    },\n  },\n})\n\nobj.x = 10\nobj.y = 20\nobj.moveBy(5, 5)\n```\n\nWhen `type D = { x: number; y: number }` and `type M = { moveBy: (dx: number, dy: number) => void }`, type `D & M` is `{ x: number, y: number } & { moveBy(dx: number, dy: number): number }`.\n\nThe `methods` object in the argument to `makeObject` has a contextual type that includes `ThisType<D & M>` and therefore the type of `this` in methods within the `methods` object is `{ x: number, y: number } & { moveBy(dx: number, dy: number): number }`\n\nNotice how the type of the `methods` property simultaneously is an inference target and a source for the `this` type in methods.\n","tags":["TypeScript"]},{"title":"TypeScript - how to decide if a value is of a type in runtime","url":"/2022/03/24/Typescript-how-to-decide-if-a-value-is-of-a-type-in-runtime/","content":"\n## Check is value is member of Union type\n\n\"Array first\" approach\nhttps://dev.to/hansott/how-to-check-if-string-is-member-of-union-type-1j4m\n\n```ts\nconst ALL_SUITS = ['hearts', 'diamonds', 'spades', 'clubs'] as const\ntype SuitTuple = typeof ALL_SUITS // type SuitTuple = readonly [\"hearts\", \"diamonds\", \"spades\", \"clubs\"]\ntype Suit = SuitTuple[number] // type Suit = \"hearts\" | \"diamonds\" | \"spades\" | \"clubs\"\n```\n\n## Type guard function\n\n```ts\nfunction isSuit(value: string): value is Suit {\n  return ALL_SUITS.includes(value as Suit)\n}\n```\n","tags":["TypeScript"]},{"title":"Merge queue to resolve CI working on branch but broken later when merging to master","url":"/2022/03/16/Merge-queue-to-resolve-CI-working-on-branch-but-broken-later-when-merging-to-master/","content":"\n> monocle branches that fail when merged to master, one thing that’s sometimes done is using something called a “merge queue”. My understanding is that instead of merging straight to master, you add your PR to the merge queue. It runs through CI again with all commits from master and if it passes, it’s merged and deployed as usual. If it fails, it’s removed from the merge queue and the problems can be resolved in the PR.\n> So it’s kind of like a speculative merge to master that gets auto-reverted if it fails, with no permanent record of the failed merge. It should address situations like Stephen’s, where a linting rule was added before his PR was merged. It would not necessarily address flaky tests if those tests got lucky and passed both the branch CI and the merge queue CI.\n\nhttps://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/configuring-pull-request-merges/using-a-merge-queue\n\nhttps://shopify.engineering/introducing-the-merge-queue\n","tags":["WIP","CI/CD"]},{"title":"Test React component/hook that uses React Apollo","url":"/2022/03/07/Test-React-component-hook-that-uses-React-Apollo/","content":"\n## Problem\n\nWe have a `usePaymentEffectReducer` that needs to trigger `useQuery` as a hook:\n\n```js\nimport { useQuery } from '@apollo/client'\n\nconst usePairingWithDispatch = (\n  // arguments\n) => {\n  useQuery<LSPAYPairingData>(GQL_QUERY), {\n    variables: { ... },\n    onCompleted: data => {\n      // Handle data\n      // dispatch PAIR_OK\n    },\n    onError: error => {\n      // error handling\n      // dispatch ERROR\n    },\n  })\n}\n```\n\nIn order to test the upper hook, the GraphQL query done by Apollo client needs to be mocked out.\n\n## Finding 1: MockedProvider\n\nhttps://www.apollographql.com/docs/react/v2/api/react-testing/\n\nExample:\n\n```js\nit('runs the mocked query', () => {\n  render(\n    <MockedProvider mocks={mocks}>\n      <MyQueryComponent />\n    </MockedProvider>\n  )\n\n  // Run assertions on <MyQueryComponent/>\n})\n```\n\n## New Problem: I don't need a component, I need to render a hook\n\nI cannot just do:\n\n```js\n    const { result, waitForNextUpdate } = renderHook(() =>\n    <MockedProvider>\n      usePaymentEffectReducer({\n        ...TEST_ARGS,\n      })\n      </MockedProvider>\n    )\n```\n\n## Finding 2:\n\nhttps://react-hooks-testing-library.com/usage/advanced-hooks#context\n\n> Often, a hook is going to need a value out of context. The useContext hook is really good for this, but it will often require a Provider to be wrapped around the component using the hook. We can use the wrapper option for renderHook to do just that.\n\nExample:\n\n```js\nconst wrapper = ({ children }) => (\n  <CounterStepProvider step={2}>{children}</CounterStepProvider>\n)\nconst { result } = renderHook(() => useCounter(), { wrapper })\n```\n\n## New problem: mocking `graphQL` error in mocks gets type error\n\n### 1. Specify graphQL error as a field of `result`\n\nhttps://github.com/apollographql/react-apollo/issues/1127\n\n### 2. use `GraphQLError` constructor from `graphql` library\n\n```js\n import { GraphQLError } from 'graphql'\n mocks={[\n          {\n            request: APOLLO_REQUEST,\n            result: {\n              errors: [\n                new GraphQLError(\n                  'pairing missing',\n                  [],\n                  undefined,\n                  [],\n                  [],\n                  undefined,\n                  { code: SWIPER_ERROR_CODES.CODE_REGISTER_PAIRING_NOT_FOUND }\n                ),\n              ],\n            },\n          },\n        ]}\n```\n\n## New problem: wrong `act`?\n\n### Symptoms\n\nI imported `act` together with `renderHook`, thinking that would make sure my `act` will be the correct one:\n\n```js\nimport { renderHook, act } from '@testing-library/react-hooks'\n```\n\nThe code where I used `act` looks like this:\n\n```js\n    const { result } = renderHook(() => usePaymentEffectReducer(TEST_ARGS), {\n      wrapper: getApolloWrapperWithPairings([TEST_PAIRING_DATA]),\n    })\n    await waitFor(() => {\n      expect(result.current[0].status).toBe('error')\n    })\n\n    act(() => {\n      result.current[1]({\n        type: 'RETRY',\n        previousState: COLLECTING_STATE as PaymentState,\n      })\n    })\n```\n\nHowever, I still get an error message like this:\n\n```js\n    Warning: It looks like you're using the wrong act() around your test interactions.\n    Be sure to use the matching version of act() corresponding to your renderer:\n\n    // for react-dom:\n    import {act} from 'react-dom/test-utils';\n    // ...\n    act(() => ...);\n\n    // for react-test-renderer:\n    import TestRenderer from 'react-test-renderer';\n    const {act} = TestRenderer;\n    // ...\n    act(() => ...);\n        in TestComponent\n        in ApolloProvider (created by MockedProvider)\n        in MockedProvider\n        in Unknown\n        in Suspense\n        in ErrorBoundary\n```\n\n### Research on `act` doc\n\nAccording [render-hooks/act](https://react-hooks-testing-library.com/reference/api#act):\n\n> This is the same act function function that is exported from your chosen renderer.\n\nWhich of the [two renderers](https://react-hooks-testing-library.com/installation#renderer) will I import by importing `react-hooks`?\n\n> When using standard import for this library (show below), we will attempt to auto-detect which renderer you have installed and use it without needing any specific wiring up to make it happen. If you have both installed in your project, and you use the standard import (see below) the library will default to using react-test-renderer.\n\n> We use react-test-renderer by default as it enables hooks to be tested that are designed for either react or react-native and it is compatible with more test runners out-of-the-box as there is no DOM requirement to use it.\n\nThe interesting thing is that the `wrapper` will use `dom` renderer, so if `act` use the `native` renderer imported by auto detection, there will be a mismatch.\n\nIf I import specifically `dom` renderer like this:\n\n```js\nimport { renderHook, act } from '@testing-library/react-hooks/dom'\n```\n\nThe mismatch will be resolved.\n","tags":["React","GraphQL","Testing"]},{"title":"Front end polling back end for update","url":"/2022/03/01/Front-end-polling-back-end-for-update/","content":"\nConsider a payment flow:\n\n- Create a payment intent\n- Collect the payment from user's card\n- Capture the payment\n\nThere are two ways to handle it:\n\n1. Make the entire flow one API request that awaits for the card presentation to finish. The request can take quite long.\n2. Break the payment intent creation and payment capture into separate api requests, and each of them takes the time of a normal API request.\n\nFor type 1, there is an issue for front end - the http request cannot keep open forever, and the caller will get timeout error if the request does not get a response in ~1 minute.\nTo resolve this issue, the mechanism is:\n\n- The frontend sends the API request\n- When the backend receives the API request, write an empty database record and respond that the request has been received to frontend\n- The backend keeps going with the execution until the transaction has finished, and update the database record\n- In the meantime, the frontend polls for database update with an interval, and gives feedback to user when it detects the db status has been updated\n","tags":["Web Development"]},{"title":"Never trust a test until you see if fail","url":"/2022/02/17/Never-trust-a-test-until-you-see-if-fail/","content":"\nI got the title from a Pluralsight course [Testing React Components](https://app.pluralsight.com/library/courses/b4fc3c57-5e5f-42f8-ba38-4c6085740a22/table-of-contents), and I happened to experience an example yesterday:\n\n## queryBy vs findBy vs getBy\n\nTesting library docs provides [cheatsheet for queries](https://testing-library.com/docs/react-testing-library/cheatsheet/#queries) that list the differences, here I just want to note down one thing that almost tricked me:\n\nIf I do:\n\n```\nexpect(screen.findByPlaceholderText('Card Number')).not.toBeNull()\n```\n\nIt will always pass, because `findBy` query is `await`ed, and I am expecting the promise not to be null. Instead, I should go with `queryByPlaceholderText` if I want to know whether the component exists.\n","tags":["Testing","TDD"]},{"title":"Testing our component that wrapps a Stripe component and depends on events fired by Stripe component","url":"/2022/02/17/Testing-our-component-that-wrapps-a-Stripe-component-and-depends-on-events-fired-by-Stripe-component/","content":"\nNever thought I would have to do this before!\n\nWe accept card-not-present payment via a Stripe, and we use the API provided by Stripe.js library to render Stripe iframes that handle the user input for us.\nThe overall structure is:\n\n```javascript\n<Element stripe> //provider\n  <OurComponentToCollectCardDetails>\n    // ..\n    <CardElement>\n    // ..\n  </OurComponentToCollectCardDetails>\n</Element>\n```\n\n## Challenge 1: Mock all the moving parts of the package\n\nFound a mock that got me to render the CardCollectionForm (but not the iframe) [at this GitHub issue](https://github.com/stripe/react-stripe-js/issues/59#issuecomment-632271980) with [this tweak to make TS happy](https://github.com/stripe/react-stripe-js/issues/59#issuecomment-968633371)\n\nThe most helpful takeaway is the usage of `jest.requireActual`, which largely reduces the volume of job.\n\n```javascript\nconst mockElement = () => ({\n  mount: jest.fn(),\n  destroy: jest.fn(),\n  on: jest.fn(),\n  update: jest.fn(),\n})\n\nconst mockElements = () => {\n  const elements: { [key: string]: ReturnType<typeof mockElement> } = {}\n  return {\n    create: jest.fn((type) => {\n      elements[type] = mockElement()\n      return elements[type]\n    }),\n    getElement: jest.fn((type) => {\n      return elements[type] || null\n    }),\n  }\n}\n\nconst mockStripe = () => ({\n  elements: jest.fn(() => mockElements()),\n  createToken: jest.fn(),\n  createSource: jest.fn(),\n  createPaymentMethod: jest.fn(),\n  confirmCardPayment: jest.fn(),\n  confirmCardSetup: jest.fn(),\n  paymentRequest: jest.fn(),\n  _registerWrapper: jest.fn(),\n})\n\njest.mock('@stripe/react-stripe-js', () => {\n  const stripe = jest.requireActual('@stripe/react-stripe-js')\n\n  return {\n    ...stripe,\n    Element: () => {\n      return mockElement()\n    },\n    useStripe: () => {\n      return mockStripe()\n    },\n    useElements: () => {\n      return mockElements()\n    },\n  }\n})\n```\n\n## Challenge 2: The child component `CardElement` needs to invoke callback from the tested parent component\n\nI mocked the implementation of the `CardElement`:\n\n```js\njest.mock('@stripe/react-stripe-js', () => {\n  const stripe = jest.requireActual('@stripe/react-stripe-js')\n\n  return {\n    ...stripe,\n    Element: () => mockElement(),\n    useStripe: () => mockStripe(),\n    useElements: () => mockElements(),\n    CardElement: ({ onChange, options }: any) =>\n      cardElementSimulationButtons(onChange),\n  }\n})\n\nconst cardElementSimulationButtons = (onChange: any) => (\n  <>\n    <button\n      data-testid={simulateTestIds.error}\n      onClick={() =>\n        onChange!(\n          generateStripeCardElementChangeEvent(false, false, {\n            type: 'validation_error',\n            code: '110',\n            message: testErrorMessage,\n          })\n        )\n      }\n    />\n    <button\n      data-testid={simulateTestIds.complete}\n      onClick={() =>\n        onChange!(generateStripeCardElementChangeEvent(true, false, undefined))\n      }\n    />\n    <button\n      data-testid={simulateTestIds.empty}\n      onClick={() =>\n        onChange!(generateStripeCardElementChangeEvent(false, true, undefined))\n      }\n    />\n    <button\n      data-testid={simulateTestIds.nonEmpty}\n      onClick={() =>\n        onChange!(generateStripeCardElementChangeEvent(false, false, undefined))\n      }\n    />\n  </>\n)\n\nconst generateStripeCardElementChangeEvent = (\n  complete: boolean,\n  empty: boolean,\n  error?: any\n): StripeCardElementChangeEvent => ({\n  complete,\n  error,\n  empty,\n  elementType: 'card',\n  value: {\n    postalCode: '',\n  },\n  brand: 'visa',\n})\n\n```\n\nThen I could fire click event to simulate `onChange` invocations.\n\n## Are these practices actually good?\n\nFollow up readings\n\nhttps://maksimivanov.com/posts/dont-mock-what-you-dont-own/\n","tags":["React","Testing","Integration"]},{"title":"Overcoming some React hook testing challenges","url":"/2022/02/16/Overcoming-some-React-hook-testing-challenges/","content":"\n## Step 1: mock modules\n\n- [Understanding Jest Mocks](https://medium.com/@rickhanlonii/understanding-jest-mocks-f0046c68e53c): `fn`, `mock` and `spyOn`\n- [Mocking React hooks](https://chanonroy.medium.com/mocking-hooks-for-testing-with-jest-and-react-testing-library-d34505616d12)\n\nAfter these two readings I am able to mock modules that returns a dynamic value (with jest's mock naming convention).\n\n## Step 2: testing `useEffectReducer`\n\n- [Stackoverflow discussion on how to test useReducer itself and how the component uses useReducer](https://stackoverflow.com/a/59277748/10951931)\n\nTo test hooks, there's [@testing-library/react-hooks](https://github.com/testing-library/react-hooks-testing-library)\n\n### Challenge 1: state is not updating after `dispatch`\n\nNote that state got from `[state, dispatch] = result.current` will not get updated:\n\n> The current value of the result will reflect the latest of whatever is returned from the callback passed to renderHook.\n\nIn order to see what happens after `dispatch`, a new `current` needs to be obtained from `result` - or use `result.current[0]`\n\n### Challenge 2: Warning - \"An update to TestComponent inside a test was not wrapped in act(...).\"\n\nAfter investigation, I found that comes from code blocks like this:\n\n```\n    someAsyncMethod()\n        .then(result => {\n          dispatch({ type: 'SOME_EVENT'})\n        })\n        .catch(error => {\n          dispatch({     👈 warning produced here\n            type: 'ERROR',\n            errorMessage: SOME_MESSAGE,\n          })\n        })\n```\n\nI wrapped the `dispatch` in the test script with `act` but cannot wrap this one.\n\nI did not see this problem in the tests of `useReducer` because `useReducer` does not have effects that make function calls to do the async loads - `useReducer` depends on `dispatch` to move forward to the next state, while `useEffectReducer` depends on the effects.\n\nTherefore instead of calling `dispatch` in my test script like in [this Stackoverflow discussion on how to test useReducer](https://stackoverflow.com/a/59277748/10951931), I should mock `someAsyncMethod`'s resolved value.\n\n### Challenge 3: Cannot mock readonly imported property `someAsyncMethod`\n\nSolved with this reading: [How To Spy On An Exported Function In Jest](https://www.chakshunyu.com/blog/how-to-spy-on-a-named-import-in-jest/)\n\n### Challenge 4: Mock error tracking function\n\n```\nconst mockTrackError = jest.fn()\njest.mock('../errorTracking', () => ({\n  useTrackError: jest.fn().mockReturnValue(mockTrackError), 👈 `mockTrackError` will be evaluated upon `jest.mock`\n}))\n```\n\nThis is getting:\n\n```\n ReferenceError: Cannot access 'mockTrackError' before initialization\n```\n\nFixed block:\n\n```\nconst mockTrackError = jest.fn()\njest.mock('../errorTracking', () => ({\n  useTrackLSPAYError: jest.fn().mockImplementation(() => mockTrackError), 👈 `mockTrackError` will only be evaluated when test executes\n  TAG: { cardNotPresentPayment: 'card-not-present' },\n}))\n```\n\nMore details see: [FIX JEST MOCK 'CANNOT ACCESS BEFORE INITIALIZATION' ERROR](https://blog.bam.tech/developer-news/fix-jest-mock-cannot-access-before-initialization-error)\n\n### Challenge 4: Mock method seems to be using the implementation from the prior test suite\n\nTwo points to be aware of:\n\n- `jest.clearAllMocks`: Clears the **mock.calls and mock.instances** properties of all mocks. Equivalent to calling .mockClear() on every mocked function. It does NOT clear the implementation mock!\n- If you do `mockImplementationOnce` in `beforeEach`, you kind of need to make sure it really gets called once in each test. The \"leftover\" implementation will be carried out to the next test suite - which was why I got the bizarre behaviour that the test outcome changed when I swapped two test suites.\n","tags":["React","Testing"]},{"title":"JSON.parse(data) in TypeScript - do not expect type check","url":"/2022/02/14/JSON-parse-data-in-TypeScript-do-not-expect-type-check/","content":"\nI assumed this block would do some type check for me.\n\n```typescript\n// types\n\n// snippet including only public info from LSPAY Payment API\ntype PaymentStepEvent = PaymentStepSetup | PaymentStepData\n\ninterface PaymentStepSetup {\n  step: PAYMENT_STEPS.SETUP\n  // ... some omitted fields\n}\n\ninterface PaymentStepData {\n  step: PAYMENT_STEPS.DATA\n  // ... some omitted fields\n}\n\nenum PAYMENT_STEPS {\n  DATA = 'DATA',\n  SETUP = 'SETUP',\n}\n```\n\n```typescript\n// event handling logic\nlet eventData: PaymentStepEvent\ntry {\n  eventData = JSON.parse(event.data)\n} catch (e) {\n  // some error handling\n}\n```\n\nThe `eventData: PaymentStepEvent` looked like assigning the result of `JSON.parse(event.data)` to `eventData` would pass some type checking, but the sad truth is not.\n\nExecute this block in playground:\n\n```typescript\nlet data = '{\"dog\": \"DATA\"}' // or \"{\\\"step\\\": \\\"DATA\\\"}\"\nlet eventData: PaymentStepEvent\ntry {\n  eventData = JSON.parse(data)\n  console.log(eventData) // 👉 {\"dog\": \"DATA\"}\n  console.log(\n    'is recognized gateway message event: ' +\n      !!isRecognizedGatewayMessageEventData(eventData)\n  ) // 👉 \"is recognized gateway message event: false\"\n} catch (e) {\n  console.log('error parsing!')\n}\n\nfunction isPaymentStepEvent(eventData: PaymentStepEvent): boolean {\n  return eventData.step && Object.values(PAYMENT_STEPS).includes(eventData.step)\n}\n```\n\nThe `isPaymentStepEvent` serves as a type guard, which has to be implemented by user. TypeScript adds no type checking at run time at all.\n","tags":["TypeScript"]},{"title":"Nice readings grab bag","url":"/2022/02/02/Nice-readings-grab-bag/","content":"\n- [Don't use else (and how Swift/Rust is guiding us not to use else with `guard`)](https://medium.com/@jamousgeorges/dont-use-else-66ee163deac3)\n- [Testing library intro - why it is different from Enzyme by testing DOM behaviour instead of component implementation details](https://testing-library.com/docs/react-testing-library/intro/)\n- [Avoid soul-crushing components - composite component pattern](https://epicreact.dev/soul-crushing-components/)\n","tags":["Readings","Grab bag"]},{"title":"Using feature flag to decouple deploy from release","url":"/2022/01/25/Using-feature-flag-for-staggered-release-test/","content":"\nWe have three environments that can be used for testing:\n\n- Local dev environment, running all containers via Docker Desktop with a docker compose file\n- Proxy dev environment, run the service we are actively using locally, and all requests to other services are proxied to production.\n- Production environment, which is live to all our retailers.\n\nThere are some cases where there's a gap between proxy dev environment and production environment, and the feature cannot be tested unless deployed in production. I was lucky enough to have one of these: I am implementing an origin validation security feature to ensure the message events come from the legitimate payment provider. However, the integrated payment provider is only happy to work with our production domain.\n\nAfter working with our QA engineer, the solution is: use a feature flag in front of the feature!\n\nFeature flags help **decouple deployment from release** letting you manage the full lifecycle of a feature.\n","tags":["Testing","Live production"]},{"title":"Some good git tricks I picked up from team","url":"/2022/01/18/Some-good-git-practices-I-just-picked-up-from-team/","content":"\n## Use `git add -p` to interactively decide whether to stage a change\n\nThis is to avoid unexpectedly staging changes that's not relevant - which I've done quite a lot via `git add .`\n\n## Use `git rebase master -i` to interactively rebase onto master before submitting a PR\n\n## Use `git config --global core.editor <editor>` to change editor\n\nEnough wrestling with vim :D\n\n## Use `git diff HEAD~` to review what has been committed\n\n## Use `git commit --amend --no-edit` to amend command without changing commit message\n","tags":["git"]},{"title":"Functional option pattern in Go","url":"/2022/01/11/Functional-option-pattern-in-Go/","content":"\nReference: https://www.sohamkamani.com/golang/options-pattern/\n\n## Is it tied directly to a Go feature, or a design pattern based on the basics?\n\nIt's a design pattern based on the basics features:\n\n- First class function\n- Variadic function: `func NewHouse(opts ...HouseOption)`\n\n## What's the user case that makes it necessary?\n\nWhen we have a set of default values for a struct that we are generally happy with, and then variable number of these values might need to be different upon construction.\n","tags":["Go","Design Patterns"]},{"title":"Event-driven architecture","url":"/2021/10/27/Event-driven-architecture/","content":"\nWikipedia:\n> Event-driven architecture (EDA) is a software architecture paradigm promoting the production, detection, consumption of, and reaction to events.\n\n- Event driven, generally asynchronous\n- Debugging: can be hard for event driven architecture, you would have to move away from your traditional debugging tools.\n\n\n\n","tags":["AWS","Architecture"]},{"title":"The -race flag in Go","url":"/2021/10/24/The-race-flag-in-Go/","content":"\n## Race condition\n- A race condition occurs when two or more threads can access shared data and they try to change it at the same time.\n- Problems often occur when one thread does a \"check-then-act\" (e.g. \"check\" if the value is X, then \"act\" to do something that depends on the value being X) and another thread does something to the value in between the \"check\" and the \"act\".\n- In order to prevent race conditions from occurring, you would typically put a lock around the shared data to ensure only one thread can access the data at a time. \n\n## -race\nEnable data race detection.\nSupported only on linux/amd64, freebsd/amd64, darwin/amd64, windows/amd64,\nlinux/ppc64le and linux/arm64 (only for 48-bit VMA).\n\nSee more in [go cmd documentation](https://pkg.go.dev/cmd/go)\n\n## Test the effect\n\n```go\n// test.go\n\npackage main\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\nvar counter int\n\nfunc main() {\n\twg := sync.WaitGroup{}\n\twg.Add(2)\n\tgo addToCounterTenTimes(1, &wg)\n\tgo addToCounterTenTimes(2, &wg)\n\twg.Wait()\n}\n\nfunc addToCounterTenTimes(add int, wg *sync.WaitGroup) {\n\tfor i := 0; i < 10; i++ {\n\t\tcounter += add\n\t\tfmt.Println(\"counter:\", counter)\n\t}\n\twg.Done()\n}\n\n```\n\n```\ngo run -race test.go \n```\n\nOutput:\n```\ncounter: 1\ncounter: 2\ncounter: 3\ncounter: 4\ncounter: 5\ncounter: 6\ncounter: 7\ncounter: 8\ncounter: 9\ncounter: 10\n==================\nWARNING: DATA RACE\nRead at 0x00000122fcb0 by goroutine 8:\n  main.addToCounterTenTimes()\n      /Users/anran/Desktop/AnranBlog/source/_posts/test.go:20 +0x4a\n\nPrevious write at 0x00000122fcb0 by goroutine 7:\n  main.addToCounterTenTimes()\n      /Users/anran/Desktop/AnranBlog/source/_posts/test.go:20 +0x66\n\nGoroutine 8 (running) created at:\n  main.main()\n      /Users/anran/Desktop/AnranBlog/source/_posts/test.go:14 +0xda\n\nGoroutine 7 (finished) created at:\n  main.main()\n      /Users/anran/Desktop/AnranBlog/source/_posts/test.go:13 +0xaf\n==================\ncounter: 12\ncounter: 14\ncounter: 16\ncounter: 18\ncounter: 20\ncounter: 22\ncounter: 24\ncounter: 26\ncounter: 28\ncounter: 30\nFound 1 data race(s)\nexit status 66\n```\n\n> Note that I had to put `-race` after `go run` to make it work. I developed a habbit of putting all flags at the end working with Docker, though this would not work - this will make `-race` an argument past to `test.go` instead of `go run`.","tags":["Go","MIT 6.824"]},{"title":"Lint fun 😼 ","url":"/2021/10/19/Lint-fun-😼/","content":"\nGot a handful of lint errors in a project to resolve. This would be an interesting JS best practice learning experience.\n\n# [@typescript-eslint/no-this-alias](https://github.com/typescript-eslint/typescript-eslint/blob/master/packages/eslint-plugin/docs/rules/no-this-alias.md)\n\n## Origintal code\n\n```javascript\ncomponentDidMount() {\n    let my = this\n    this.timeHandle = setTimeout(() => {\n        my.setState(prev => {\n            const state = {...prev}\n            state.timeSwitch = true\n            return state\n        })\n    }, this.props.timeLimitInSeconds * 1000)\n}\n```\n\n## Why this is an issue\n\nAssigning a variable to this instead of properly using arrow lambdas may be a symptom of pre-ES6 practices or not managing scope well.\n\n## Fix\n\nUse () => arrow lambdas, as they preserve this scope for you:\n\n```javascript\ncomponentDidMount() {\n    this.timeHandle = setTimeout(() => {\n        this.setState(prev => {\n            const state = {...prev}\n            state.timeSwitch = true\n            return state\n        })\n    }, this.props.timeLimitInSeconds * 1000)\n}\n```\n\n# [react/jsx-no-target-blank](https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/jsx-no-target-blank.md)\n\nUsing `target=_blank` links with no `rel=noreferrer` attribute is a security hazard in older browsers.\n\n# [@typescript-eslint/ban-types]()\n\n## `{}` type\n\n[This comment](https://github.com/typescript-eslint/typescript-eslint/issues/2063#issuecomment-675156492) gives a clear explanation of why disallowing `{}` is a rule default.\n\n**The type {} doesn't mean \"any empty object\", it means \"any non-nullish value\".**\nUnfortunately, there's no type in TS that means \"an empty object\".\n\nSee the link for the alternative (`type EmptyObject = Record<string, never>`) and the safe use case of `{}` (intersection type; as in React `type Props={}`)\n\n# [@typescript-eslint/no-namespace](https://github.com/typescript-eslint/typescript-eslint/blob/master/packages/eslint-plugin/docs/rules/no-namespace.md)\n\n> Custom TypeScript modules (module foo {}) and namespaces (namespace foo {}) are considered **outdated** ways to organize TypeScript code. ES2015 module syntax is now preferred (import/export).\n\nIn this project, `namespace` is used to organise some constants:\n\n```\n// constants.ts\n\nexport namespace PageName {\n    export const Loading = 'Loading';\n    export const InitialAuthorization = 'InitialAuthorization'\n}\n```\n\nSo I just need to replace it with an object:\n\n```\n// constants.ts\n\nexport const PageName = {\n    Loading: 'Loading',\n    InitialAuthorization: 'InitialAuthorization'\n}\n```\n","tags":["TypeScript","Web development"]},{"title":"MIT 6.824 - Distributed Systems - Lecture 1","url":"/2021/10/12/MIT-6-824-Distributed-Systems-Lecture-1/","content":"\n# Reading\n\n## Paper: [MapReduce](http://nil.lcs.mit.edu/6.824/2020/papers/mapreduce.pdf)\n\n`map` and `reduce` are just what you would think they are, as in `Javascript` 😊. You write functional code and the run-time system takes care of the details of：\n\n- partitioning the _input_ data\n- _scheduling_ the program’s execution across a set of machines\n- handling machine _failures_\n  - achieve availability and reliability of storage on machines' local disks via **replication**\n  - achieve fault tolerance via **re-execution**: take advantage of the functional model of map and reduce operations\n- managing the required inter-machine _communication_\n\n### How distribution is achieved\n\n- The `Map` invocations are distributed by partitioning input data into a set of _M_ splits\n- The `Reduce` invocations are distributed by y partitioning the intermediate key space into _R_ pieces using a partitioning function (e.g., _hash(key) mod R_). The number of partitions (_R_) and partitioning function are specified by the user.\n\n### Staggler issue\n\nAs a map reduce execution gets to an end, the last few tasks might take very long for reasons like other tasks being scheduled on the machine. This issue is alleviated by back-up mechanism: the master will schedule back-up executions for the last few in-progress tasks, and the task will be considered complete whether the primary or the back-up execution is completed.\n\n### Use case: large-scale production indexing\n\nGoogle migrated the document indexing workload to MapReduce. Benefits in the outcome:\n\n- Alterability - separation of concerns\n- Acceptable performance and therefore no need to mix conceptually unrelated computations together to avoid extra passes over the data.\n- Smaller code base - 3800 lines of C++ to 700\n\n### Related work\n\n#### Restricted programming models\n\n> e.g. An associative function can be computed over all prefixes of an N element array in log N time on N processors using parallel prefix computations.\n\nAdvantage of MapReduce over these restricted models: Fault-tolerant that scales to up to thousands of processes v.s. small scale and programmers take care of machine failure.\n\n# Lecture\n\n## Why distributed system?\n\n- Paralism\n- Fault Tolerance\n- Physical: naturally distributed\n- Security/ Isolation\n\n## Challenges\n\n- Concurrency\n- Partial failure\n- Performance (achieving the performance you think you deserve): Original reason is for higher performance via paralism, but obtaining the 100* speed with 100* machines is hard.\n\n# Assignment\n\nTask: Implement the \"master\"/\"cordinator\" and \"worker\" process for MapReduce.\n\n## Interesting observations from the starter code\n\n### build in `--plugin` mode, find `Map` and `Reduce` functions via `loadPlugin`\n\nNice to know that there is a [`plugin` go package](https://pkg.go.dev/plugin) taking care of the loading and symbol resolution of Go plugins.\n\n### The `timeout` Linux command\n\ntimeout is a command-line utility that runs a specified command and terminates it if it is still running after a given period of time.\n\nGet the Mac alternative:\n\n```sh\nbrew install coreutils\n```\n\n## Implementation\n\n### Worker's steps\n\nStep 1: Request a job from the coordinator\nStep 2: Open the file and read in the content\nStep 3: Do the Map or the Reduce job\nStep 4: Write result into file\nStep 5: Send output file name back to coordinator\n\n### Coordinator's RPC handlers\n\nHandler 1: `HandleJobRequest`: Find an indle task and put the information (job type, filename) into the response.\nHandler 2: `HandleJobFinishReport`: Update the task state. Decide if all `Map` tasks have finished; decide if all `Reduce` tasks have finished and update the `Done` return value.\n\n### Questions asked\n\n#### Question 1: Can a worker do either map or reduce?\n\nYes. It is determined by the reply from the job request.\n\n##### Question 2: How is `nReduce` and `ihash` used by Worker?\n\nAccording to the MapReduce paper:\n\n> Reduce invocations are distributed by partitioning the **intermediate key space** into `R` pieces using a partitioning function (e.g.,hash(key) mod R). The number of partitions (R) and the partitioning function are specified by the user\n\n- R: nReduce\n- Intermediate key space: the collection of all keys produced by map\n- hash function: ihash\n- who's responsible for the distribution: When the map functions write their results\n\n#### Question 3: Who is responsible for the distribution of the reduce invocations?\n\nThe worker, when it has invoked the `mapf` function and writes the outcomes to the intermediate files.\nIt should decide which file to write the intermediate key-value pairs according to the result of `ihash`.\n\n#### Question 4: How does the Coordinator decide which map job to assign to a worker?\n\nAccording to the MapReduce paper:\n\n> The Map invocations are distributed across multiple machines by automatically partitioning the input data into a set of M splits.\n\nAccording to the implementations of `Map` in the `mrapps`, they are passed two arguments `filename string` and `content string`. It might not always use these two arguments - for example, the word count app only uses the `content`. Therefore, I think I can assume that each map invocation will only involve one file, and the `Worker` will be responsible for opening the file and reading the content before invoking the `Map` function.\n\n#### Question 5: How do I manage the jobs in the Coordinator?\n\n- `[]Job`: hard to query by file name\n- `map[string]Job` with filename as the key: hard to find incomplete jobs, not possible for reduce jobs as there are up to `mMap` intermediate files to be processed by each task.\n- `map[int]Job` with job number as the key - this is what I did for the first iteration. The Coordinator maintains two such maps, `reduceJobs` and `mapJobs`. - there turned out to be a better solution, more on this later.\n\n## Performance issue\n\nNow that I have the first iteration running (Yes! 🎉), the performance issue becomes notable. Yihao got his version finishing one invocation of `test-mr.sh` within 1 minutes while I could finish a song waiting for it. Here are some design decisions that I noted down after some discussion with him:\n\n### Rethinking Question 5 - Coordinator's management of the jobs\n\nCurrently, my Coordinator needs to loop through the whole map of jobs to find an available job or to conclude that there's no suitable job to assign. I originally designed it this way because I was thinking of the \"resetting\" of jobs back to idle state and thought it would be hard to keep a collection of idle jobs. However, it was actually not that bad. All I had to do was to maintain a slice/queue of jobs to be assigned, and if the job did not get finished in time, I can still append it back. On the other end, the already scheduled jobs could be maintained in a map with keys of job numbers. This makes the querying of scheduled job O(1) if the job gets finished/ did not finish in time.\n\n### Rethinking writing to files: parallel writing\n\nThe map tasks need to write into intermediate files, and the reduce tasks need to write to output files.\nCurrently I maintain a map of json encoders, and loop through each key-value pair, and write into the right encoders. This should be largely optimized by dividing the content in-memory and then writing into each file in parallel.\n\n### Rethinking reading from files: use glob\n\nThe reduce tasks need to read from intermediate files formatted `mr-int-X-Y` where `Y` is the current reduce task number and `X` is number of any the map tasks that writes into it. Before I looped through all map task numbers and read their content. Two problems: performance, and the map job might have not produced kv for this specific reduce task therefore unnecessary error handling.\n\nUsing glob and write all files in via glob formatted file name would be more robust - as long as there's no side effect of `mapf` and `reducef` producing some other files that follow the same format :)\n\n## Don't communicate by sharing memory; share memory by communicate\n\nUse a channel to manage unscheduled jobs. It has these advantage:\n\n- It is blocking. The rpc method caller will just be blocked instead of receiving an idle response.\n- It is thread safe. No need to apply lock on the slice of job, or each single job.\n\n## Use channel to manage completed jobs\n\nI really like this\n\n```go\ngo func() {\n\t\tfor i := 0; i < nMap; i++ {\n\t\t\t<-c.jobsCompleted\n\t\t}\n\t\tc.mu.Lock()\n\t\tc.state = doingReduce\n\t\tc.mu.Unlock()\n\t\tgo c.startReduceJobScheduling()\n\t\tfor i := 0; i < nReduce; i++ {\n\t\t\t<-c.jobsCompleted\n\t\t}\n\t\tc.mu.Lock()\n\t\tc.state = done\n\t\tc.mu.Unlock()\n\t}()\n```\n\n## Now the last ambitious goal: remove all locks\n\nSame philosophy: I am using locks to control access to a piece of memory shared by multiple go routines - i.e. communicate by sharing memory.\nIf I could allow the go routines to communicate with a data access go routine via a channel, it is then natively thread safe.\n\nWhat I needed to do is to have a long-running go routine to monitor channels and handle requests from these channels to manipulate map data:\n\n```\nfunc (c *Coordinator) manageScheduledJobsMap() {\n\tfor {\n\t\tselect {\n\t\tcase j := <-c.jobsToComplete:\n\t\t\t_, going := c.jobsInProgress[*j]\n\t\t\tif going {\n\t\t\t\tdelete(c.jobsInProgress, *j)\n\t\t\t\tc.jobsCompleted <- j\n\t\t\t}\n\t\tcase j := <-c.jobsToReschedule:\n\t\t\t_, going := c.jobsInProgress[*j]\n\t\t\tif going {\n\t\t\t\tdelete(c.jobsInProgress, *j)\n\t\t\t\tc.jobsToSchedule <- j\n\t\t\t}\n\t\tcase j := <-c.jobsToRecordScheduled:\n\t\t\tc.jobsInProgress[*j] = true\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\t}\n}\n```\n","tags":["Go","MIT 6.824","Distributed Systems"]},{"title":"Why I gave up Ant Design","url":"/2021/10/07/Why-I-gave-up-Ant-Design/","content":"\nI had my taste of Ant Design and got rid of it from my project three weeks later. Here are some issues that I think would help keep me using it longser if improved:\n\n## API documentation not comprehensive enough\nThis is the key reason I decided ant design was not for me. For example, I was using the [Antd Menu](https://ant.design/components/menu/) component and was trying to add handler for menu selection change. The documentation site goes (as of 7th Oct 2021):\n```\nonSelect\t\n- Called when a menu item is selected\t \n- function({ item, key, keyPath, selectedKeys, domEvent })\n```\n\nThen I got warning that the `item` argument has been depracated when using it; I did not find example/documentation on what `keyPath` and `selectedKeys` look like and had to log these arguments myself to get a better understanding. By comparison, Material-ui provides much more extended examples and more detailed API docs.\n\n## Lack of customizability\nThere are multiple places that I find it, if not impossible, very hard to customize some components. One example that got me stuck was to change the behaviour of finished [Steps](https://ant.design/components/steps/). \n\n## Look\nI still prefer the look of material style up to now :)","tags":["Web Development","Component libraries"]},{"title":"Some debug notes when you have to work with 3-year-old TypeScript","url":"/2021/09/27/Some-debug-notes-when-you-have-to-work-with-3-year-old-Typescript/","content":"\nI was wrestling with a feature that refused to run locally and refused to build. It was originally running on Helix Container but people knowing how to run it have all left - let's forget about that for now and get it running in a demo app based on a newer (but unfortunately not the newest) create-react-app environment.\n\n## @babel/plugin-transform-typescript\n\nThe fact that the TypeScript code was two-year-old made babel most upset, for example, babel's tolerance to `namespace` has changed:\n\n```\nallowNamespaces\nboolean, defaults to true.\n\nHistory\nVersion\tChanges\n- v7.5.0\tAdded allowNamespaces, defaults to false\n- v7.13.0\tdefaults to true\nEnables compilation of TypeScript namespaces.\n```\n\nThe babel version from the demo app was 7.12 and I had to enable the `allowNamespaces` config as the code was using the namespace feature.\n\n## craco plugin to allow customizing `.bablerc`\n\nI found this plugin: [craco-use-babelrc](https://github.com/jackwilsdon/craco-use-babelrc) to utilize `.bablerc` and customize babel.\n\n## Create `.babelrc`\n\nContent of `.babelrc`:\n\n```\n{\n  \"presets\": [\n      \"@babel/react\",\n      \"@babel/env\"\n  ],\n  \"plugins\": [\n      \"@babel/plugin-transform-typescript\", {\n          \"allownamespaces\": true\n      }\n  ]\n}\n```\n\nSee [plugin options](https://babeljs.io/docs/en/plugins#plugin-options) for reference.\n","tags":["TypeScript"]},{"title":"Authenticate golang http client with certificate","url":"/2021/09/14/Authenticate-golang-http-client-with-certificate/","content":"\nThis is a challenge at work: in order to talk with a QA service, I have to provide a certificate that makes F5 happy.\n\n## What is a x509 certificate and how it works\nX.509 certificates are used in many Internet protocols, including TLS/SSL\n\n## x509 certificate signed by unknown authority\nSee [this post](https://writeabout.net/2020/03/25/x509-certificate-signed-by-unknown-authority/) on the reason and fix to this error.\n\n## Apply client certificate: `crypto/tls`\n\n- Serving HTTP with TLS: see [this example from denji](https://github.com/denji/golang-tls) and [this example from crypto/tls pkg](https://pkg.go.dev/crypto/tls#example-X509KeyPair-HttpServer)\n- Calling HTTP with TLS: [http.Transport](https://pkg.go.dev/net/http#Transport) is an implementation of RoundTripper that supports HTTP, HTTPS, and HTTP proxies (for either HTTP or HTTPS with CONNECT). To make a call to an HTTPS endpoint with self-provided key:\n```\ntlsConfig := &tls.Config{\n    Certificates: []tls.Certificate(cert),\n}\ntlsConfig.BuildNameToCertificate()\ntransport := &http.Transport{TLSClientConfig: tlsConfig}\nclient := &http.Client{Transport: transport}\n```\n\n## Decode pfx file: pkcs12\n`pkcs12` is intended for decoding P12/PFX-stored certificates and keys for use with the `crypto/tls` package.\n\n","tags":["Go","Security"]},{"title":"Less and Cacro with Ant Design","url":"/2021/09/10/Less-with-Ant-Design/","content":"\n# Less with Ant Design\nInteresting that I never really used LESS while call myself a web developer. Learning Ant Design gives me a good opportunity to explore it as lots of Ant design's style customizing is done by LESS.\n\n## What is LESS\n- Leaner Style Sheet\n- Backwards compatible with CSS\n- It basically bakes in programming capacities into CSS.\n\n### Some useful features that solves problems I had before\n- Nesting: No more need to combine selectors, and make the CSS structure consistent with my HTML\n- Mixins: No more duplicate styling\n- Variables: simpler to write than CSS variables\n\n## Cacro and Ant Design Plugin\nCacro - Create React App Configuration Overrid\n\n> Get all the benefits of create-react-app and customization without using 'eject' by adding a single craco.config.js file at the root of your application and customize your eslint, babel, postcss configurations and many more.\n\nAnt design allows user customisation of styles via a cacro plugin - `craco-antd`. This plugin helps overriding theme variables in `~antd/dist/antd.less` imported into my `less` files. Behind the plugin is the variable overriding feature of `less-loader`.\n\n## Trouble shooting: my overriding takes no effect 🤔\n\n<img src=\"overriden.png\" width=\"400\" alt=\"overriden\"/>\n\nIt looks that the original setting is still loaded and overrules the value I set in `craco.config.js`.","tags":["Web Development","CSS","Web Beautify","Less"]},{"title":"Rethink Go Design Patterns","url":"/2021/09/10/Rethink-Go-Design-Patterns/","content":"\nI ran into an a blog [All Design Patterns in Go (Golang)](https://golangbyexample-com.cdn.ampproject.org/c/s/golangbyexample.com/all-design-patterns-golang/amp/) but feel that it does not have to be this OOP and this complicated in Go. So I will try to re-implement these patterns in a more Go-ish way.\n\n# Behavioural Design Patterns\n\n## Observer pattern\n\n- [Original implementation](https://golangbyexample.com/observer-design-pattern-golang/)\n- [My implementation](https://github.com/niuniuanran/GoDesignPatternRethink/blob/master/observer/my.go)\n\nGo has very good support for functional programming, making a lot of object-ish wrappers unnecessary.\nInstead of having the `observer` interface receiving a function, I can directly keep track of the functions to be invoked at events:\n\n```\ntype item struct {\n\tobserverFuncs map[string]func(string)\n\tname          string\n\tinStock       bool\n}\n```\n\nThe functions to notify can be directly be registered to and invoked by the subject:\n\n```\nfunc (i *item) register(id string) {\n\tif i.notifyFuncs == nil {\n\t\ti.notifyFuncs = map[string]func(string){}\n\t}\n\ti.notifyFuncs[id] = customerNotificationFunc(id)\n}\n\nfunc (i *item) notifyAll() {\n\tfor _, notifyFunc := range i.notifyFuncs {\n\t\tnotifyFunc(i.name)\n\t}\n}\n```\n\n## Strategy pattern\n\n- [Original implementation](https://golangbyexample.com/strategy-design-pattern-golang/)\n- [My implementation](https://github.com/niuniuanran/GoDesignPatternRethink/tree/master/strategy/my)\n\nWhile I'm here let me implement the caching strategies in full.\n\n- Extended the `item` data type to keep track of the `accessCount`, `lastAccessed` time and `lastUpdated` time\n- Implemented the algos - although now not wrapped in an interface but passed in as a function.\n","tags":["WIP","Go","Design Patterns"]},{"title":"Quick git note: git restore","url":"/2021/09/10/Quick-git-note-git-restore/","content":"\nI wanted to discard all changes I accidently did on the vendor folders of my go programme.\n\n\nMy file structure looks like this:\ngo/src\n├───unauth_plugin\n│   ├───vendor\n│   └───.go files\n├───auth_plugin\n│   ├───vendor\n│   └───.go files\n├───logging_plugin\n│   ├───vendor\n│   └───.go files\n\nDoing this from `src` discards all changes done within the three vendor folders:\n```\ngit restore ./*/vendor\n```\n\nNote that this is also available:\n```\n./**/vendor\n```\n\nwhere ** means all folders RECURSIVELY from this point in the path.","tags":["git"]},{"title":"Visual Studio tip - capture groups and replacement patterns","url":"/2021/09/10/Regular-expression-search-and-REPLACE/","content":"\nI was using `tt.Error(fmt.Sprintf(\"...\", ...))` in many places around my go test scripts, then I realized there's a more elegant way, `tt.Errorf(\"...\", ...)`\n\nI want to do a global search and replace, and the easiest way would be to use regular expression to match both sides of the quote. Great to know that this is indeed an option if I do:\n- Search for `Error\\(fmt.Sprintf\\((.+)\\)\\)`\n- Replace with `Errorf\\($1\\)`\n\nThe VS code documentation listed more examples:\n- [Capture groups and replacement patterns](https://docs.microsoft.com/en-us/visualstudio/ide/using-regular-expressions-in-visual-studio?view=vs-2019#capture-groups-and-replacement-patterns)\n- [Named capture groups](https://docs.microsoft.com/en-us/visualstudio/ide/using-regular-expressions-in-visual-studio?view=vs-2019#named-capture-groups)\n","tags":["Visual Studio","Regex"]},{"title":"Deploy spy api to AWS ECS-Fargate","url":"/2021/08/11/Deploy-spy-api-to-AWS-ECS-Fargate/","content":"\n# Referenses\n- [Pushing a Docker image](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html)\n- [Getting started with the Amazon ECS console using AWS Fargate](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started-fargate.html)\n\n# Result on Fargate\n<img src=\"fargate-result.png\" alt=\"fargate-task\" width=\"500px\">\n\n# Connect github page to api\n\n```\nxhr.js:177 Mixed Content: The page at 'https://niuniuanran.github.io/en/new-room' was loaded over HTTPS, but requested an insecure XMLHttpRequest endpoint 'http://34.221.65.76:8080/create-room'. This request has been blocked; the content must be served over HTTPS.\n```\n\nThe problem is I don't want to pay for a domain name 😂 So I want to find a managed PAAS to take care of it for me.\n\n# The other option\nThe next day I received a subscription email introducing that Lightsail now also supports containers.","tags":["AWS","Go","who-is-spy","Docker"]},{"title":"Hold my game together when people leaves room","url":"/2021/07/20/Hold-my-game-together-when-people-leaves-room/","content":"\n## My challenge\nThe game is looking good and gets us playing - as long as everyone behaves. By behaving I mean staying at the game web page and never allowing their device to lock the screen - otherwise the game will enter a random state and start sending people inconsistent instructions. \nMy friends soon proved that my expectation on my users are way too high. They have to Google for the word from time to time, and they have to put their phone on their belly more often :D\nSo it is up to me to make the game survive these.\n\n## The reason for the random behaviour\nCurrently, the game is driven by the updates of player states. Here's a code snippet: \n\n```\n// game.go\n\nfunc (room *Room) runTalkRound() bool {\n\troom.setAllAlivePlayersToState(PlayerListeningState)\n\talivePlayers := room.getAlivePlayerPointersInRoom()\n\tif len(alivePlayers) < 1 {\n\t\troom.setAllPlayersToState(PlayerIdleState)\n\t\troom.broadcastPlayersState(\"No alive players in room\", \"\", AlertTypeWarning)\n\t\treturn false\n\t}\n\n    rand.Seed(time.Now().UnixNano())\n\tstartFrom := rand.Intn(len(alivePlayers))\n\ti := startFrom\n\tfor {\n\t\talivePlayers[i].State = PlayerTalkingState\n\t\troom.broadcastPlayersState(\"\", fmt.Sprintf(\"%s's turn to talk\", alivePlayers[i].Nickname), \"\")\n\t\twaitForState(func() bool { return alivePlayers[i].State == PlayerTalkFinishedState })\n\t\talivePlayers[i].State = PlayerListeningState\n\t\ti++\n\t\tif i == len(alivePlayers) {\n\t\t\ti = 0\n\t\t}\n\t\tif i == startFrom {\n\t\t\treturn true\n\t\t}\n\t}\n}\n\nfunc waitForState(check func() bool) {\n\tfor {\n\t\ttime.Sleep(time.Second)\n\t\tif check() {\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n\nIf a player leaves the room (i.e. the player's pointer gets removed from the `room.players` map) in the middle of the talking round, the `alivePlayers` slice still has the reference to this player, but the game might be stuck waiting for this player to change state.\n\n## Available tools\n\nTo solve this problem, I need to know what relevant tools are available. \n\n### Reconnecting WebSocket\nOne problem for now is that when a player loses their connection, they lose it for good. The solution would be to add retries upon `onclose`. Luckily there's already wrapper packages that provide this functionality on top of `WebSocket`. [reconnecting-websocket](https://www.npmjs.com/package/reconnecting-websocket) looks like what I need.\n\n### go `context` package\nAnother challenge I face is the management of ongoing games when an unexpected event happens - for example, when a player quits unexpectedly. This means I need something like `CancellationToken` in C# - luckily, there is [context package](https://pkg.go.dev/context#pkg-index) in go that takes care of cancellation signals.\n\n[This](https://www.sohamkamani.com/golang/context-cancellation-and-values/) is a thorough walkthrough of what it is like to use go context.\n\n## Actions taken\n\n### Step 1: Reconnecting WebSocket\nAt this moment, there's no attempt to reconnect the player if the websocket connection is lost. The first step towards a more robust game experience would be an automatic connection recovery. \n\n### Step 2: Manage offline players's state\nAll buggy behaviours start from me removing the player pointer whenever their connection is lost. I should allow the player to be in an \"offline/appears away\" state, which can:\n- interact with other actively connected players, as dictated by the server\n- later be recovered to an active state, if a request to re-connect is received.\n- used to calculate the game result, if for example, the spy is offline and there's no point keep talking.\n\nA key to implementing this update is to dig deeper into `player.readPump` and `player.disconnect`:\n\n```\n// readPump reads new messages from a player's connection\nfunc (player *Player) readPump() {\n\tdefer func() {\n\t\tplayer.disconnect()\n\t}()\n\n\tplayer.conn.SetReadLimit(maxMessageSize)\n\tplayer.conn.SetReadDeadline(time.Now().Add(pongWait))\n\tplayer.conn.SetPongHandler(func(string) error { player.conn.SetReadDeadline(time.Now().Add(pongWait)); return nil })\n\n\t// Start endless read loop, waiting for messages from player\n\tfor {\n\t\t_, jsonMessage, err := player.conn.ReadMessage()\n\t\tif err != nil {\n\t\t\tif websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway, websocket.CloseAbnormalClosure) {\n\t\t\t\tlog.Printf(\"unexpected close error: %v\", err)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tplayer.handleNewMessage(jsonMessage)\n\t}\n}\n```\n\n```\nfunc (player *Player) disconnect() {\n\tplayer.room.unregister <- player\n\tclose(player.send)\n\tplayer.conn.Close()\n}\n```\n\nBefore making the change, whenever the player is unregistered from the room, the player is removed from the room's `players` map; now they are not, so it is my responsibility to ensure that the server will now try to send message to the player's websocket connection - otherwise the server will crash.\n\nThis is implemented in [this commit](https://github.com/niuniuanran/WhoIsSpy/commit/fa5be118d1e1873c6208cedcd35277ed66ad74f2).\n\n\n### Step 3: Handle the game state change if a player appears away/ comes back\nThe gaming experience would have been damaged when a player leaves unexpectedly when:\n- no one knows if the player that left is a spy\n- when the player joins the room again, they will be recognized as a new user. They will re-trigger a game state refresh, causing everyone to receive a new word, and the current game is lost.\n\nTherefore, the server should take over the responsibility to:\n- do a check to see if the game can still go on without the player that already left\n- keep track of the player that left, and allow them to come back in a non-destructive way\n- stop new players from joining a room that has a game running.\n\n\n#### Step 3.1 Make state check functions more expressive\nTo make ground for the change to handle player states, I [made the state check functions more expressive with variadic parameter](https://github.com/niuniuanran/WhoIsSpy/commit/47fb392935858a27eec1cc2af1f969621d5d4082). Now I can easily check for multiple player states at the same time, or wait for one of multiple conditions to turn true before the game can continue - instead of being stuck with a single check.\n\n#### Step 3.1 Find out a way to update game state when a player appears away/ comes back\n\n##### Option 1\nThe current implementation has a goroutine to run the game. Each \"blocking\" stage (e.g. waiting for all players to vote) waits for the players' state to update, and now it is important that the `offline` state gets checked too.\n\nLet me start from the `waitForState` function and allow it to handle special cases.\n```\nfunc waitForState(check func() bool) {\n\tfor {\n\t\ttime.Sleep(time.Second)\n\n\t\t// TODO Add special case and handler function\n\n\t\t// TODO check if someone has been away\n\n\t\tif check() {\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n\n##### Option 2\nCreate a separate goroutine to monitor player that appears away - long running while game status is active, and communicates with other goroutine via game status.\n\n\n##### Implementation\nI decided on option 2 - a long running health check routine, as this is more generic and more resilient to different timing of player leaving the room. See [this commit that adds healthcheck](https://github.com/niuniuanran/WhoIsSpy/commit/1ca648cd9889fdfd845cd6ffb9c4fece1337f553) and [this commit that tidies up at end of game](https://github.com/niuniuanran/WhoIsSpy/commit/d4d73f3ad5a15734cff2fc27921238d22e29d152).\n\n","tags":["Go","who-is-spy","Websocket"]},{"title":"Using embed feature in go 1.16","url":"/2021/07/16/Using-embed-feature-in-go-1-16/","content":"\n## Problem to be solved\nI want to embed the txt file containing all words into a single binary.\n\n## Source of information\n- [stackoverflow](https://stackoverflow.com/a/29500100)\n- [go package docs: embed](https://pkg.go.dev/embed)\n\n## Upgrade my project to use go 1.16\n\nInstall new go version; \n\nUpdate my go mod: \n```\ngo mod edit -go=1.16\n```\n\n## Code\n\n```\nimport _ \"embed\"\n//go:embed words-en\nvar englishWords string\n\n//go:embed words-cn\nvar chineseWords string\n```\n\nThis will make these two strings available to the package, and the file will be packaged into the single compiled file upon build.\n","tags":["Go","who-is-spy"]},{"title":"React naming and file structure","url":"/2021/06/19/React-naming-and-file-structure/","content":"\n# Time to decide on my React preferences\n\nReactJS itself does not have opinions, but it takes time whenever I start writing a new React project to decide the naming and file organisation styles.\n\n[AirBnb's convention is a good start point](https://github.com/airbnb/javascript/tree/master/react#basic-rules).","tags":["React"]},{"title":"AWS Developer Associate Exam Preparation Note","url":"/2021/05/24/AWS-Developer-Associate-Exam-Preparation-Note/","content":"\n## CodeDeploy AppSpec file\n- AppSpec file should be placed in the root directory of the application's source content's directory structure.\n\n## AWS X-Ray\nAWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.\n\n## Lambda function invoking\n\n### Asynchronous invokes (Event)\nSeveral AWS services invoke functions asynchronously to process events: \n- Amazon Simple Storage Service\n- Amazon Simple Notification Service\n- Amazon Simple Email Service\n- AWS CloudFormation\n- Amazon CloudWatch Logs\n- Amazon CloudWatch Events\n- AWS CodeCommit\n- AWS Config\nAsynchronous invokes place your invoke request in Lambda service queue and we process the requests as they arrive. You should use AWS X-Ray to review how long your request spent in the service queue by checking the “dwell time” segment.\n\n### Poll-based invokes (Stream/Queue)\n\nLambda polls the queue/steam and invokes your Lambda function **synchronously** with an event that contains queue messages. \n\n- SQS\n- DynamoDB Stream\n- Kinesis data stream \n\n### Synchronous invokes (Push)\n\nAWS executes your Lambda function and wait for the function to complete. When you perform a synchronous invoke, you are responsible for checking the response and determining if there was an error and if you should retry the invoke.\n\n- Elastic Load Balancing (Application Load Balancer)\n- Amazon Cognito\n- Amazon Lex\n- Amazon Alexa\n- Amazon API Gateway\n- Amazon CloudFront (Lambda@Edge)\n- Amazon Kinesis Data Firehose\n\n\n<img src=\"https://d2908q01vomqb2.cloudfront.net/fc074d501302eb2b93e2554793fcaf50b3bf7291/2019/06/27/Screen-Shot-2019-06-27-at-2.23.51-PM-1024x510.png\" alt=\"lambda invoke models\" width=\"400\"/>\n\n# AWS Cognito Sync\nAmazon Cognito lets you save end user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. \n**Amazon Cognito Sync** is an AWS service and **client library** that enables cross-device syncing of application-related user data. \n\n# AWS Cognito Push Sync\nAmazon Cognito automatically tracks the association between identity and devices. \n\nPush sync ensures that, whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change. Note that you need to grant AWS Cognito the right permission to send an SNS notification.\n\n# SQS long polling vs short polling\n If you used long polling, the **connection stays open** to SQS until a message has been found in SQS, **or** until the timeout (e.g. max 20s) is reached. \n\n - With short polling, the `ReceiveMessage` request queries only a subset of the servers (based on a weighted random distribution) to find messages that are available to include in the response. Amazon SQS sends the response right away, even if the query found no messages.\n- With long polling, the `ReceiveMessage` request queries all of the servers for messages. Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\n\nShort polling occurs when the `WaitTimeSeconds` parameter of a `ReceiveMessage` request is set to `0` in one of two ways:\n- The `ReceiveMessage` call sets `WaitTimeSeconds` to 0.\n- The `ReceiveMessage` call doesn’t set WaitTimeSeconds, but the queue attribute `ReceiveMessageWaitTimeSeconds` is set to `0`.\n\n# Caching strategy\n\n## Lazy loading / cache aside\nA disadvantage when using cache-aside as the only caching pattern is that because the data is loaded into the cache only after a cache miss, some overhead is added to the initial response time because additional roundtrips to the cache and database are needed.\n\n## Write through\n- A disadvantage of the write-through approach is that infrequently-requested data is also written to the cache, resulting in a larger and more expensive cache.\n\n# Decouple RDS instance with beanstalk\n- Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple an RDS DB instance from environment A.\n- Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the RDS DB instance.\n\n# API Gateway cache\n- For better performance and faster API execution, you can optionally provision a dedicated cache for each stage of your APIs.\n- Cache is far more important than calls. Use caching in dev and test environment only when needed.\n\n# Lambda `/tmp` storage\n`/tmp` directory storage quota: `512 MB`, CANNOT be changed.\n\n# Lambda logs\nAWS Lambda automatically monitors Lambda functions on your behalf, reporting metrics through Amazon CloudWatch. To help you troubleshoot failures in a function, Lambda logs all requests handled by your function and also automatically stores logs generated by your code through Amazon CloudWatch Logs.\n\n# Requiring HTTPS for Cloudfront \n## Origin Protocol Policy\nChange the Origin Protocol Policy for the applicable origins in your distribution:\n\n- HTTPS Only – CloudFront uses only HTTPS to communicate with your custom origin.\n- Choose Match Viewer only if you specify Redirect HTTP to HTTPS or HTTPS Only for Viewer Protocol Policy.\n\n## Viewer Protocol Policy\nChoose the protocol policy that you want viewers to use to access your content in CloudFront edge locations:\n- HTTP and HTTPS: Viewers can use both protocols.\n- Redirect HTTP to HTTPS: Viewers can use both protocols, but HTTP requests are automatically redirected to HTTPS requests.\n- HTTPS Only: Viewers can only access your content if they're using HTTPS.\n\n# DynamoDB Read Capacity Units\n- A read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to `4 KB` in size.\n- Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB × 2) consumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit. Item sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500-byte item consumes the same throughput as reading a 4 KB item.\n\n# Amazon EC2 Query API toubleshooting\nIf an API request exceeds the API request rate for its category, the request returns the `RequestLimitExceeded` error code. \nTo prevent this error, ensure that your application doesn't retry API requests at a high rate. \nYou can do this by using care when polling and by using **exponential backoff retries**.\n\n# Deploy lambda function via CloudFormation\n- Write the code directly in CloudFormation\n- Upload zip to S3 and refer to it in CloudFormation\n\n# Run X-Ray with ECS\n- Create a docker image that runs X-Ray daemon\n- Instrument the code\n- Create a role for the task\n\n# Large message in SQS\nTo manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. \n\n# DynamoDB random number partition key to distribute loads more evenly\nOne strategy for distributing loads more evenly across a partition key space is to add a random number to the end of the partition key values. Then you randomize the writes across the larger space.\n\n# A namespace is a container for CloudWatch metrics\n- A dimension is a name/value pair that is part of the identity of a metric. You can assign up to 10 dimensions to a metric.\n- A namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.\n\n# web identity federation\nWith web identity federation, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP.\n\n# X-Ray annotation and metadata\n- Annotations are simple key-value pairs that are **indexed for use with filter expressions**. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API. X-Ray indexes up to 50 annotations per trace.\n- Metadata are key-value pairs with values of any type, including objects and lists, but that are not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces.\n\n# Git Credentials\nWith Git credentials, you can generate a static user name and password in the Identity and Access Management (IAM) console that you can use to access AWS CodeCommit repositories from the command line, Git CLI, or any Git tool that supports HTTPS authentication.\n\n# API Gateway Lambda Authorizor\n\nA Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API.\n\nA Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.\n\nWhen a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.\n\nThere are two types of Lambda authorizers:\n\n- A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.\n- A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables.\n\nFor WebSocket APIs, only request parameter-based authorizers are supported.\n\n# MFA - GetSessionToken API\nThe primary occasion for calling the GetSessionToken API operation or the get-session-token CLI command is when a user must be authenticated with multi-factor authentication (MFA).\n\n# Cognito user pool vs identity pool\n\n- User pools are for authentication (identify verification). With a user pool, your app users can sign in through the user pool or federate through a third-party identity provider (IdP). \n- Identity pools are for authorization (access control).\n\n# Cognito user pool API gateway authorizor\nAfter the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header.\n\n# Service role\nA role that a service assumes to perform actions on your behalf is called a service role.\n\n# API Gateway usage plan\nAfter you create, test, and deploy your APIs, you can use API Gateway usage plans to make them available as product offerings for your customers. You can configure usage plans and API keys to allow customers to access selected APIs at agreed-upon request rates and quotas that meet their business requirements and budget constraints. \n\n# CLI command: `put-metrics-data` and `put-metrics-alarm`\n- `put-metrics-data` publishes metric data points to Amazon CloudWatch. \n- `put-metrics-alarm` creates or updates an alarm and associates it with the specified metric, metric math expression, or anomaly detection model.\n\n# Redis read replica\nA Redis (cluster mode disabled) cluster has a single shard, inside of which is a collection of Redis nodes; one primary read/write node and up to five secondary, read-only replica nodes. Each read replica maintains a copy of the data from the cluster's primary node. ","tags":["AWS","AWS DVA"]},{"title":"Golang: how string encoding impacts result of len()","url":"/2021/05/24/Golang-how-string-encoding-impacts-result-of-len/","content":"\n## builtin function: `len(v Type) int`\nThe len built-in function returns the length of v, according to its type:\n\n- Array: the number of elements in v.\n- Pointer to array: the number of elements in *v (even if v is nil).\n- Slice, or map: the number of elements in v; if v is nil, len(v) is zero.\n- String: the number of **bytes** in v.\n- Channel: the number of elements queued (unread) in the channel buffer; if v is nil, len(v) is zero.\n\n## What is `rune`\nrune is an alias for int32 and is equivalent to int32 in all ways. It is used, by convention, to distinguish character values from integer values.\n\nRepresenting Unicode code points.\n\n`type rune = int32`\n\n## What is `string\n\nstring is the set of all strings of 8-bit bytes, conventionally but not necessarily representing UTF-8-encoded text. A string may be empty, but not nil. Values of string type are immutable.\n\n`type string string`\n\n## Range expression on a string\nSee [Range expression doc here](https://golang.org/ref/spec#RangeClause)\n\nFor a string value, the \"range\" clause iterates over the **Unicode** code points in the string starting at byte index 0. \n- On successive iterations, the index value will be the index of the first byte of successive UTF-8-encoded code points in the string, \n- and the second value, of type `rune`, will be the value of the corresponding code point. \n\nIf the iteration encounters an invalid UTF-8 sequence, the second value will be 0xFFFD, the Unicode replacement character, and the next iteration will advance a single byte in the string.\n\n## Index expression on a string\nSee [Index expression doc here](https://golang.org/ref/spec#Index_expressions)\n\n`a[x]` is the non-constant byte value at index x and the type of a[x] is byte\n\n## Difference between len([]rune(s)) and len(s)\n`rune[]` splits string into four-byte characters; len(s) counts how many bytes are in the string.\n\n## Example\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n)\n\nfunc main() {\n\ts := \"H世界\"\n    fmt.Println(\"Range expression on rune slice\")\n\trunes := []rune(s)\n\tfor _, r := range runes {\n\t\tfmt.Println(r)\n\t}\n\tfmt.Println(\"Range expression on string\")\n\tfor _, l := range s {\n\t\tfmt.Println(l)\n\t}\n    fmt.Println(\"Index expression on string\")\n\tfor i := 0; i < len(s); i++ {\n\t\tfmt.Println(s[i])\n\t}\n\tfmt.Println(\"------\")\n\n\tfmt.Println(\"string length: \", len(s))\n\tfmt.Println(\"runes length: \", len(runes))\n}\n\n```\n\nOutput:\n```\nRange expression on rune slice\n72\n19990\n30028\nRange expression on string\n72\n19990\n30028\nIndex expression on string\n72\n228\n184\n150\n231\n149\n140\n------\nstring length:  7\nrunes length:  3\n```","tags":["Go","Encoding"]},{"title":"Read a Go package: scs.SessionManager","url":"/2021/05/04/Read-a-Go-package-scs-SessionManager/","content":"\n## Usage in project\nI was learning how to maintain sessions in Go and went across the [scs](https://github.com/alexedwards/scs) package for session middleware.\n\n```\n// In main.go\n// available to whole main package\nvar session *scs.SessionManager\n\nfunc main() {\n\tapp.InProduction = false\n\tsession = scs.New()\n\tsession.Lifetime = 24 * time.Hour\n\tsession.Cookie.Persist = true\n\tsession.Cookie.SameSite = http.SameSiteLaxMode\n\tsession.Cookie.Secure = app.InProduction\n    // ... \n\n    srv := &http.Server{\n        Addr: addr,\n        Handler: routes(&app),\n\t}\n}\n\n```\n\n```\n// In middleware.go\n\n// SessionLoad loads and saves the session on every request\nfunc SessionLoad(next http.Handler) http.Handler{\n\treturn session.LoadAndSave(next)\n}\n\n```\n\n```\nIn routes.go\n\nfunc routes(app *config.AppConfig) http.Handler{\n\tmux := chi.NewRouter()\n\t// ...\n\tmux.Use(SessionLoad)\n\tmux.Get(\"/\", handlers.Repo.ServeHome)\n\tmux.Get(\"/about\", handlers.Repo.ServeAbout)\n\treturn mux\n}\n\n```\n\n## Go through basic usage\n\n```\npackage main\n\nimport (\n\t\"io\"\n\t\"net/http\"\n\t\"time\"\n\n\t\"github.com/alexedwards/scs/v2\"\n)\n\n// Declare sessionManager here and make it available to whole main package\nvar sessionManager *scs.SessionManager\n\nfunc main() {\n\t// Initialize a new session manager and configure the session lifetime.\n\tsessionManager = scs.New()\n\tsessionManager.Lifetime = 24 * time.Hour\n\n    // mux is used for routing\n\tmux := http.NewServeMux()\n\tmux.HandleFunc(\"/put\", putHandler)\n\tmux.HandleFunc(\"/get\", getHandler)\n\n\t// Wrap your handlers with the LoadAndSave() middleware.\n\thttp.ListenAndServe(\":4000\", sessionManager.LoadAndSave(mux))\n}\n\nfunc putHandler(w http.ResponseWriter, r *http.Request) {\n\t// Store a new key and value in the session data.\n\tsessionManager.Put(r.Context(), \"message\", \"Hello from a session!\")\n}\n\nfunc getHandler(w http.ResponseWriter, r *http.Request) {\n\t// Use the GetString helper to retrieve the string value associated with a\n\t// key. The zero value is returned if the key does not exist.\n\tmsg := sessionManager.GetString(r.Context(), \"message\")\n\tio.WriteString(w, msg)\n}\n```\n\n## Understanding `Load` function\n```\n// Load retrieves the session data for the given token from the session store,\n// and returns a new context.Context containing the session data. If no matching\n// token is found then this will create a new session.\n//\n// Most applications will use the LoadAndSave() middleware and will not need to\n// use this method.\nfunc (s *SessionManager) Load(ctx context.Context, token string) (context.Context, error) {\n\n    // A Context carries a deadline, a cancellation signal, and other request-scope values across API boundaries.\n    // Context's methods may be called by multiple goroutines simultaneously.\n\n    // contextKey is the key used to set and retrieve the session data from a context.Context. It's automatically generated to ensure uniqueness.\n\n    // .(*sessionData) is a type assertion, similar to the 'as' keyword in C#\n    // If the session is already in the context, just return the context and nothing needs to be done.\n\tif _, ok := ctx.Value(s.contextKey).(*sessionData); ok {\n\t\treturn ctx, nil\n\t}\n\n    // If the token is empty, simply add a new, empty session data to the context.\n\tif token == \"\" {\n\t\treturn s.addSessionDataToContext(ctx, newSessionData(s.Lifetime)), nil\n\t}\n\n    // Find should return the data for a session token from the store, in byte array format\n\tb, found, err := s.Store.Find(token)\n\tif err != nil {\n\t\treturn nil, err\n\t} else if !found {\n        // If no session data with the given token is found in the store, add empty session data to the context.\n\t\treturn s.addSessionDataToContext(ctx, newSessionData(s.Lifetime)), nil\n\t}\n\n\tsd := &sessionData{\n\t\tstatus: Unmodified,\n\t\ttoken:  token,\n\t}\n    // Decode the byte array found from the store\n\tif sd.deadline, sd.values, err = s.Codec.Decode(b); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Mark the session data as modified if an idle timeout is being used. This\n\t// will force the session data to be re-committed to the session store with\n\t// a new expiry time.\n\tif s.IdleTimeout > 0 {\n\t\tsd.status = Modified\n\t}\n\n\treturn s.addSessionDataToContext(ctx, sd), nil\n}\n```\n\n\n## Understanding `LoadAndSave` function\n\n```\n// LoadAndSave provides middleware which automatically loads and saves session\n// data for the current request, and communicates the session token to and from\n// the client in a cookie.\nfunc (s *SessionManager) LoadAndSave(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tvar token string\n\t\tcookie, err := r.Cookie(s.Cookie.Name)\n\t\tif err == nil {\n\t\t\ttoken = cookie.Value\n\t\t}\n\n\t\tctx, err := s.Load(r.Context(), token)\n\t\tif err != nil {\n\t\t\ts.ErrorFunc(w, r, err)\n\t\t\treturn\n\t\t}\n\n\t\tsr := r.WithContext(ctx)\n\t\tbw := &bufferedResponseWriter{ResponseWriter: w}\n\t\tnext.ServeHTTP(bw, sr)\n\n\t\tif sr.MultipartForm != nil {\n\t\t\tsr.MultipartForm.RemoveAll()\n\t\t}\n\n\t\tif s.Status(ctx) != Unmodified {\n\t\t\tresponseCookie := &http.Cookie{\n\t\t\t\tName:     s.Cookie.Name,\n\t\t\t\tPath:     s.Cookie.Path,\n\t\t\t\tDomain:   s.Cookie.Domain,\n\t\t\t\tSecure:   s.Cookie.Secure,\n\t\t\t\tHttpOnly: s.Cookie.HttpOnly,\n\t\t\t\tSameSite: s.Cookie.SameSite,\n\t\t\t}\n\n\t\t\tswitch s.Status(ctx) {\n\t\t\tcase Modified:\n\t\t\t\ttoken, expiry, err := s.Commit(ctx)\n\t\t\t\tif err != nil {\n\t\t\t\t\ts.ErrorFunc(w, r, err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tresponseCookie.Value = token\n\n\t\t\t\tif s.Cookie.Persist || s.GetBool(ctx, \"__rememberMe\") {\n\t\t\t\t\tresponseCookie.Expires = time.Unix(expiry.Unix()+1, 0)        // Round up to the nearest second.\n\t\t\t\t\tresponseCookie.MaxAge = int(time.Until(expiry).Seconds() + 1) // Round up to the nearest second.\n\t\t\t\t}\n\t\t\tcase Destroyed:\n\t\t\t\tresponseCookie.Expires = time.Unix(1, 0)\n\t\t\t\tresponseCookie.MaxAge = -1\n\t\t\t}\n\n\t\t\tw.Header().Add(\"Set-Cookie\", responseCookie.String())\n\t\t\taddHeaderIfMissing(w, \"Cache-Control\", `no-cache=\"Set-Cookie\"`)\n\t\t\taddHeaderIfMissing(w, \"Vary\", \"Cookie\")\n\t\t}\n\n\t\tif bw.code != 0 {\n\t\t\tw.WriteHeader(bw.code)\n\t\t}\n\t\tw.Write(bw.buf.Bytes())\n\t})\n}\n```","tags":["Go"]},{"title":"Resolve 'useEffect has missing dependency ' with useCallback","url":"/2021/03/22/Resolve-useEffect-has-missing-dependency-with-useCallback/","content":"\n## The original problem: useEffect missing dependency\n\nThis is the original code, which leads to a warning \"useEffect has a missing dependency `fetchData`\"\n``` \nexport default function useGet(url, initialState = null) {\n\n    const [data, setData] = useState(initialState);\n    const [isLoading, setLoading] = useState(false);\n    const reFetch = async () => {\n        await fetchData()\n    }\n\n    const fetchData = async () => {\n        setLoading(true);\n        const response = await axios.get(url);\n        setData(response.data);\n        setLoading(false);\n    }\n\n    useEffect(() => {\n        fetchData();\n    }, [url]);\n\n    return { data, isLoading, reFetch};\n}\n```\n\n## Why this problem: useEffect's skipping effect optimization \nAs emphasized in React documentation:\n> If you use this [optimization](https://reactjs.org/docs/hooks-effect.html#tip-optimizing-performance-by-skipping-effects), make sure the array includes all values **from the component scope (such as props and state)** that change over time and that are used by the effect.\n\n`fetchData` is a component scope value, and lint is warning that if fetchData is changed, the useEffect will skip, potentially causing an out-of-sync.\n\n## Add fetchData to useEffect dependency\nThe immediate solution you would think of is to add `fetchData` to the `useEffect` dependencies:\n\n``` \n    useEffect(() => {\n        fetchData();\n    }, [fetchData]);\n```\n\nThe problem here is that `fetchData` is a local const and will be evaluated with each render. This will case `useEffect` being triggered more times than needed.\n\n## Memorize the callback\n\n- Given an inline callback and an array of dependencies, [useCallback](https://reactjs.org/docs/hooks-reference.html#usecallback) will return a memoized version of the callback that only changes if one of the dependencies has changed. \n- Here we know `fecthData` depends on `url` and `useEffect` depends on `fetchData`. Now we have:\n\n``` \nexport default function useGet(url, initialState = null) {\n\n    const [data, setData] = useState(initialState);\n    const [isLoading, setLoading] = useState(false);\n    const reFetch = async () => {\n        await fetchData()\n    }\n\n    const fetchData = useCallback(async () => {\n        setLoading(true);\n        const response = await axios.get(url);\n        setData(response.data);\n        setLoading(false);\n    }, [url])\n\n    useEffect(() => {\n        fetchData();\n    }, [fetchData]);\n\n    return { data, isLoading, reFetch};\n}\n```\n\n- The dependency chain is `useEffect` -> `fetchData` -> `url`.\n- `fetchData` will only be reloaded if `url` changes\n- `useEffect` will only be triggered if `fetchData` changes.\n\n## Readings \n- [Github issue: React Hook useEffect has a missing dependency #6903](https://github.com/facebook/create-react-app/issues/6903)\n- [React docs: using the effect hook](https://reactjs.org/docs/hooks-effect.html)\n- [React docs: useCallback](https://reactjs.org/docs/hooks-reference.html#usecallback)","tags":["React"]},{"title":"Learn Elastic stack","url":"/2021/03/07/Learn-Elastic-stack/","content":"\n## Logstash\n\n<img src=\"https://www.elastic.co/guide/en/logstash/current/static/images/basic_logstash_pipeline.png\" width=\"500px\" alt=\"\"/>\n\n> A Logstash pipeline has two required elements, input and output, and one optional element, filter. The input plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write the data to a destination.\n\n## Logstash -> Elasticsearch -> Kibana\n- In logstash, specify output as elasticsearch and specify the index\n- In Kibana UI, specify index\n- Kibana will retrieve the data with the correct index.\n\n## grok filter\nReading:\n- [grok pattern syntax](https://streamsets.com/documentation/datacollector/latest/help/datacollector/UserGuide/Apx-GrokPatterns/GrokPatterns_title.html)\n- [grok pattern examples](https://logz.io/blog/logstash-grok/)\n- [optional parts in grok pattern](https://discuss.elastic.co/t/making-a-part-in-the-grok-expression-optional/43961)\n    - Look out for your spaces! If there is a space before the optional field and a space after, the filter will only match two spaces.\n    - If a field does not exist, it will break Kibana. See [Available fields not showing in kibana (Issue resolved)](https://discuss.elastic.co/t/available-fields-not-showing-in-kibana-issue-resolved/31872/3)\n    \n","tags":["Elastic stack"]},{"title":"Comparability and Assignability in Go","url":"/2021/03/01/Comparability-and-Assignability-in-Go/","content":"\n## [Value of a named type] comparable with [Value of the same type] \n``` \n    type Cat uint8\n\tvar c1 Cat = 1\n\tvar c2 Cat = 2\n\tfmt.Println(c1 < c2) // true\n```\n\n## [Value of an unnamed type] comparable with [Value of the same underlying type]\n\n### value literal is of unnamed type\n``` \n    type Cat uint8\n\tvar c Cat = 1\n\tfmt.Println(1 == c) // true\n```\n\n### const is of unnamed type\n``` \n\tconst b = 1\n    type Cat int\n\tvar c Cat = 2\n\tfmt.Println(b < c)\n\tfmt.Printf(\"%T %v\\n\", b, b) // int 1\n```\n\n## Two values of different types cannot be compared directly\n\n### Value declaration gives a type\n``` \n    type Cat int\n\tb := 1\n\tvar c Cat = 2\n\tfmt.Println(b < c) // invalid operation: b < c (mismatched types int and Cat)\n\tfmt.Printf(\"%T %v\\n\", b, b) // int 1\n```\n\n### I can do a conversion\n\n``` \n\ttype Cat uint8\n\tvar c Cat = 2\n\tb := 1\n\tfmt.Println(Cat(b) < c)  // true\n\tfmt.Printf(\"%T %v\\n\", b, b) // int 1\n```\n\n#### Follow the assignability rule for type conversion\n\n``` \n\tb := 1\n\ttype Cat uint8\n\tvar c Cat = 2\n\tcat := Cat(b)\n\tfmt.Println(cat < c) // true\n\tfmt.Printf(\"%T %v\\n\", cat, cat) //main.Cat 1\n```\n\n``` \n\tb := 1\n\ttype Cat uint8\n\tvar c Cat = 2\n\tb = Cat(b) // cannot use Cat(b) (type Cat) as type int in assignment\n\tfmt.Println(b < c) // invalid operation: b < c (mismatched types int and Cat)\n```\n\n","tags":["Go"]},{"title":"Enum and Flag with Go's iota","url":"/2021/02/23/Flag-and-Enum-with-Go-s-iota/","content":"\n## Enum\n``` \npackage main\n\nimport (\n\t\"fmt\"\n)\n\ntype Color uint8\n\nconst (\n\tRed = iota\n\tGreen\n\tBlue\n)\n\nfunc main() {\n\tfmt.Printf(\"%04b\\n\", Red)\n\tfmt.Printf(\"%04b\\n\", Green)\n\tfmt.Printf(\"%04b\\n\", Blue)\n}\n```\n\n## Flag\n\n``` \npackage main\n\nimport (\n\t\"fmt\"\n)\n\ntype Color uint8\n\nconst (\n\tRed = 1 << iota\n\tGreen\n\tBlue\n)\n\nfunc main() {\n\tvar color Color\n\n\tcolor |= Red\n\tfmt.Printf(\"%04b\\n\", color)\n\n\tcolor |= Green\n\tfmt.Printf(\"%04b\\n\", color)\n\n\tcolor &^= Red\n\tfmt.Printf(\"%04b\\n\", color)\n\n\tfmt.Printf(\"%04b\\n\", Red)\n\tfmt.Printf(\"%04b\\n\", Green)\n\tfmt.Printf(\"%04b\\n\", Blue)\n}\n```","tags":["Go"]},{"title":"const in .NET","url":"/2021/02/22/const-in-NET/","content":"\n<img src=\"const.png\" alt=\"\" width=\"300px\"/>\n\nFound a useless grammar point: .NET allows const of reference type; \n\nalthough const of reference type can only be initialized with null 😂","tags":["dotNET"]},{"title":"Go http handler route matching rule","url":"/2021/02/22/Go-http-handler-route-matching-rule/","content":"\n## Source code\nSee [The Go Programming Language/chapter1: server2](https://github.com/adonovan/gopl.io/blob/master/ch1/server2/main.go), where two handlers are created to handle `/` and `/count`\n\n## Symptom\nEach time I visit `/count` with Chrome, the `counter` value is incremented too, while I would expect only the handler for the **more specific** path, `/counter` to be called, which shouold not increment `counter`.\n\n## Reason\n<img src=\"network-inspection.png\" width=\"400px\" alt=\"inspection\"/>\n\n- So my theory about the route matching rule is correct, see [documentation for ServeMux](https://pkg.go.dev/net/http#ServeMux). \n- The reason why the `counter` is incremented when visiting `/counter` is that the browser made a call to `/favicon.ico`, which you can see from the Network inspection result that the `/` handler is called.\n\n## Finding the Go documentation\nGo has good documentation, accessible by hovering over the code in IDE.","tags":["Go"]},{"title":"My internship from 9 Nov to 29 Jan","url":"/2021/01/29/My-internship-from-9-Nov-to-29-Jan/","content":"\nI did my internship at Theta from 9 Nov to 29 Jan. See [here](https://www.theta.co.nz/news-blogs/theta-news/theta-interns-automate-ui-testing-from-40-hours-to-a-single-digit/) for what we achieved and [here](https://autotest-uat.azurewebsites.net/help) for demos of what we made.","tags":["Progress"]},{"title":"AWS SAP - AWS Best Practice Design Patterns","url":"/2021/01/04/AWS-SAP-Final-Preparation-⛽️/","content":"\n# AWS SAM with build-in CodeDeploy: Deploying serverless applications gradually\n- Deploys new versions of your Lambda function, and automatically creates aliases that point to the new version.\n- Gradually shifts customer traffic to the new version until you're satisfied that it's working as expected, or you roll back the update.\n- Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected.\n- Rolls back the deployment if CloudWatch alarms are triggered.\n\n# Use AWS CloudFormation `StackSets` to deploy to multiple accounts/regions\n## Concepts\n### StackSets\nA stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. \n\n### Stack Instance\nA stack instance is a reference to a stack in a target account within a Region.  \n\n## Permissions models for stack sets\n### `self-managed` permissions  \n- You create the IAM roles required by StackSets to deploy across accounts and Regions\n- Need **trusted relationship** between the account you're administering the stack set from and the account you're deploying stack instances to.\n\n### `service-managed` permissions\n- Deploy stacks into Organization.\n- Supports automatic deployment.\n\n# Amazon Kinesis Video Streams\n\n<img src=\"https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/images/acuity-arch-3a.png\" alt=\"\" width=\"600px\"/>\n\n## Kinesis Video Streams Producer libraries\nA set of easy-to-use software and libraries that you can install and configure on your devices. These libraries make it easy to securely connect and reliably stream video in different ways, including in real time, after buffering it for a few seconds, or as after-the-fact media uploads.\n\n## Consumer\n- Gets data, such as fragments and frames, from a Kinesis video stream to view, process, or analyze it. Generally these consumers are called Kinesis Video Streams applications. \n- You can write applications that consume and process data in Kinesis video streams in real time, or after the data is durably stored and time-indexed when low latency processing is not required.\n- Kinesis Video Streams does not support putting data into S3 \"out-of-the-box\".\n\n# Associating an Amazon VPC and a private hosted zone from different AWS accounts\n- You need to use CLI, SDK or Route 53 API.\n- To associate a Route 53 private hosted zone in one AWS account (Account A) with a virtual private cloud that belongs to another AWS account (Account B):\n    1. Connect to an EC2 instance in Account A.\n    2. Update CLI version.\n    3. Run this command to list the available hosted zones in Account A. Note the hosted zone ID in Account A that you will associate with Account B.  `aws route53 list-hosted-zones`\n    4.  Run the following command to authorize the association between the private hosted zone in Account A and the virtual private cloud in Account B. Use the hosted zone ID from the previous step, as well as the Region and ID of the virtual private cloud in Account B. `aws route53 create-vpc-association-authorization --hosted-zone-id <hosted-zone-id> --vpc VPCRegion=<region>,VPCId=<vpc-id>`\n    5. Connect to an EC2 instance in Account B.\n    6. Run the following command to create the association between the private hosted zone in Account A and the virtual private cloud in Account B. Use the hosted zone ID from step #3, as well as the Region and ID of the virtual private cloud in Account B. `aws route53 associate-vpc-with-hosted-zone --hosted-zone-id <hosted-zone-id> --vpc VPCRegion=<region>,VPCId=<vpc-id>`\n    7.  It's a best practice to delete the association authorization after the association is created. Doing this prevents you from recreating the same association later. To delete the authorization, reconnect to an EC2 instance in Account A. Then, run this command: `aws route53 delete-vpc-association-authorization --hosted-zone-id <hosted-zone-id>  --vpc VPCRegion=<region>,VPCId=<vpc-id>`\n    \n\n# Use Lambda@Edge to authorize request before it reaches origin infrastructure\nYou cannot cache authorization.\n\n<img src=\"https://d2908q01vomqb2.cloudfront.net/5b384ce32d8cdef02bc3a139d4cac0a22bb029e8/2018/01/24/Capture.png\" alt=\"\"/>\n5. Lambda@Edge decodes the JWT and checks if the user belongs to the correct Cognito User Pool. It also verifies the cryptographic signature using the public RSA key for Cognito User Pool. Crypto verification ensures that JWT was created by the trusted party.\n\nMore detaiils see [Authorization@Edge – How to Use Lambda@Edge and JSON Web Tokens to Enhance Web Application Security](https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-how-to-use-lambdaedge-and-json-web-tokens-to-enhance-web-application-security/).\n\n# Sources and Targets of DMS\n## Sources\n- Cannot be ElasticSearch\n- Can be S3\n## Targets\n- Can be ElasticSearch\n- Can be DynamoDB\n\n# Cross-Region Snapshot Copy for RDS and EBS\nThere is no charge for the copy operation itself; you pay only for the **data transfer out** of the source region and for the **data storage** in the destination region. \n\n# Stop and Re-start Elastic Beanstalk on a schedule\n1. Lambda\n    - Need the Elastic Beanstalk ID\n    - Need to assume a role with `AWSElasticBeanstalkFullAcces` policy\n2. CloudWatch Event on a cron schedule\n\n## S3 object-level logging with CloudTrail\nCloudTrail supports logging Amazon S3 object-level API operations such as GetObject, DeleteObject, and PutObject. \n\n## S3 event notifications\n### Targets\n- SNS\n- SQS\n- Lambda\n\nCloudWatch Events is not supported.\n\n### Limitation\nDoes not act on failed operations.\n\n### Filtering\nCan filter based on key suffix/prefix\n\n## Real-time Processing of Log Data with Subscriptions\n- Subscriptions \n- Real-time feed of log events from CloudWatch Logs\n- Delivered to:\n    1. Amazon Kinesis stream, \n    2. Amazon Kinesis Data Firehose stream, \n    3. AWS Lambda \n- Steps:\n    1. Create the receiving resource\n    2. Create a subscription filter: what log events get delivered and where to send them\n- Each log group can have up to two subscription filters associated with it.\n\n# AWS Batch user case: Digital Media\n\n<img src=\"https://d1.awsstatic.com/Test%20Images/Kate%20Test%20Images/Dilithium-Diagrams_Visual-Effects-Rendering.ad9c0479c3772c67953e96ef8ae76a5095373d81.png\" alt=\"\" width=\"600px\"/>\n\n- AWS Batch provides content producers and post-production houses with tools to automate content rendering workloads and reduces the need for human intervention due to execution dependencies or resource scheduling. \n- This includes the scaling of **compute cores** in a render farm, utilizing **Spot Instances**, and coordinating the execution of disparate steps in the process.\n\n# AWS Single Sign-On\n- Simplifies managing SSO access to AWS accounts and business applications. \n- Across AWS accounts in AWS Organizations. \n\n# AWS Directory Service for AD - Trust Relationship\n- One or two-way\n- External or forest trust\n- Between your AWS Directory Service for Microsoft Active Directory and on-premises directories\n- Between multiple AWS Managed Microsoft AD directories in the AWS cloud\n\n# Use self-created authentication and IAM for authorization\n- Create SAML 2.0 based authentication and use AWS Organizations SSO for authorization\n- Adding OIDC (Open ID Connect) Identity Providers to a Cognito User Pool \n\n# Moving an EC2 instance into a placement group\n1. Stop the instance using the `stop-instances` command.\n2. Use the `modify-instance-placement` command and specify the name of the placement group to which to move the instance: `aws ec2 modify-instance-placement --instance-id i-0123a456700123456 --group-name MySpreadGroup`\n3. Start the instance using the start-instances command.\n\n# CloudFormation Change Set\n- **Preview** how proposed changes to a stack might impact your running resources.\n- AWS CloudFormation makes the changes to your stack only when you decide to **execute** the change set.\n\n# CloudFormation `DeletePolicy` attribute\nAllowed values:\n- Retain: resource retains but not in the CloudFormation's scope.\n- Snapshot: supports EC2 volume, ElastiCache, Neptune, RDS, Redshift\n- Delete (default, except for `AWS::RDS::DBCluster` and `AWS::RDS::DBInstance`)\n\n# Deploy new AMI to resources defined with CloudFormation\n## Stand-alone EC2 instance\n1. Create your new AMIs containing your application or operating system changes.\n2. Update your template to incorporate the new AMI IDs.\n3. Update the stack, either from the AWS Management Console or by using the AWS command `aws cloudformation update-stack`.\n3. When you update the stack, AWS CloudFormation detects that the AMI ID has changed. It launches a new instance with the new AMI, point the other resources to the new instance, and remove the old instance.\n\n## Let Auto-Scaling Group use a new AMI\n- Instance type and AMI info are encapsulated in the Auto Scaling launch configuration.\n- Edit the `AWS::AutoScaling::LaunchConfiguration` resource in the template.\n- Changing the launch configuration does not impact any of the running Amazon EC2 instances in the Auto Scaling group. An updated launch configuration applies only to new instances that are created after the update.\n- If you want to propagate the change to your launch configuration across all the instances in your Auto Scaling group, you can use an [`UpdatePolicy` attribute](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html). \n\n# RDS Event Notification\n[Amazon RDS uses the **Amazon Simple Notification Service (Amazon SNS)** to provide notification when an Amazon RDS event occurs. ](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html)\nThe event includes RDS failure. Therefore you can:\n> Configure an Amazon RDS event notification to react to the failure of the database in us-east-1(Aurora master region) by invoking an AWS Lambda function that promotes a read replica in another region.\n\n# DataSync v.s. Fire Gateway configuration of AWS Storage Gateway\n- Use AWS DataSync to migrate existing data to Amazon S3\n- Subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications.\n\n# Receiving CloudTrail logs from Multiple Accounts into a single S3 bucket\n1. Turn on CloudTrail in account A, where the destination bucket will belong. Do not turn on CloudTrail in any other accounts yet.\n2. Update the bucket policy on your destination bucket to grant **cross-account permissions to CloudTrail**.\n3. Turn on CloudTrail in the other accounts you want CloudTrail logs delivered. Configure CloudTrail in these accounts to use the same bucket belonging to account that A you specified in step 1.\n\n# AWS Systems Manager\n## Session Manager\n- Supports Port Forwarding\n- browser-based\n- Log commands to S3 or Amazon CloudWatch logs using CloudTrail\n\n## Run Command\n- Do not need to log into servers\n- Replaces SSH - No SSH\n- Simply send commands\n- All actions recorded by CloudTrail\n\n# AWS Certificate Manager - Regions\n- To use a certificate with Elastic Load Balancing for the same fully qualified domain name (FQDN) or set of FQDNs in more than one AWS region, you must request or import a certificate for **each region**. \n- To use an ACM certificate with Amazon CloudFront, you must request or import the certificate in the **US East** (N. Virginia) region. \n\n# Routing traffic to CloudFront\nUse alias record.\n\n# Aurora cross-region replication: Logical vs Physical replication\nYou can set up cross-region Aurora replicas using either physical or logical replication.\n## Physical Replication = Aurora Global Database\n- Dedicated infrastructure\n- Low-latency global reads\n- Disaster recovery\n- Replicate to up to five secondary regions with typical latency of under a second.\n\n## Logical Replication\n- Replicate to Aurora and non-Aurora databases, even across regions\n- Based on single threaded MySQL binlog replication\n- The replication lag will be influenced by the change/apply rate and delays in network communication between the specific regions selected.\n\n# ECS Service Load Balancing with ALB\nApplication Load Balancers offer several features that make them attractive for use with Amazon ECS services:\n- Each service can serve traffic from multiple load balancers and expose multiple load balanced ports by specifying multiple target groups.\n- They are supported by tasks using both the **Fargate** and EC2 launch types.\n- Application Load Balancers allow containers to use **dynamic host port mapping** (so that **multiple tasks** from the same service are allowed per container instance).\n- Application Load Balancers support **path-based** routing and **priority rules** (so that *multiple services* can use the *same listener port* on a single Application Load Balancer).\n\n# Fargate Task Storage\nFor Fargate tasks, the following storage types are supported:\n- Amazon EFS volumes for persistent storage.\n- Ephemeral storage for nonpersistent storage.\n\n# CodeBuild, CodePipeline, CodeDeploy\n\n## How CodeDeploy works\nAWS CodeDeploy is a fully managed deployment service that **automates software deployments** to a variety of compute services such as:\n- Amazon EC2, \n- AWS Fargate, \n- AWS Lambda, and \n- your on-premises servers. \n\n<img src=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/images/sds_architecture.png\" alt=\"\" width=\"600px\"/>\n\n- Deploys to **any instance**, including EC2.\n- The CodeDeploy agent must be **installed** and running on each instance.\n- How it works:\n    1. First, you create deployable content on your local development machine or similar environment\n    2. You add an application specification file (AppSpec file). The **AppSpec file** is unique to CodeDeploy. It defines the deployment actions you want CodeDeploy to execute. \n    3. You bundle your deployable content and the AppSpec file into an **archive file**, and then upload it to an **Amazon S3 bucket or a GitHub repository**. This archive file is called an application revision (or simply a revision).\n    4. You provide CodeDeploy with information about your deployment, such as which Amazon S3 bucket or GitHub repository to pull the revision from and to which set of Amazon EC2 instances to deploy its contents. \n    5. CodeDeploy calls a set of Amazon EC2 instances a **deployment group**. A deployment group contains individually tagged Amazon EC2 instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.\n    6. Each time you successfully upload a new application revision that you want to deploy to the deployment group, that bundle is set as the target revision for the deployment group. In other words, the application revision that is currently targeted for deployment is the target revision. This is also the revision that is pulled for automatic deployments.\n    7. Next, the CodeDeploy agent on each instance polls CodeDeploy to determine what and when to **pull** from the specified Amazon S3 bucket or GitHub repository.\n    8. Finally, the CodeDeploy **agent** on each instance pulls the target revision from the Amazon S3 bucket or GitHub repository and, using the instructions in the **AppSpec** file, deploys the contents to the instance.\n\n## Real Example: Automated AMI Builder\n\n<img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/21/ami-builder-diagram.png\" alt=\"\" width=\"600px\"/>\n\n## CodeBuild\nAWS CodeBuild is a fully managed **continuous integration** service that:\n- compiles source code, \n- runs tests, and \n- produces software packages\n\n### Running CodeBuild\nAWS CodePipeline is a fully managed **continuous delivery** service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. \n\n<img src=\"https://docs.aws.amazon.com/codebuild/latest/userguide/images/overview.png\" alt=\"\" width=\"500px\"/>\n\n- CodeBuild Console\n- CodePipeline Console\n- CLI or SDK\n\n### CodeBuild as a build or test action of a CodePipeline pipeline\n\n<img src=\"https://docs.aws.amazon.com/codebuild/latest/userguide/images/pipeline.png\" alt=\"\" width=\"400px\"/>\n\n## CodePipeline\n\n<img src=\"https://d1.awsstatic.com/product-marketing/CodePipeline/CodePipeline_Elements.1390531beabe7fd38b2414c39800136eed24e9c8.png\" alt=\"\" width=\"400px\"/>\n\nAWS CodePipeline integrates with AWS services such as:\n- AWS CodeCommit, \n- Amazon S3, \n- AWS CodeBuild, \n- AWS CodeDeploy, \n- AWS Elastic Beanstalk, \n- AWS CloudFormation, \n- AWS OpsWorks, \n- Amazon ECS,\n- AWS Lambda\n\n# AWS IoT Core\nAWS IoT Core provides secure, bi-directional communication for Internet-connected devices (such as sensors, actuators, embedded devices, wireless devices, and smart appliances) to connect to the AWS Cloud over **MQTT, HTTPS, and LoRaWAN**.\n\n# ELB Routing algorithm\n## ALB\nThe load balancer node that receives the request uses the following process:\n1. Evaluates the listener rules in **priority** order to determine which rule to apply.\n2. Selects a target from the target group for the rule action, using the routing algorithm configured for the target group. \n    - The default routing algorithm is round robin. \n    - Routing is performed independently for each target group, even when a target is registered with multiple target groups.\n\n### Routing ALgorithms supported by ALB\n#### Round Robin\nRound robin load balancing is a simple way to distribute client requests across a group of servers. A client request is forwarded to each server in turn. The algorithm instructs the load balancer to go back to the top of the list and repeats again.\n\n#### Least Outstanding Requests\nA \"least outstanding requests routing algorithm\" is an algorithm that choses which instance receives the next request by selecting the instance that, at that moment, has the **lowest number of outstanding (pending, unfinished) requests**.\n\n## NLB\n1. Selects a target from the target group for the default rule using a **flow hash algorithm**. It bases the algorithm on:\n    - The protocol\n    - The source IP address and source port\n    - The destination IP address and destination port\n    - The TCP sequence number\n2. Routes each individual TCP connection to a single target **for the life of the connection**. The TCP connections from a client have different source ports and sequence numbers, and can be routed to different targets.\n\n## Classic Load Balancer\n- Uses the **round robin routing algorithm** for TCP listeners\n- Uses the **least outstanding requests routing algorithm** for HTTP and HTTPS listeners\n\n# AWS Batch Scheduling\nThe AWS Batch scheduler evaluates when, where, and how to run jobs that have been submitted to a job queue. Jobs run in approximately the order in which they are submitted as long as all dependencies on other jobs have been met.\n\n# DynamoDB Continuous Backup\n- You can enable continuous backups with a single click in the AWS Management Console, a simple API call, or with the AWS Command Line Interface (CLI). \n- DynamoDB can back up your data with per-second granularity and restore to any single second from the time PITR was enabled up to the prior 35 days. \n\n# Secrets Manager - rotating RDS password\n- Because each service or database can have a unique way of configuring secrets, Secrets Manager uses a **Lambda** function you can customize to work with a selected database or service. You **customize** the Lambda function to implement the service-specific details of rotating a secret.\n- The `AWS::SecretsManager::RotationSchedule` resource configures rotation for a secret. \n\n","tags":["AWS","AWS SAP"]},{"title":"CompSci701 Testability Reflection","url":"/2020/10/30/CompSci701-Testability-Reflection/","content":"# CompSci701 Lecture Reflection: Testability \n\n## It's a Trade-Off\n- Encapsulation helps comprehensibility and alterability, but hinders testability.\n- For testability， I want to break encapsulation so that I have:\n    - Controllability, being able to directly control the state of objects\n    - Observability: being able to see directly see what's in the state.\n    \n### Having a well-encapsulated production class, and a transparent MOCK\nIn order to solve the dilemma, we separate the class being used in production and the class being used in tests.\n\n### Inject the MOCK in via DI\nIn order to allow the production class and the mock to be flexibly swapped, we use Dependency Injection. \n- We follow the Dependency Inversion principle, so that:\n    1. Both services, the production class and the mock class, extend the same abstract service\n    2. The client (where the service is injected) depends on abstraction.\n- We follow the Liskov Substitution principle, so that:\n    1. The mock service still has the functionalities  \n    2. We can safely substitute the service with the mock service, and let it \"pretend\" to be the real class, by good usage of inheritance.\n\n## Establishing Testing Criteria\nIn the definition of Testability, the establishment of test criteria, and the conduction of tests, are separated: \n\n> Testability — degree of effectiveness and efficiency with which **test criteria can be established** for a system, product or component and **tests can be performed** to determine whether those criteria have been met.\n\nWhat are good testing criteria that I should establish?\n\nHere are some of my thoughts:\n- The test criteria should match the requirements of the product owner:\n    - If the team have defined the user story well, the description of the user story is a good source of extracting items of testing criteria. \n    - If the user story does not tell us much about how it should be tested, maybe the team should review the story and see how to make the acceptance criteria clearer.\n    - This part of testing criteria is likely to be stable.\n- Given that unit tests often go into more details than the user story, the work structure breakdown will be important too.\n    - Important to internal quality.\n- The testing criteria **specific to the design decision** are the parts lead to tests that might have to be altered when the design is altered. \n\n## TDD: I'm not alone in not getting it 🎉 \n## but there is good stuff inside\nIt is really good to know that I am not the only one not getting the point of TDD. However, there are some insights that could be gained from the TDD philosophy:\n1. Writing tests beforehand is a good way to ensure that the team understands the **requirements** well before digging into coding. \n    - If we are not doing TDD, we still need to define the requirements well\n    - We could ask for \"Acceptance Criteria\" similar to test cases, although we might not be writing the actual tests until we have finished developing the actual code.\n2. TDD tends to lead to smaller classes because the class can only be unit tested if it is properly isolated and has a \"unit\" functionality. \n    - If the design is made following good OOP practice, we could still create small and good classes of SR without the facilitation of TDD.\n\n\n\n\n\n\n","tags":["Maintainable Software","CompSci701 Journal"]},{"title":"Design Patterns in One Image and One Sentence, and Alterability","url":"/2020/10/15/Design-Patterns-in-One-Image-and-One-Sentence/","content":"# Design Patterns in One Image and One Sentence, and Alterability\n\n## Composite Pattern\n\n### What it is\n\n**Treat parts and wholes in the same way.**\n\n<img src=\"composite.png\" alt=\"\" width=\"200px\"/>\n\n### Alterability\n1. To add a new kind of leaf class `L`: `L` is new code not counted in C; one class will be changed to construct from `L` -> C = L.\n2. To add a new composite class `Cm`: same.\n\n### Related Principles\n- Open-Closed Principle\n- Dependency Inversion: depend on the abstract component\n\n## Command Pattern\n\n### What it is\n\n**Pack some code to be executed into an object, so that the code can be executed at another class/time.**\n\n<img src=\"command.png\" alt=\"\" width=\"500px\"/>\n\n### Typical examples\n- GUI action implementations: Action listener (Attaching a piece of code to a button by adding an event listener. Executed when someone clicks the button).\n- Delaying command executing\n- Record commands: \"undo\"\n- Change location of command execution: have another computer execute the command (call the `execute()` method of the object.\n\n<img src=\"expression.png\" alt=\"\" width=\"500px\"/>\n\nThis is both Composite pattern and Command pattern.\n\n### Alterability\nAdding the new command does not add on to C. \n1. Adding a new possible behaviour of the receiver:\n    - Add a new method to the Receiver\n    - Use the new command in the client\n    - C = 2\n2. Add a new command that calls existing receiver behaviours:\n    - Use the new command in the client\n    - C = 1\n\n## Dependency Injection\n\n### What it is\n\n**Separates the use of service objects from the construction of service objects**\n\n<img src=\"dependency-injection.png\" width=\"300px\" alt=\"\"/>\n\n### Alterability\nAdd a new concrete service: C = 1 (Injector)\n\n### Related Principles\n- Dependency Inversion: Depend on Abstraction\n- Open-closed principle: abstract entities whose behaviour is extended by the concrete services supplied to it.\n\n## Strategy Pattern\n\n### What it is \n**Define a family of algorithms, encapsulate each one, and make them\n  interchangeable.**\n\n<img src=\"strategy.png\" alt=\"\" width=\"200px\"/>\n\nRelatable with Dependency Injection ⬆️\n\n### Alterability\n- Adding a new strategy: C=1 (context change)\n- Using a different strategy: C=1 (context change)\n\n## Observer Pattern\n\n### What it is \n\n**When one object (publisher) changes state, all its dependents (subscriber) are notified and updated automatically**\n\n<img src=\"observer.png\" alt=\"\" width=\"400px\"/>\n\n### Alterability\n- Adding a new concrete observer: C=1 (class doing the registration)\n- Adding a new concrete subject: C=1 (class using the subject)\n\n## Decorator Pattern\n### What it is\n**Extend functionality by subclassing**\n\n<img src=\"decorator.png\" width=\"500px\" alt=\"\"/>\n\n## Factory Method\n### What it is\n**Let subclassed factories decide which class to instantiate.**\n\n<img src=\"factory.png\" alt=\"\" width=\"300px\"/>\n\n## Template Method\n**Let subclasses redefine certain steps of an algorithm without changing the algorithm’s structure**\n\n<img src=\"template.png\" width=\"150\" alt=\"\"/>\n\n\n# MVC and Design Patterns\n1. Observer pattern\n    - UI components need to show the state of the system, and get updated when state changes\n2. Composite pattern\n    - Some UI components are supposed to behave the same way as their parents/the whole user interface\n3. Decorator pattern\n    - Some UI components must be associated with another element, and modifies some aspects of the element\n4. Strategy pattern \n    - Some UI elements provide a way for the user to interact with the UI\n    - Exactly how a user performs a specific action may differ\n5. Factory method\n    - The exact action/behaviour of some elements may change for different users\n    - Decision deferred and constructed through a template method\n    \n<img src=\"mvc.png\" alt=\"\" width=\"350px\"/>\n\n## MVC Alterability\nMVC allows a number of change cases to be implemented with only the minimum number of existing classes needing to change (C). Let's examine four change cases:\n1. Adding new functionality\n    - New classes, usually not by changing existing classes \n2. Change what state of the system has to be shown\n    - Change the model (if needed)\n    - Change ONLY the view affected (possibly only adding new view component class)\n    - No change on controllers\n3. Change how the state of the system is shown\n    - Change the relevant view class(s)\n    - No need to change model\n    - No need to change controller\n4. Change how the user performs actions in the system\n    - Change (or add) only the relevant controllers\n    - Usually no need to *change* any controllers\n\n# JUnit and Design Patterns\n1. Composite pattern\n    - `Test` is the Component, `TestSuite` is the Composite, and `TestCase` is the leaf. \n2. Command pattern\n    - `TestCase` is the command\n    -  Command relies on a single method to invoke it. Here it is `TestCase.run()`. \n    - This simple interface allows us to invoke different implementations of a command through the same interface.\n3. Template Method\n    - `run` is the template method, `setUp`, `runTest` and `tearDown` are the hook methods.\n4. Adapter pattern\n    - Make all the test cases look the same from the point of view of the invoker of the test.\n    - Convert the interface of a class into another interface clients expect.\n5. Collecting Parameter\n    - We only want to record the failures, and get a highly condensed summary of the successes.\n``` \npublic class TestResult extends Object {\n    protected int fRunTests;\n    public TestResult() {\n       fRunTests= 0;\n    }\n    public synchronized void startTest(Test test) {\n        fRunTests++;\n    }\n}\n\npublic class TestCase {\n    public void run(TestResult result) {\n        result.startTest(this);\n        setUp();\n        runTest();\n        tearDown();\n    }\n    public TestResult run() {\n        TestResult result= createResult();\n        run(result);\n        return result;\n    }\n    protected TestResult createResult() {\n        return new TestResult();\n    }\n}\n```\n\nSee [JUnit A Cook's Tour](http://junit.sourceforge.net/doc/cookstour/cookstour.htm) for more details.\n\n    ","tags":["Maintainable Software","OOP"]},{"title":"SOLID Principle","url":"/2020/10/05/SOLID-Principle/","content":"\n# SOLID Principle\n\n> Just **consequences** of doing OOB \"properly\".\n\n## Static: a bad idea\nI have been thinking if static fields are an awful idea for a while, and it is good to get some confirmation from Ewan's view. \n### Bad for alterability\nHaving a static field means we assume the field should be the same across all instances - it might be true for the current scenario, but the situation may change any time.\nIf this assumption is broken, we will have to modify the class and modify ALL of its clients.\n\n### Bad for comprehensibility\nThe reader has to keep track of the static field over the life cycles of all objects of the class - it may be changed any time. \n- This is not efficient because it requires reading more code from external presentation.\n- This is not effective because missing any changes to the static field will lead to very different understanding of the programme.\n\n## C/R Alterability heuristic revisited\n\n> \"Change Ratio\" C/N varies by change cases, then how is the change ratio used to assess the overall alterability of the design?\n\n### Look for the design that minimises C\n\"For likely and non-trivial impact change cases\"\n\n### Avoid dependency for change case\nAvoid designs where which design has lower CR highly depends on the change case\n\n## Single Responsibility Principle (SRP)\n\n### v.s. Responsibility-Driven Design\nResponsibility = an obligation to perform a task **or** know information\n\nWith this principle, you'll have classes with multiple \"responsibility\", because here the definition of \"responsibility\" is different to Martin's SRP.\n\n### SRP and Alterability\n\n#### Consider two designs:\n- Design A: A1, A2, A3, .... , AN\n- Design B: B1, A3, ... , AN. For Design B, B1 is the combo of A1 and A2.\n\n#### Consider possible change cases:\n1. Change case c1 requires Ai to change (i = 1...N)\n    - Design A: CR = 1/N\n    - Design B: CR = 1/(N-1)\n    - For c1, A is more changeable than B\n2. Change case c2 requires Ai and Ak to change (i + k > 3)\n    - Design A: CR = 2/N\n    - Design B: CR = 2/(N-1)\n    - For c2, A is more changeable than B\n3. Change case c3 requires Ai and Ak to change (i + k = 3)\n    - Design A: CR = 2/N\n    - Design B: CR = 1/(N-1)\n    - For c3, B is more changeable than A, when N > 2.\n\n#### Conclusion\n- For **most** change cases, the C/N of Design A will be smaller than Design B.\n- Therefore, Design A has a better overall alterability than Design B.\n\n### SRP and Design Patterns\n> Many of the elements of a design pattern are there to do just one thing.\n\nExamples:\n1. Decorator: each decorator\n2. Composite: each leaf and composite\n3. Command: each concrete command\n4. Dependency Injection: each service, and the client\n5. Strategy: each concrete strategy\n\n### SRP Conclusion\n- \"One responsibility\": not actionable\n- The “one reason to change” or “group things that change for the same reason together” definitions are perhaps easier to action, because the reasoning can be done in terms of change cases.\n\n## Open Closed Principle (OCP)\n\n> F**undamentally, we create classes and we do not intend them to change. If a new behaviour is needed, we extend instead of modifying**.\n\nThe real power comes from judicious use of **polymorphism**, that is identifying the method to execute via dynamic dispatch.\n\n### Consequence\nTypically leads to more **abstract entities** that are made concrete either through:\n- extension, or\n- supplying the concrete implementations as parameters (e.g. with dependency injection)\n\n### What is open, what is closed?\n- Implementation of method is closed for modification, but open to being overriden/extended by subclasses. \n- Contexts of method (where the method is being called) is closed, yet by polymophism the behaviour is open to be varied.\n\n### OCP and Alterability\nReducing C.\n\n### Actionable\n- When we change something, anything that needs to change is not closed to change.\n- When designing something, think about what variations are needed for the context schema.\n\n### Polymorphism!\nThe core of change. \nProper use of *polymorphism* lies at the core of OOP.\n\n## Liskov Substitution Principle (LSP)\n> **Child classes must be substitutable for their Parent classes**.\n\n- Code reuse is not very interesting.\n- **Context reuse** is more interesting: the context can remain unchanged, but program behaviour changes.\n\n**Any design pattern that relies on inheritance (pretty much all of them) is based on LSP**.\n\n## Interface Segregation Principle\n> **Make fine-grained interfaces that are client-specific.**\n\nBecause we are limiting between the knowledge exposed to the client, we are reducing the possibility that the client needs to change according to the class.\n\n### ISP and Alterability\nReduce the impact of change -> Reduce C.\n\n### ISP and Design Patterns\ne.g. Observer\nCommand typically only have `execute()`\n\n## The Dependency Inversion Principle (DIP)\n\n### Actionable\nLook for opportunities to make fields abstract instead of concrete.\n\n\n\n\n\n\n","tags":["Maintainable Software","CompSci701 Journal"]},{"title":"What is Dependency Inversion Principle","url":"/2020/10/04/What-is-Dependency-Inversion-Principle/","content":"\n# What is Dependency Inversion Principle(DIP)\n\n1） High-level modules should not depend on low-level modules. Both should depend on abstractions. 高层模块不应该直接依赖低层模块，两者都应该依赖抽象层。\n2） Abstractions should not depend on details. 抽象不能依赖细节，细节必须依赖抽象。\n\n## What are \"Modules\"\nCan be varied based on the context:\n- From the view of architecture, \"modules\" can be subsystem\n- From the view of subsystem, \"modules\" can be components\n- From the view of components, \"modules\" can be classes\n\n## What are \"Dependency\"\nCan also be varied based on the context:\n- High-level modules should not \"depend\" on low-level modules: high-level modules should not **call methods** of low-level modules\n- High-level modules \"depend\" on abstraction: high-level modules **call methods** of abstraction (interface/abstract class)\n- Low-level modules \"depend\" on abstraction: low-level modules **implement/extend** abstraction (interface/abstract class)\n\n\n\n","tags":["Maintainable Software","CompSci701 Journal"]},{"title":"Build my first application with AWS Serverless infrastructure - Client","url":"/2020/09/21/Build-my-first-application-with-AWS-Serverless-infrastructure-Client/","content":"\nThis is the client side notes for my first AWS Serverless infrastructure. API notes are [here](https://niuniuanran.github.io/2020/09/20/Build-my-first-application-with-AWS-Serverless-infrastructure-api/)\n\n# AWS Amplify\n\nAWS Amplify is a set of tools and services that enables **mobile and front-end web developers** to build secure, scalable full stack applications, powered by AWS.\n\nI leveraged three modules of Amplify:\n1. Auth (Cognito User Pool and Cognito Identity Pool)\n2. Storage (S3)\n3. API (API Gateway)\n\n## Configuration\nAt the start of the programme, I need to specify the configuration of Amplify. In this project, I did it in [`src/index.js`](https://github.com/niuniuanran/NS3-client/blob/master/src/index.js) which runs upon loading the client side.\n\nThe corresponding Serverless Stack chapter is [Configure AWS Amplify](https://serverless-stack.com/chapters/configure-aws-amplify.html).\nThe corresponding Amplify Docs page for Javascript Amplify Libraries is [Configure Amplify Categories](https://docs.amplify.aws/lib/client-configuration/configuring-amplify-categories/q/platform/js). There are top-level and scoped configurations.\n\n## Using Amplify\n\nOnce Amplify is configured, it is able to make requests to AWS resources on the client's behalf. Just like the API handlers, Amplify could use the Cognito Identity session to access the resources private to the current user. \n\n# react-route-dom\n\n## useLocation\nThe useLocation hook returns a `Location` object that represents the **current URL**.\n\nThe `Location` interface is explained very well in the [MDN docs](https://developer.mozilla.org/en-US/docs/Web/API/Location). It has these parts:\n1. `href` the whole URL\n2. `protocol` http/https\n3. `host` localhost:3000\n4. `pathname` /foo/bar\n5. `search` ?q=baz\n6. `hash` #header1\n\n## BrowserRouter\nA <Router> that uses the HTML5 history API (pushState, replaceState and the popstate event) to keep your UI in sync with the URL.\n\nYou wrap the `<App/>` within the `<BrowserRouter>` to have access to the `<Switch>`/`<Route>`/`<Link>` etc.\n\n## Switch and Route\n- `<Switch>` is unique in that it renders a route exclusively. It will look through the `<Route>`s from top to bottom and render the first match. We could create a `<NoMatch>` and put it into the last `<Route>` as a 404 page.\n- In contrast, if we don't wrap the `<Route>`s into `<Switch>`, every `<Route>` that matches the location renders inclusively. This is a delicate design to enable us to compose our `<Route>`s.\n\n## Redirect\nRendering a <Redirect> will navigate to a new location. The new location will **override the current location in the history stack**, like server-side redirects (HTTP 3xx) do. So it is a good way to elegantly route unauthenticated user away from locations that require authentication, and they will not get back to the authenticated locating by hitting the \"Back\" button.\n\n# AuthenticatedRoute and UnauthenticatedRoute\nThere are some pages that we only want the user to see when they are not authenticated, for example, `/signup` and `/login`; There are also pages that we only want authenticated user to see, for example, `/notes/{id}`.\n\nTo achieve this, we can wrap `Route` into `AuthenticatedRoute` and `UnauthenticatedRoute` components, and ensure the user is redirected elegantly if they wade to somewhere they don't belong.\n\nThe step-through guideline in Serverless Stack is [Create a Route That Redirects](https://serverless-stack.com/chapters/create-a-route-that-redirects.html).\n\n# Extract query string with given name by Regex\nThis is a pretty cool usage of `RegExp` and its `exec` method.\n\n```javascript\nfunction querystring(name, url = window.location.href) {\n    name = name.replace(/[[]]/g, \"\\\\$&\");\n\n    const regex = new RegExp(\"[?&]\" + name + \"(=([^&#]*)|&|#|$)\", \"i\");\n    const results = regex.exec(url);\n\n    if (!results) {\n        return null;\n    }\n    if (!results[2]) {\n        return \"\";\n    }\n\n    return decodeURIComponent(results[2].replace(/\\+/g, \" \"));\n}\n```\n\nGiven `name` of value `\"redirect\"` and `url` of value `\"http://localhost:3000/login?redirect=/notes\"`, the result of `regex.exec(url)` is:\n\n```javascript\n[\n  '?redirect=/notes/26e9b6b0-fb14-11ea-bcae-ef1bd0468689',\n  '=/notes/26e9b6b0-fb14-11ea-bcae-ef1bd0468689',\n  '/notes/26e9b6b0-fb14-11ea-bcae-ef1bd0468689',\n  index: 27,\n  input: 'http://localhost:3000/login?redirect=/notes/26e9b6b0-fb14-11ea-bcae-ef1bd0468689',\n  groups: undefined\n]\n```\n\nThe array of the first results are refined step by step with the `()`s specified in the `RegExp` construction: `\"[?&]\" + name + \"(=([^&#]*)|&|#|$)\"`.\n\nThe [`decodeURIComponent()`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/decodeURIComponent) function decodes a Uniform Resource Identifier (URI) component previously created by encodeURIComponent or by a similar routine. `%20%28correct%29` decoded -> `(correct)`\n\n","tags":["Web Development","AWS","Serverless"]},{"title":"Build my first application with AWS Serverless infrastructure - API","url":"/2020/09/20/Build-my-first-application-with-AWS-Serverless-infrastructure-API/","content":"\nToday (20 Sep 2020) I started working on my first full-stack Serverless app with the [Serverless Stack](https://serverless-stack.com/) project.\nThis is the API side notes for my first AWS Serverless infrastructure. API notes are [here](https://niuniuanran.github.io/2020/09/21/Build-my-first-application-with-AWS-Serverless-infrastructure-client/)\n\n# Cognito User Pool\n\nCreated my first Cognito User Pool. There are two settings to be noted:\n\n1. **DISABLE client secret generation**: user pool apps with a client secret are not supported by the JavaScript SDK. We need to un-select the option.\n2. **Enable username password auth for admin APIs for authentication**: required by AWS CLI when managing the pool users via command line interface. We will be creating a test user through the command line interface in the next chapter.\n\n# Serverless Framework\n\nThe [serverless framework](https://www.serverless.com/) makes it easier for developers to make serverless applications locally.\n\nSee [here](https://serverless-stack.com/chapters/setup-the-serverless-framework.html) for the detailed tutorial on setting up a node server from the serverless framework starter.\n\n# Lambda Handler\n\n- Lambda handlers are invoked when a request is made to the API. Serverless Framework takes care of the mapping between the API endpoint and the handler, as long as we specify it in the `functions` part of the `serverless.yml`.\n- Lambda Handlers need to call DynamoDB to persist data, using the DynamoDB SDK for Node.js. Serverless comes with a transpiler between Node.js and Javascript/TypeScript.\n\n# DynamoDB SDK\n\nSee [Class: AWS.DynamoDB documentation](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB.html). When configuring the Lambda handlers, you can configure the params as needed by the methods and call these DynamoDB methods. With [Serverless Framework](https://www.serverless.com/framework/docs/providers/aws/guide/iam/), the Lambda is given the permission to work on DynamoDB in the `iamRoleStatements` section in [serverless.yml](https://github.com/niuniuanran/serverless-notes-api/blob/master/serverless.yml).\n\n## Add rights to the Lambda functions by IAM\n\nTo add specific rights to this service-wide Role, define statements in provider.iamRoleStatements which will be merged into the generated policy. As those statements will be merged into the CloudFormation template, you can use Join, Ref or any other CloudFormation method or feature.\n\n# Cognito Identity Pool v.s. Cognito User Pool\n\n<img src=\"https://d33wubrfki0l68.cloudfront.net/4602d3b127c9b3f1dbe49f9fc77e8d8a4aff20a6/9c3a1/assets/cognito-user-pool-vs-identity-pool.png\" alt=\"identity-pool-vs-user-pool\" width=\"600px\"/>\n\n- Cognito User Pool handles user registration, authentication, and account recovery\n- Cognito Identity Pool provides authorization for users to use the various AWS services.\n- The Cognito Identity Pool simply takes all your identity providers and puts them together (federates them). And with all of this it can now give your users secure access to your AWS services, regardless of where they come from.\n\n# Infrastructure as Code\n\nServerless Framework supports defining CloudFormation for the infrastructure.\n","tags":["Web Development","AWS","Serverless"]},{"title":"Logical Clocks and Distributed Snapshots","url":"/2020/09/07/Logical-Clocks-and-Distributed-Snapshots/","content":"\n# \"Happens Before\"\n\nEvent a *happens-before* b, a->b, iff:\n- Internal event: a and b occur in the same node\n- Same message, a is send and b is receive\n- a->c, c-> b: transitive\n\n## Partial Order\n*happens-before* relationships form a DAG (directed asynchronous graph)\n\n# Logical Time, Logical Clock\n\n## Logical Time\n\nLogical time is a mapping C from **events** to a **partially ordered set**.\n\n```a->b => C(a) < C(b)```\n\nIf a happens before b, then the logical time of a is smaller than the logical time of b.\n\n## Logical Clock\n\n### Must-have features:\n1. Unique, one-to-one. You cannot let two nodes share the same logical clock\n2. Determined by nodes themselves\n\n### The Good feature\n\"Essentially\" same executions will lead to same local times of nodes. \n\n1. A good logical clock capture only the \"essential\" charasteristics of the execution.\n2. Non-deterministic delays/labs will not lead to changes in logical time.\n\n### The Ideal feature\n  ```a->b <=> C(a) < C(b)```\n1. The value of the logical time indicates the order of events. \n2. If two events do not have a happens-before relationship, then their logical time should not be comparable.\n3. If the clock maps to a *totally ordered set*, i.e. any two logical times are comparable to each other, then it is not ideal. This is because even if two events do not have a happens-before relationship, their logical time will still be comparable.\n\n## Assessing the Logical Time/Logical Clock in #A1\n\n### What is the logical time in this algorithm?\n`Message.time`.\n\nGiven two message events (either receive or send) `a` and `b`, \n\n`a` happens before `b` => `a.time` < `b.time`\n\n### Must-have features of a logical clock?\nNo. Unique, but *not* determined by node themselves.\n\n### The ideal feature of a logical clock?\nNo. The integer set is totally ordered.\n\n\n\n\n\n    \n        \n        \n\n    \n\n# Lamport Logic Clock\n\n## Implementation 1\n\n- Each node has its own logical clock, initial: `clock = 0`\n- Before each internal/send event, `clock += 1`\n- Each sent messages carries the `sender`'s clock\n- After receiving a message, `clock = max(clock, receivedClock) + 1`\n\n<img src=\"lamport-1.png\" alt=\"\" width=\"600px\"/>\n\n- We have a \"total order\" logical time: a->b => Lamport(a) < Lamport(b)\n- We have one must-have feature - each node manages their own logical clock\n- We lost the other must-have feature: **unique**\n\n## Implementation 2\nEnsure uniqueness by adding process IDs as the second field: lexicographic order (递归字典顺序)\n\n<img src=\"lamport-2.png\" alt=\"\" width=\"600px\"/>\n\nNow we ensure **uniqueness**\n\nStill, not ideal: (1,3) < (2,2), 根据lexicographic递归字典顺序，看到第一位1<2就可以得出结论了。 But (1,3) and (2,2) are concurrent. They should not be comparable.\n\n# Vertical Logical Clock\n- Each process keeps a vector (tuple) containing all known local logical times.\n- These tuples are ordered on each component, not lexicographically. (只是前面的一位比较不够，每一位都需要比较)\n\n<img src=\"vertical-clock.png\" alt=\"\" width=\"500px\"/>\n\n- Each process increments its local logical time before local/send event.\n- At receive event:\n\n<img src=\"vertical-receive.png\" alt=\"\" width=\"500px\"/>\n\n# Global Snapshot: Count Money\n\nThe snapshot will count in:\n1. The amount already in the node at time t\n2. The money already sent to the node, but are still in transit\n\nTherefore to take the snapshot:\n- For each channel, keep adding all incoming money, until you get the first message sent after time t (marker message)\n\n## Non-FIFO work-around\n\nAdd sequence number for each channel\nEnsure that you sum all messages sent before the \"markers\"\n\n\n\n\n\n\n\n\n\n\n","tags":["Distributed Algorithms"]},{"title":"AWS Share and Copy AMI","url":"/2020/09/03/AWS-Share-and-Copy-AMI/","content":"\n# Copy AMI\n\n<img src=\"copy-ami.png\" alt=\"\" width=\"600px\"/>\n\n- You can copy AMI to another region.\n- You can give it a different name.\n- You can encrypt it.\n\n# Share AMI\n\n<img src=\"share-ami.png\" alt=\"\" width=\"600px\"/>\n\n- You can share an AMI with another account, or make it publicly available.\n- Sharing an AMI does not affect the ownership of the AMI.\n- If you copy an AMI that has been shared with your account, you are the owner of the target AMI in your account.\n- To copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either the associated EBS snapshot (for an Amazon EBS-backed AMI) or an associated S3 bucket (for an instance store-backed AMI).\n- You can't copy an AMI with an associated billingProduct code that was shared with you from another account. This includes Windows AMIs and AMIs from the AWS Marketplace. To copy a shared AMI with a billingProduct code, launch an EC2 instance in your account using the shared AMI and then create an AMI from the instance.\n- You can't copy an encrypted AMI that was shared with you from another account. Instead, if the underlying snapshot and encryption key were shared with you, you can copy the snapshot while re-encrypting it with a key of your own. You own the copied snapshot, and can register it as a new AMI.\n\n\n\n","tags":["AWS","AWS SAA"]},{"title":"Inject a Singleton Test object into Controller's Constructor","url":"/2020/09/02/Inject-a-Singleton-Test-object-into-Controller-s-Constructor/","content":"# Add a Test singleton in StartUp.ConfigureServices\n\nIn `Startup.js`:\n```csharp\n// This method gets called by the runtime. \n// Use this method to add services to the container.\n\npublic void ConfigureServices (IServiceCollection services) {\n    services.AddControllers ();\n    services.AddSingleton<Server.Controllers.Test>();\n}\n```\n\n`services.AddControllers ();` will add all the classes that extend `ControllerBase` into the service collection;\n\n`services.AddSingleton` will add a singleton of the class we specify.\n\n# Inject the Test singleton when Controller is constructed\n```csharp\npublic WeatherForecastController (ILogger<WeatherForecastController> logger, Test test) {\n    _logger = logger;\n    _test = test;\n}\n```\n\nWe can add as many arguments (objects) into the controller's constructor as we need.\nA `WeatherForecastController` object will be constructed each time a request is made. Now each time the controller is constructed, it will know about the `Test` object and can use it in its methods.","tags":["Web API","dotNET"]},{"title":"AWS Auto Scaling and Load Balancing","url":"/2020/09/02/AWS-Auto-Scaling-and-Load-Balancing/","content":"\n# Amazon EC2 Auto Scaling lifecycle hooks\nLifecycle hooks enable you to perform custom actions by **pausing** instances as an Auto Scaling group launches or terminates them.\n\nEach Auto Scaling group can have multiple lifecycle hooks. However, there is a limit on the number of hooks per Auto Scaling group.\n\nLifecycle hooks per Auto Scaling group: 50\n\n# Amazon Event Bridge\n\nAmazon EventBridge is a serverless event bus service that makes it easy to connect your applications with data from a variety of sources. EventBridge delivers a stream of real-time data from your own applications, Software-as-a-Service (SaaS) applications, and AWS services and routes that data to targets like AWS Lambda. \n\n# Scaling Policies, Scheduled Actions\nScaling policies per Auto Scaling group: 50\nScheduled actions per Auto Scaling group: 125\n\nThese two can co-exist.\n\n# EC2 and ELB Health Check\nAmazon EC2 Auto Scaling can **determine the health status of an instance** using one or more of the following:\n- Status checks provided by Amazon EC2 to identify hardware and software issues that may impair an instance. The default health checks for an Auto Scaling group are EC2 status checks only.\n- Health checks provided by Elastic Load Balancing (ELB). These health checks are disabled by default but can be enabled.\n- Your custom health checks.\n\n## Health checks provided by Elastic Load Balancing (ELB)\nELB health check verifies if the instance responds to a certain request as expected (e.g. 200 status). Healthy indicates that an instance has passed the Load Balancer's health check. This means that the Load Balancer will send traffic to the instance. \n\n### ALB Health Check\nProtocols:\n- HTTP\n- HTTPS\n\nExamines the response code.\n\n### NLB Health CHeck\nProtocols:\n- TCP\n- HTTP\n- HTTPS\n\n\n","tags":["AWS"]},{"title":"Reading Journal - Scrum and XP from the Trenches by Henrik Kniberg","url":"/2020/09/01/Reading-Journal-Scrum-and-XP-from-the-Trenches-by-Henrik-Kniberg/","content":"\n(28 Aug 2020)\nToday I started reading Scrum and XP from the Trenches, written by Henrik Kniberg. I knew about XP from the book Refactoring, and thought that this book will be a nice complementary reading for CompSci701, as Scrum/XP has so much to do with trade-offs and scope decisions. \nEnhancing comprehensibility/alterability/testability takes time, but pays off richly as a key part of the **internal quality** - quality that product owners/users cannot see.\n\nI am reading the English and Chinese versions as a pair - Chinese version for the speed and English version to map key terms. The Chinese version is translated by 李剑. \n\n# Story/Backlog\nI tried working with User Stories when working on TripTime, and found that a very helpful way of keeping track of our project progress, making decisions about priorities and trade-offs, and organising end-to-end tests. \n\nHenrik gives his recommendation on the six key entries that should be included in a story:\n1. ID\n2. Name (descriptive, brief, differentiable)\n3. Importance (each story should have different value!)\n4. Initial estimate: story point/man*day estimation (should have relative correctness)\n5. How to demo (could be used as pseudo code for **Acceptance Testing** - I did similar thing on TripTime but called it acceptance criteria)\n6. Notes (referring to supportive information, explanations, examples)\n\n## Who can decide what\nPeople other than the product owner can add stories to backlog, but:\n- Only Product Owner can decide the **importance**.\n- Only Developing Team can add **time estimation**.\n\n## Don't compromise on internal quality\nTherefore, as a developer, I should be aware that I need to budget the time for refactoring and enhancing the maintainability of my code. The product owner might want to negotiate on this - they might want to get something that \"works\" (external quality) faster, without giving the time for internal quality. We shall not compromise on internal quality: ask them to make the scope smaller instead.\n\n# Sprint Plan Meeting\n## Why product owner MUST be there\nEvery story has three variables, and they have strong dependency on each other:\n\n- Scope (decided by product owner)\n- Importance (decided by product owner)\n- Estimation (decided by developers)\n\n**Direct collaboration on dynamically adapt this triangle is the foundation of Scrum.**\n\n## Why never compromise on Internal Quality\nA third variable: Quality\n- External Quality\n    - what users can feel\n    - **Part of Scope: negotiable with product owner**. A slower system with simpler UI could be released first, followed by a cleaner one.\n- Internal Quality\n    - what users cannot see\n    - long-term impact on maintainability and lifelong cost of ownership\n    - **Non-negotiable**: always ensure internal quality!\n\nHow to negotiate when the product owner wants a shorter estimated time but doesn't want to \"pay\" by external quality: **Redirect the topic back to scope**:\n- Maybe we could simplify error handling in the current story, and put \"Advanced error handling\" as a separate story to be implemented later?\n- Maybe we could reduce the priority of other stories, so that we can focus on delivering the current story sooner?\n\n> **Once the product owner realise that the internal quality is non-negotiable, they are usually able to go and address other variables.**\n\n## Timeboxing\nTimeboxing is equally helpful for sprint and **meeting**. \nIf the sprint meeting has reached the pre-defined timebox and no valid plan is made, still end the meeting and learn from it. Next time there will be less rejection on a longer timebox for planning meeting, after the team suffers from the poorly planned sprint.\n\nThis methodology will be helpful to my practices other than code. I tend to push families/friends/teammates to reach the goal, no matter how long it takes, and sometimes we just give up because it has been so long since we embarked for the goal. \nWe should **box the time instead of the scope**. It takes practice to make a good estimation of what can be done within the box, but this is the better approach for more efficient team work.\n\n## Sprint Length: Compromises and dynamic adaptations\n\n- Shorter = More frequent delivery = More frequent Feedback = Less time on the wrong direction = Faster learning and improvement = agile movement, Often preferred by product owner.\n- Longer = More time to prepare and solve problems = Less tedious Meetings and Presentations = More \"flow\" working state, Often preferred by developing team.\n\nNo need to spend time analysing. Get started with an acceptable length, and commit to a length after the team knows what suit us.\nScrum is the spirit to do, to experiment and to swiftly adapt. The spirit applies both to developing and to other decision making in my life.\n\n## Sprint Goal: You have to have one\n- Express it with business terms instead of technology terms.\n    - Make more money\n    - Impress CEO\n    - Make the system good enough to be released to real users as a beta version\n    - Implement the three most important storeis\n- \"Why this sprint? Why don't we just go for a holiday?\"\n- You come back to the sprint goal during the sprint, when you are confused about what you are doing.\n- Sprint goals of all teams could be listed on a significant position of the companies' wiki page, so that everyone in the company can come and have a look what the company is doing 👊\n\n## Decide what stories to include\n\n<img src=\"what-to-include-in-sprint.png\" alt=\"\" width=\"400px\"/>\n\n- Each rectangle is a story, ranged by importance\n- Size is the estimated time by story points\n- Height of brace means estimated velocity (story points that could be finished in next sprint)\n\n### Decide the velocity\nThese three methods could be used in combination to decide the team's velocity for the current sprint:\n- Intuitive reactions: good for small team\n- Yesterday's weather: Estimate how many stories can be included based on the velocity history of the team.\n- Man-Day available * Focus Factor estimation\n\n#### Focus Factor * Man-Day Available estimation\n<img src=\"focus-factor-2.png\" alt=\"\" width=\"400px\"/>\n\n<img src=\"focus-factor.png\" alt=\"\" width=\"400px\"/>\n\n<img src=\"focus-factor-3.png\" alt=\"\" width=\"400px\"/>\n\nNow we know we should include no more than 20 points for the current sprint, with the 50 man-days available.\n\n#### \"Almost done\" is not included in actual velocity\nThe requirement of Scrum: **Get things done to a deliverable state**\nIf the thing is \"almost done\", its value is 0 (or maybe negative)\n\n### Story Index Card and Work Breakdown Structure \n- Physical Story Index Cards that encourage engagement of the whole team.\n- Sticky note under the Index Cards for task breakdown. These details do not need to be in backlog.\n\nThis will require the Scrum Master to manually update the excel from changes of the physical cards, but is worth it considering the enhancement of meeting plan efficiency.\n\nThis relates back to a video, [Work Breakdown Structure demonstration](https://www.youtube.com/watch?v=FklYonNknRs) that I watched at project management course. Now the man's talk make more sense.\n\n## Defining \"Done\"\nThere has to be a consensus on how to define \"DONE\" between the product owner and the developer team.\nPossible definitions:\n- Ready to be realeased any time\n- On the testing server, ready for acceptance test\n- The testing team says OK\n\nAn entry in each story stating how to define \"DONE\" for it would be helpful, as each story may vary.\n    \n## Planning Poker\nWe used this at the CompSci718/719 Project debriefing!\n\nEvery member should have a certain understanding of the story. The Planning Poker approach ensures that everyone is actively thinking.\n\n## Story v.s. Tasks\n- Stories are deliverable. \n- Product owners care about stories.\n\nBreak stories into smaller stories at the sprint planning meeting, in collaboration with product owners.\nBreak storeis into tasks with the developer team, which helps deciding the story points.\n\n### Tasks in TDD\nAlmost every stories' first task is \"write a test that fails\".\nThe last task is \"refactoring\" (Remove any duplicate code; improve comprehensibility)\n\n\n","tags":["Maintainable Software","Good Practices"]},{"title":"Reading Journal - The Transformation Priority Premise, by Robert C. Martin","url":"/2020/08/20/Reading-Journal-The-Transformation-Priority-Premise-by-Robert-C-Martin/","content":"\nWill read [The Transformation Priority Premise](https://blog.cleancoder.com/uncle-bob/2013/05/27/TheTransformationPriorityPremise.html), as promised in the [Evolutionary Design](https://niuniuanran.github.io/2020/08/20/What-is-Evolutionary-Design/) journal.\n","tags":["Maintainable Software","WIP","Good Practices"]},{"title":"What is Evolutionary Design","url":"/2020/08/20/What-is-Evolutionary-Design/","content":"\nAs I promised myself in my [Reading journal: Refactoring, by Martin Fowler](https://niuniuanran.github.io/2020/08/20/CompSci701-Reading-Journal-Refactoring-Martin-Fowler/) I shall find out more about this concept, because:\n- I am having confusion on how much (if any) I should leave spaces for future changes in my programme, or should I just focus on what is currently needed.\n- I want to know if this concept will offer me more insights in understanding requirements and creating/understanding user stories.\n\n# What is Evolutionary Design\n\nEvolutionary Design is:\n- one of these techniques that helps us grow a system well by understanding its **current characteristics**.\n- an approach to incrementally grow a system, while \n    - observing growth patterns\n    - normalising and optimising the growth\n    \nTo practice evolutionary design we:\n- add the **minimum** amount of code to satisfy the business needs in an **iterative and incremental approach**.\n- Continuously evolve the **code structure** to optimize for change, thus allowing a constant speed of development for longer periods of time.\n\n# Evolutionary Design Cycle\nRed -> Green -> Refactor. \nPass the test, then refactor the code to optimise.\nRefactoring means changing the existing structure of the code without changing its behavior. Unit Testing is our **safety net** when refactoring the code with TDD.\n\n# Patterns of growth\nThe three steps of growth patterns, discussed by [Adi Bolboaca](https://mozaicworks.com/author/adi-bolboaca/):\n## Normalizing growth\nNormalizing Growth is the process of observing **irregular developments** in a system, and refactoring it so it grows nice and balanced. \n## Optimizing growth\nOptimizing Growth means we **choose a growth outcome**. \nOptimization means compromising, choosing your way. \n\nSoftware system cannot be very easy to change and with great performance. Often you need a balance between the two.\n\n## Maximizing growth\nWe want to make the most out of each optimization. \nOften the blooming period is critical. With our software system we need to understand when that moment arises.\nWe can observe patterns of evolution like Low Coupling, High Cohesion(凝聚), Testing Strategy, and many others.\n\nAdi recommends [Transformation Priority Premise by Robert Martin](https://blog.cleancoder.com/uncle-bob/2013/05/27/TheTransformationPriorityPremise.html) as an example for identifying such a pattern.  -- Will do.","tags":["Maintainable Software","Good Practices"]},{"title":"Reading Journal - Refactoring, by Martin Fowler","url":"/2020/08/20/CompSci701-Reading-Journal-Refactoring-Martin-Fowler/","content":"\n# =============30/07/2020=============\n\nToday I started reading the book recommended at CompSci701 class:\nMartin Fowler (feat. Kent Beck) Refactoring: Improving the Design of Existing Code. Addison-Wesley (2012).\n\nI am reading the Chinese translated version, translated by Mr 熊节. I will write my note back in English to reinforce learning.\n\n# Introduction\n## When to Refactor\nThis is a nice description for when to refactor, which could help me consider how I improve my assignment.\n> If you want to add a new feature to the program, and the structure of the code makes it inconvenient to do so, then refactor before adding the feature.\n\n## Before refactoring\n\n**Good tests are the foundation for refactoring!**\n\nRefactoring could introduce bugs, so write a reliable testing environment to ensure you don't break things during refactoring.\n\n## Extract methods\nThe smaller your code block, the easier to manage, change and move them.\nTo extract methods, you need to find the local variables and parameters.\nAny **unchanged** variables in the long block could be passed to the extracted method/function as arguments.\nBe careful with changed variables\". If there is only one variable's value being changed, it can be the return value of the new argument.\n\n## Placing methods in the right place\nMost of the time, functions/methods should be put in where the data it uses belongs. This is relatable to a static method I wrote for Kalah:\n\n```java   \n    /**\n     * Generate a pair of player partner for Kalah game.\n     * */\n    public static KalahPlayer[] createNewPlayerPair() {\n        KalahPlayer[] newPlayerPair = new KalahPlayer[2];\n        newPlayerPair[0] = new KalahPlayer(DEFAULT_PLAYER_ONE_NAME);\n        newPlayerPair[1] = new KalahPlayer(DEFAULT_PLAYER_TWO_NAME);\n        newPlayerPair[0].setOpponent(newPlayerPair[1]);\n        newPlayerPair[1].setOpponent(newPlayerPair[0]);\n        return newPlayerPair;\n    }\n```\n\nTo start with it was in the `Kalah` class because this is where it is used.\nThen I moved it to `KalahJudge` class because I don't want `Kalah` to know too much about processes other than driving the game state.\nFinally I moved it to `KalahPlayer`, because:\n- `KalahPlayer` is all this method deals with and needs to know;\n- If I put this \"constructor\" method in `KalahPlayer`, I will be able to make `KalahPlayer`'s constructor private, because this should be the only way to create a valid `KalahPlayer` instance.\n\n## Try to get rid of temporary variables\nTemporary variables could be problems. They are only valid in the function they belong to, and therefore could contribute to a long and complex function. It is easy to lose track of temporary variables in a long function.\n\n## Performance issue in refactoring\nReplacing temporary variables with query **could** cause performance issue. For example, there might need to be extra loops in the query method. It says **could** because you never know until you run assessments on the running time. You do not need to worry about this when you refactor. Worry about them when you are optimising - by that time you are at a good position with well-refactored code.\n\n## Don't `switch` on another object's attribute\nWith the `Movie` and `Rental` example in the book, `getCharges` switch among different movie types.\n\nThis method is originally in the `Rental` class and uses the `getDaysRented()` method belonging to `Rental` and the `getPriceCode()` method belonging to `Movie`:\n```java\ndouble getCharge()\n{ double result=0;\n  switch(getMovie().getPriceCode()){\n    caseMovie.REGULAR:\n      result+=2;\n      if(getDaysRented()>2)\n        result+=(getDaysRented()-2)*1.5;\n      break;\n    caseMovie.NEW_RELEASE:\n      result+=getDaysRented()*3;\n      break;\n    caseMovie.CHILDRENS:\n      result+=1.5;\n      if(getDaysRented()>3)\n        result+=(getDaysRented()-3)*1.5;\n      break;\n  }\n}\n```\n\nIt should be moved to `Movie` class and pass the result of `getDaysRented` as an argument.\nThis is because this method depends on how many possible `priceCode` there are in `Movie`.\nIf the potential change to this system is an addition of new movie type/`priceCode`, we want to **contain the change caused by adding new movie types** within the `Movie` class.\n\n## State pattern by Gang of Four\n\nThis looks like the Strategy pattern, but is different because `Price` class represents a \"state\" of the movie, instead of a \"behaviour\". It could be updated from `NEW` to `REGULAR` after some time.\n\n<img alt=\"\" src=\"https://github.com/niuniuanran/niuniuanran.github.io/blob/master/2020/01/01/Image-Vault/state-pattern.png?raw=true\" width=\"700px\"/>\n\nRelated refactoring technique:\n- Replace Type Code with State/Strategy\n- Replace Conditional With Polymorphism\n\nInteractions after adding the State pattern:\n\n<img src=\"state-pattern-interaction.png\" width=\"700px\" alt=\"\"/>\n\nUML after adding the State pattern:\n\n<img src=\"state-uml.png\" alt=\"\" width=\"700px\"/>\n\n\n# Principles of Refactoring\n## What is refactoring\nA change to the internal structure of a software to improve its comprehensibility and reduce its maintaining cost, without changing its observable behaviours.\n\n## When to refactor\nYou do not refactor for the sake of refactoring. You have another goal and use refactoring to facilitate that.\n\n## Three strikes and refactor\nDon Roberts' \"The rule of three\": \n> The third time you do something similar, you refactor.\n\n## Refactor when adding new functionality\n- Refactoring helps us understand the code to be modified;\n- Refactoring makes the structure clear and easier to be extended.\n\n# Changing Published Interfaces when Refactoring\nIf you change the interface, anything could happen.\n\nThe problem with refactoring is that some refactoring techniques DO change interfaces - as simple as Rename Method (273). What's its impact on the treasured notion of Encapsulation?\n\n- If all callers are under your control, no problem - even with a **public** interface.\n- **Published** is a step ahead public. If an interface is published, i.e. \n\n## Maintain both old and new interfaces\n\n### Rewrite old function to call the new one\nDon't copy the function implementation, or you'll get stuck into code duplication!\n\n### Use Java's deprecation facility \nMark the old interface as `deprecated` so that your callers will notice.\n\n## Try not to publish interfaces unless necessary\nOrganizations with an overly strong notion of code ownership tend to publish interfaces inside the team too early. This will introduce unnecessary needs to maintain new/old interfaces.\nChange your notion of ownership, and let everyone be able to change others' code to adapt to new interfaces. This will make refactoring much smoother.\n\n## A special interface change: adding a new exception in `throws` clause\nThis is not change to function signature, so you cannot use delegation to cover it. But the compiler will be unhappy with this change.\n\nTo solve this, define a **superclass exception** for a whole package (such as SQLException for java.sql) and ensure that public methods only declare this exception in their throws clause. \n\nThis way, callers will only know the more generic exception, while we could define new subclass exceptions as we need.\n\n# When not to refactor\n\n## When rewriting is just easier\nA clear signal for **rewriting** instead of refactoring is: the current code could not function properly. \n\nRemember, before refactoring, your code should be functional in most cases.\n\n### Break into components first\nA compromise route is to refactor a large piece of software into components with strong encapsulation. Then you can make a *refactor-versus-rebuild* decision for one component at a time.\n \n## Before the deadline of the project\nAvoid refactoring at this time, because the productivity you gain will only be available after the deadline.\n\n# Refactoring and Performance\nEven if you totally know the system, do measure its performance instead of assuming. \n\nThe secret to fast software, in all but hard real-time contexts, is to **write tunable software\nfirst** and then to tune it for sufficient speed.\n\nThree ways to write fast software:\n## Time budgeting\nOften used in hard real-time system.\n\nBudget each component certain amount of resource - time and footprint.\n## Constant attention approach\nTry to keep high performance anytime doing anything.\nThis feels attractive but are not so effective. Problems:\n\n- Changes that improve performance usually make the program harder to work with. This slows development. \n- The performance improvements are spread all around the program, and each improvement is made with a narrow perspective of the program's behavior.\n\n## Delay until your performance optimization stage\n If you optimize all the code equally, you end up with 90 percent of the optimizations wasted, because you are optimizing code that isn't run much. \n Focus on writing well-mannered program, and then follow specific procedure to adjust the system's performance later in your development, when you enter a performance optimisation stage.\n \n### Use a profiler to monitor the system \n First you use a profiler to tell you what places in your system consumes the most time and space. \n \n### Focus on \"performance hot spots\" for optimisation\n Then you could focus on these **\"performance hot spots\"** to conduct optimizations.\n \n### Stay cautious\n Just like refactoring, you should adjust in small steps when optimising:\n - Adjust in small steps\n - For each step, compile, test, rerun the profiler. If no improvements, undo the adjustment.\n \n Follow the above process of **finding and removing hot spots**, until you get the performance wanted by your client.\n \n## How refactoring helps late optimisation\nAlthough refactoring could slow down the software in the short term, a well-refactored programme helps this style of refactoring in two ways:\n- More time to focus on performance because adding new features is now quicker.\n- Finer granularity for your performance analysis. \n\n\n\n# =============31/07/2020: Bad Smells=============\n\n\nToday I'll be reading chapter 3: Code smells. The number behind the technique name is the page number in the book in the [Chinese Kindle version](https://www.amazon.cn/dp/B07QKC6RN7/ref=sr_1_1?__mk_zh_CN=%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E7%AB%99&keywords=%E9%87%8D%E6%9E%84&qid=1596241892&sr=8-1) for future reference.\n\nNo set of metrics rivals informed human intuition. \n\n# Duplicated Code\n- Same expressions in two functions in one class: Extract Method(110)\n- Same expressions in sibling subclasses: Extract Method(110) and Pull Up Method(332) into their superclass\n- Similar methods in sibling subclasses: Extract Method(110) to separate identical parts and differences, then you mind find the opportunity to use Form Template Method(345) to get a Template Method pattern.\n- Functions doing the same thing with different algorithms: choose the clearer one and use Substitute Algorithm (139).\n- Two unrelated classes using duplicated code: \n    - Extract Class(149) and move the duplicate code into an independent class\n    - Or maybe it should only belong to one class and be invoked in the other.\n    \n# Long Method\n\n> The object programs that live best and longest are those with short methods. \n> All of the payoffs of indirection— **explanation, sharing, and choosing**—are supported by little methods\n\nOlder languages carried an overhead in subroutine calls, which deterred people from small methods. Modern OO languages has almost eliminated this overhead.\nIt takes effort to read the methods though, so the key to making small methods comprehensible is **a good name**, so that your reader do not need to jump to the methods to know what it does.\n\n## Be more aggressive about decomposing methods\n\nA heuristic we follow:\n\n> When you feel the need to add comment to explain something, extract what needs to be explained into a new method, and name it with its **intention**(not how it does the job).\n\nWe can do this on even a short line of code, even if the method is longer than the code it replaces.\n\n> The key here is not method length, but the semantic distance between what the method does and how it does it.\n\n## When there are many parameters and temporary variables\nIf you use Extract Method(110), you'll get a lot of parameters passed to the extracted method.\n- Replace Temp with Query(120) to eliminate these temporary elements.\n- Introduce Parameter Object(295) and Preserve Whole Object(288) to make your long parameter list cleaner.\n- If you've tried that, and you still have too many temps and parameters, it's time to get out the\n  heavy artillery: **Replace Method with Method Object**(135).\n  \n## Deciding which code to extract\n### Comments\nComments often signal the **semantic distance** between the code's intention and how it does it.\n**A block of code with a comment that tells you what it is doing can be replaced by a method whose name is based on the comment.**\n\n### Conditionals and loops \nConditionals and loops  also give signs for extractions. \n- Use Decompose Conditional(238) to deal with conditional expressions. \n- With loops, extract the loop and the code within the loop into its own method.\n  \n# Large Class\nIf you want to do too many things with a single class, it often shows up as too many instance variables - then duplicated code will not be far behind.\n- When you have too many instance variables: \n    - Extract Class(149) and organise several variables into a new class. \n    - If several variables has the same prefixes or suffixes they might belong to the same component\n    - You could Extract Subclass(330) when suitable.\n- When you have too much code: Also Extract Class(149) and Extract Subclass(330)\n    - First find out how the clients use the code, then Extract Interface(341)\n- If your large class is a GUI class, you could  move data and behavior to a separate **domain object**.\n    -  This may require keeping some duplicate data in both places and keeping the data in sync. Duplicate Observed Data(189) suggests how to do this.  \n\n# Long Parameter List\nIn object-oriented programs, parameter lists tend to be much smaller than in traditional programs. This is because you just need to pass your method with enough objects for it to help itself to everything it needs.\n\n- If a request to an existing object could replace a parameter, Replace Parameter with Method(292). Here the \"existing object\" could be:\n    - a field within the class that the method belongs\n    - another parameter\n- Preserve Whole Object(288) to collect a handful of data from the same object, and replace these parameters with that object.\n- If some data items have no logical object, you could Introduce Parameter Object(295) to create a \"parameter object\".\n- One important exception: Sometimes you do not want to create **dependency from the called object to the larger object**. In those cases unpacking data and sending it along as parameters is reasonable.\n    - pay attention to the pain involved. If the parameter list is too long or changes too often, you need to rethink your dependency structure.\n    \n# Divergent Change\nDivergent change occurs when one class is commonly changed in different ways for different reasons.\n\nTo clean this up you identify **everything that changes for a particular cause** and use Extract Class to put them all together.\n\n# Shotgun Surgery\nYou smell Shotgun Surgery when every time you make a kind of change, you have to make a lot of little changes to a lot of different classes. \n- In this case you want to use Move Method and Move Field to put all the changes into a single class. \n- If no current class looks like a good candidate, create one. \n- Often you can use Inline Class to bring a whole bunch of behavior together. \n\n> Divergent change is one class that suffers many kinds of changes, and shotgun surgery is one change that alters many classes. \n> Either way you want to arrange things so that, ideally, there is a **one-to-one link between common changes and classes**.\n\n# Feature Envy\n- A classic smell: a method's interest is higher than its interest in its own class. Often desiring for data.\n- Solution: Move the method to the place it wants to be. \n- When a method wants to use features from multiple classes, decide by seeing which class owns the data most used by this method.\n\n## Some Design Patterns require you to break this rule\nThese patterns are used to fight against the smell of Divergent Change, and to put things that change together in the same place. \n### GoF's Strategy pattern\nMakes it easy for you to change a method's behaviour, because it isolates a small amount of behaviours that need to be overridden. \nThe strategy object's methods will want data from its owner. \n### GoF's Visitor pattern\nThe visitor design pattern is a way of separating an algorithm from an object structure on which it operates. A practical result of this separation is the ability to add new operations to existing object structures without modifying the structures.\nThe visitor object's methods will want data from the visited element.\n\n# Data Clumps\nData items that always go to places together should have their own object.\n- Extract Class(149) to extract these data items into the same object.\n- Move your attention to the method signature:\n    - Introduce Parameter Object(295)\n    - Preserve Whole Object(288)\nThis way, you could shorten a lot of parameter lists and simplify method calls.\nDon't worry if only parts of the new object are used, as long as the new object replaces two or more items, it's worth the change.\n\nA good way of judgement: **If replace one item in the clump, will other data items lose their meaning? If so, it is a clear signal: they should be in an object.**\n\n# Primitive Obsession\n- Replace Data Value with Object(175) to replace the standalone values with objects.\n- Replace Type Code with Class(218) \n- Replace Type Code with Subclass(213), e.g. replace `Movie.priceCode` with subclasses of `Price`: `NewPrice`, `RegularPrice`,  `ChildrenPrice`\n- Replace Type Code with State/Strategy(227), e.g. use `price.calculateCharge(daysRented)` to calculate Rental charge for a movie. `price` maintains a state for the movie. \n- If you find yourself picking data from an array, try Replace Array with Object(186)\n\n# Switch Statement\nA very significant symptoms of object-oriented code: the lack of switch statement.\nThe essential problem to statement is **duplication**.\nYou often find the same switch cases appearing in multiple places, and if you need to add a new case, you need to go find all its appearances.\n\n The OO notion of **polymorphism** gives you an elegant way to deal with this problem.\n\n## Where should the polymorphism go?\n\nSwitch clause often switches among type code. You want \"method or class for the type code\", so you should:\n- Use Extract Method(110) to extract the switch clause to a method\n- Use Move Method(142) to move this method to the class being switched (maintaining the type code/to be made polymorphism)\n- Use Replace Type Code with Subclasses(223) or Replace Type Code with State/Strategy(227) in this class.\n- Once the logic of switching is migrated to this class, use Replace Conditions with Polymorphism(255).\n\n## If it's just some options within a single method\nIf you only want to some simple choices, polymorphism is a bit overkill. You could consider:\n- Replace Parameter with Explicit Methods(285)\n- If one of the cases is `null`, you could try using Introduce Null Object(260).\n\n# Parallel Inheritance Hierarchies\nIn this scenario, whenever you add a subclass to one class, you'll have to add a subclass to another class in parallel. \nThe strategy to eliminate this duplication:\n- Let one instances of one hierarchy **refer** to instances of the other.\n- Then you could choose to use Move Method(142) and Move Field(146) to get rid of the hierarchy on the **referring** class: now it contains a field (the referred instance) that takes care of the inheritance relationship.\n\n# Lazy Class\nEach class costs time and effort to maintain. Kill classes that aren't doing enough to pay for itself.\n- If some subclasses are not doing lots, try Collapse Hierachy(344)\n- For components almost useless outside one context, use Inline Class(154)\n\n# Speculative Generality\nDon't say \"Oh one day we'll need this\". See if you are using them now.\n- Collapse Hierarchy (344) if some abstract classes are not so useful.\n- Unnecessary delegation could be removed with Inline Class(154).\n- Methods with unused parameters should be subject to Remove Parameter (277). \n- Methods named with unnecessarily abstract names should be brought down to earth with Rename Method (273.\n- If the only user of a method/class are test cases, get rid of the test cases and the code.\n\n# Temporary field\nSometimes you see an object in which an instance variable is set only in certain circumstances.\nSuch code is difficult to understand, because you expect an object to need all of its variables. \n- Use Extract Class（149) to create a home for the poor orphan variables. Put all the code that concerns the variables into the component. \n-  You may also be able to eliminate conditional code by using Introduce Null Object (260) to create an alternative component for when the variables aren't valid.\n\nA common case of temporary field occurs when a complicated algorithm needs several variables. Because the implementer didn't want to pass around a huge parameter list (who does?), he put them in fields. But the fields are valid only during the algorithm.\nIn this case you can use Extract Class(149) with **these variables and the methods that require them**. The new object is a **method object** and knows how to do the algorithm.\n\nI found [an article by Refactoring Guru: Replace Method with Method Object](https://refactoring.guru/replace-method-with-method-object) explaining what is a method object:\n> - Problem: You have a long method in which the local variables are so intertwined that you can’t apply Extract Method.\n> - Solution: Transform the method into a separate class so that the local variables become fields of the class. Then you can split the method into several methods within the same class.\n\n# Message Chains\nYou see message chains when a client asks one object for another object, which the client then asks for yet another object, which the client then asks for yet another another object, and so on. You may see these as a long line of `getThis` methods, or as a sequence of temps. \n\nNavigating this way means the client is **coupled** to the structure of the navigation. \nAny change to the intermediate relationships causes the client to have to change.\n\n- Hide Delegate(157): \n\n<img src=\"hide-delegate.png\" alt=\"\" width=\"600px\"/>\n\nHowever, this way tends to make a handful of objects \"Middle Man\".\n\n- A better option: first observe what the object got at the end of the chain is used for. See if you could use Extract Method(110) to extract the code that uses this final object into a new method, and then use Move Method(142) to push this method **down** the chain. If several clients of one of the objects in the chain want to navigate the rest of the way, add a method to do that.\n\n# Middle Man\nYou look at a class's interface and find half the methods are delegating to this other class. \n- Use Remove Middle Man(160) and talk to the object that really knows what's going on. \n- If only a few methods aren't doing much, use Inline Method(117) to inline them into the caller. \n-  If there is additional behavior, you can use Replace Delegation with Inheritance (355) to turn the middle man into a subclass of the real object. That allows you to **extend behavior without chasing all that delegation** (Your old middle man now knows how to do the delegated job himeself, and he also knows some extra stuff).\n\n# Inappropriate Intimacy\nSometimes classes become far too intimate and spend too much time delving in each others'private parts. \n- Use Move Method (142) and Move Field (146) to separate the pieces to reduce the intimacy. \n- See whether you can arrange a Change Bidirectional Association to Unidirectional(200).\n- If the classes do have common interests, use Extract Class (149) to put the commonality in a safe place that the two lovers could share interests.  \n- Use Hide Delegate (157) to let another class act as go-between.\n\n> Inheritance often can lead to overintimacy. Subclasses are always going to know more about their parents than their parents would like them to know. If it's time to leave home, apply Replace Inheritance with Delegation (352).\n\n# Alternative Classes with Different Interfaces\n- Use Rename Method (273) on any methods that do the same thing but have different signatures for what they do. \n- Keep using Move Method (142) to move behavior to the classes until the protocols are the same.\n- If you have to redundantly move code to accomplish this, you may be able to use Extract Superclass (336).\n\n# Incomplete Library Class\n- If there are just a couple of methods that you wish the library class had, use Introduce Foreign Method (162). \n- If there is a whole load of extra behavior, you need Introduce Local Extension (164).\n  \n # Data Class\nThese are classes that have fields, getting and setting methods for the fields, and nothing else.\nSuch classes are dumb data holders and are almost certainly **being manipulated in far too much detail by other classes**. \n\n- In early stages these classes may have public fields. If so, you should immediately apply Encapsulate Field (206) before anyone notices.\n-  If you have collection fields, check to see whether they are properly encapsulated and apply Encapsulate Collection (208) if not.\n- Use Remove Setting Method (300) on any field that should not be changed.\n- Look for where these getting and setting methods are used by other classes. Try to use **Move Method** to move behavior into the data class. \n- If you can't move a whole method, use Extract Method (110) to create a method that can be moved.\n- After a while you can start using Hide Method (303) on the getters and setters.\n\n> Data classes are like children. They are okay as a starting point, but to participate as a grownup object, they need to take some responsibility.\n\n# Refused Bequest\n- Traditional thoughts: A subclass should want to play with all the methods and data from its superclass. If it does not, there are mistakes in the design of the inheritance hierarchy. The methods and data that the child do not want to play with should be sent to a sibling of the child through using Push Down Method (328) and Push down Field (329).\n\n- Martin & Beck's opinion: If Refused Bequest causes confusion and problem, follow the traditional thoughts. Nine times out of ten this smell is too faint to be worth cleaning.\n\n- The smell of refused bequest is much stronger if the subclass is reusing behavior but does not want to support the interface of the superclass. \n- We don't mind refusing implementations, but **refusing interface** gets us on our high horses. In this case, however, don't fiddle with the hierarchy; you want to gut it by applying Replace Inheritance with Delegation(352).\n\n# Comments\nComments help us to identify bad smells. First you should eliminate the bad smells through your refactoring techniques. Often after refactoring, we realise that the code has told everything.\n\n- If you need comment to explain a block of code, try Extract Method (110)\n- If the method has been extracted, try Rename Method (273)\n- If you want to state some rules about the required state of the system, try Introduce Assertion (267).\n\n> When you feel the need to write a comment, first try to refactor the code so that any comment becomes superfluous.\n\nA good time to use a comment is when you don't know what to do. With comments you can: \n\n- Describe what is going on\n- Indicate areas in which you aren't sure. \n- Say why you did something. \n\nThese comments will give information to future modifiers.\n\n# ============= 1/8/2020 =============\n\n# Build Tests\n\n> Make sure all tests are fully automatic and that they check their own results.\n\nBenefit of frequently running automatic test:\n\n> A suite of tests is a powerful bug detector that decapitates the time it takes to find bugs. (while the bugs are freshly introduced)\n\n> The standard Java idiom for testing is the testing main. The idea is that every class should have a main function that tests the class. It's a reasonable convention (although not honored much), but it can become awkward. The problem is that such a convention makes it tricky to run many tests easily. Another approach is to build separate test classes that work in a framework to make testing easier.\n\n## JUnit Test\nJunit framework uses the  composite pattern that allows you to group tests into arbitrary layers of suites:\n\n<img src=\"junit-composition.png\" alt=\"\" width=\"700px\"/>\n\n> Run your tests frequently. Localize tests whenever you compile—every test at least every day.\n\n## Unit Test vs Functional Test\n- Unit tests are highly localized. Each test class works within a single package. \n- Functional tests are written to ensure the software as a whole works. \n- Functional tests typically treat the whole system as a black box as much as possible.\n\nWhen functional testers, or users, find a bug in the software, at least two things are needed to fix\nit:\n-  start with addding a unit test that exposes the bug\n-  change the production code to remove the bug\n\n> When I get a bug report, I begin by writing a unit test that causes the bug to surface. \n> - I write more than one test if I need to narrow the scope of the bug, or if there may be related failures. \n> - I use the unit tests to help pin down the bug and to ensure that a similar bug doesn't get past my unit tests again.\n\n> For refactoring purposes, I count on the unit tests—the programmer's friend.\n\n## Test Tips\n> The key is to test the areas that you are most worried about going wrong. That way you get the most benefit for your testing effort.\n\n> It is better to write and run incomplete tests than not to run complete tests.\n\n> Think of the boundary conditions under which things might go wrong and concentrate\n  your tests there.\n\n> Don't forget to test that exceptions are raised when things are expected to go wrong.\n\n> It is better to spend a reasonable time to catch most bugs than to spend ages trying to catch them all.\n\n# ============= 3/8/2020 =============\n\nNow I start reading the Catalog of Refactorings.\n# Refactoring techniques on methods\n## Replace Method with Method Object\nThis is a technique that interests me a lot, as it helps break down long method that actively manipulate multiple local variables.\n\nThere are two ways to share variables between two methods: pass them as arguments/return values, and making them globally accessible to both to them.\n\nExtracting a Method Object makes the old local variables accessible to all extracted short methods.\n\n## The role of temporary variables\nFowler refreshed my understanding of temporary variables. In his opinion, temporary variables should also serve the comprehensibility of the code. When possible, extracting method should be preferred over using temporary variables; queries should be preferred over in-line calculation of a temp. If the method is too hard to decompose and temporary variables are necessary, **each temporary method should only serve ONE purpose** and should never be reused for a new purpose. The name of the temporary variable, just like methods, should clearly state its intention.\n\n# ============= 7/8/2020 =============\n\nBuried in 711 assignment for three days and AC today!!!🎈🎉🎈🎉  Back to the book.\n\n# Moving Features Between Objects\n\n> You've probably heard that a class should be a crisp abstraction, handle a few clear\n  responsibilities, or some similar guideline. In practice, classes grow. You add some operations\n  here, a bit of data there. You add a responsibility to a class feeling that it's not worth a separate\n  class, but as that responsibility grows and breeds, the class becomes too complicated. Soon your\n  class is as crisp as a microwaved duck.\n\n## Introduce Foreign Method\nIntroduce a foreign method when a server class you are using needs an additional method, but you can't modify the class. To do so, create a method in the client class with an instance of the server class as its first argument.\n\n## Introduce Local Extension\nIntroduce Local Extension when a service class needs several additional methods, but you can't modify the class. You also want to reuse the additional methods in multiple places - if it is one-off thing, just create a foreign method.\n\nCreate a new class that contains these extra methods. Make this extension class a subclass or a wrapper of the original.\n\n\n# ============= 9/8/2020 =============\n\n# Organising Data\n\n## Self Encapsulate Field\nQuestion:\n> Inside a class, do we access the fields directly through the field name, or do we also use getters and setters?\n\nMartin's Choice:\n> - I'm usually happy to do what the rest of the team wants to do. \n> - Left to myself, though, I like to use direct variable access as a first resort, **until it gets in the way**. \n\nWhen do we need to self encapsulate:\nThe advantages of indirect variable access are that **it allows a subclass to override\nhow to get that information with a method** and that it supports **more flexibility in managing the data**, such as lazy initialization, which initializes the value only when you need to use it.\n\n## Encapsulate Collection\nOften a class contains a collection of instances. This collection might be an array, list, set, or vector. Such cases often have the usual getter and setter for the collection.\n\n- A getter for a multivalued attribute should return something that prevents manipulation of the collection and hides unnecessary\ndetails about its structure. The getter should not return the collection object itself, because:\n     - It **allows clients to manipulate the contents of the collection without the owning class's knowing what is going on**. \n     - It reveals too much to clients about the object's internal data structures. \n- There should not be a setter for collection: rather there should be operations to add and remove elements. \n    - This gives the owning object control over adding and removing elements from the collection.\n- With this protocol:\n    - the collection is properly encapsulated, \n    - the coupling of the owning class to its clients is reduced\n    \n## Replace Type Code with Subclasses\nBuilding the stage for Replace Conditional with Polymorphism.\n\n## Replace Type Code with Strategy/State\nThis is similar to Replace Type Code with Subclasses, but can be used if the type code\nchanges **during the life of the object** or if another reason prevents subclassing. It uses either the\nstate or strategy pattern.\n\n# ============= 10/8/2020 =============\n\n# Simplifying Conditional Expressions\n\n## Replace Nested Conditional with Guard Clauses\nWhat is \"**Guard Clause**\"?\n> If the condition is an unusual condition, check the condition and return if the condition is true. This kind of check is often called a guard clause. (Beck)\n\n> Guard clauses either return from the method or throw an exception.\n\n## Introduce Null Object (null subclass)\n> The essence of polymorphism is that instead of asking an object what type it is and then invoking some behavior based on the answer, you just invoke the behavior. \n\n> Remember, null objects are always constant: nothing about them ever\n  changes. Accordingly, we implement them using the **Singleton pattern**\n  [Gang of Four]. Whenever you ask, for example, for a missing person,\n  you always get the single instance of that class.\n\n## Introduce Assertions\n\nAssertions act as communication and debugging aids. In communication they help the reader understand the assumptions the code is making. \nIn debugging, assertions can help **catch bugs closer to their origin**. I've noticed the debugging help is less important when I write self-testing code, but I still appreciate the value of assertions in communciation. (Martin)\n\n# ============= 12/8/2020 =============\n\n# Making Method Calls Simpler\n## Separate Query from Modifier\nYou get into trouble if you \"modify\" and \"query\" in the same methods.\n\n> A good rule to follow is to say that any method that returns a value should not have observable side effects. \n\n### Example in my code\nI broke this role in the Kalah assignment with this method:\n```java\npublic int takeAllSeeds() {\n    int takeSeedNum = seedNum;\n    seedNum = 0;\n    return takeSeedNum;\n}\n```\nIt does two things:\n- query the current number of seeds\n- modify the seed number to 0.\n\n**In Martin's opinion, this is bad. But how will breaking this method into two improve my programme, If every time seeds in a house needs to be modified, I will need the number of seeds right after? How is a `getter` then a `remove` call better than a call to `takeAllSeeds`?** \n\n### Concurrency Issue\n- An important idiom in multi-thread system: Test then Set\n- You need to retain a third method for the query-and-modify operation. \n- The query-and-modify operation will call the **separate** query and modify methods and be synchronized. \n- If the query and modify operations are not synchronized, you also might restrict their visibility to package or private level. \n- That way you have a safe, synchronized operation decomposed into two easier-to-understand methods. \n- These lower-level methods then are available for other uses.\n\n## Replace Parameter with Explicit Methods\nIf the caller wants to decide what it wants to do by setting the parameter, it might as well call different methods instead of passing an instructive parameter to one compound method and let the big method run conditional operations.\n\n## Preserve Whole Object\nPreserve Whole Object means when a method needs value(s) from an object, instead of passing the value(s), pass the whole object.\n### Key consideration: Dependency Structure\nPassing in the required object causes a dependency between the required object and the called object. \nIf you don't want to introduce this dependency, don't pass the whole object.\n### Is it worth it when only one value is needed?\nMartin's opinion is yes.\n- One value and one object amount to the same thing when you pass them in, at least for clarity's sake\n- There may even be a performance cost with pass by value parameters. \n### Is your method in the right place?\n> That a called method uses lots of values from another object is a signal that the called method should really be defined on the object from which the values come. When you are considering Preserve Whole Object, consider Move Method as an alternative.\n\n## Introduce Parameter Object\nWhen you see a group of values that tend to be passed together, consider grouping them together by creating a new class, then pass its instances as parameter instead,\n- This will shorten the parameter list and improve readability.\n- Once you have the new class, you might find that the new class is a better home for the method. Moving the method to the class holding data it is more interested in will largely reduce duplicated code.\n\n## Replace Constructor with Factory Method\nIn Java/C++, you have to know the exact class of the object you want to create, when using constructors. To go around this, you could use Factory Methods.\n### Direct motivation: Subclassing instead of Type Code\nConstructors can only return an instance of the object that is asked for. So you need to replace the constructor with a factory method.\n\n## Encapsulate Downcast\n### When you'll need to downcast\nIn Java, he lack of templates means that you have to downcast whenever you take an object out of a collection.\nIn other strongly typed languages, you'll also need to do downcast when the compiler could not figure out the specific type of the object.\n\n### Do not leave the evil job of downcast to clients\nIf you return a value from a method, and you know the type of what is returned is more specialized than what the method signature says, you are putting unnecessary work on your clients. \nRather than forcing them to do the downcasting, you should always provide them with **the most specific type you can**.\n\n## Replace Error Code with Exception\nExceptions are better than returning error codes because they clearly separate normal processing from error processing. This makes programs easier to understand.\n### Deciding if the exception is checked or unchecked\n-  If the caller is responsible for testing the condition before calling, make the exception unchecked.\n    - If it is the caller's responsibility to make a check before calling, it will be a *programming error* if the requirement is not met. Therefore it is a *bug*m and an *unchecked exception* should be thrown.\n- If the exception is checked, either create a new exception or use an existing one.\n    - If the routine inside the method body will check for certain requirements, I'll need to declare the exception in the interface. This way I signal the caller to **expect** this type of exception, and take appropriate measures.\n\n# ============= 17/8/2020 =============\n# Dealing with Generalization\nDuplication gets me into trouble.\n\n## Pull Up Field\nVisibility change: If the fields used to be private, you'll need to make it `protected` in the updated superclass so that \n\n## Pull Up Method\nDuplicate behaviour is the a breeding ground for future bugs: Hard to make safe changes.\n### Challenge with Pulling up Method: reference to subclass features\nThe most awkward element of Pull Up Method is that the body of the methods may refer to features that are on the subclass but not on the superclass.\n - If the feature is a method, you can: \n    - generalize the other method \n    - create an abstract method in the superclass\n- If the feature is a field, you can:\n    - pull up the field\n    - see if Template Method will move down the part of the method that refer to the subclass field\n- If you have two methods that are similar but not the same, you may be able to use Form Template Method.\n\n## Pull Up Constructor Body\n- You can access the superclass's constructor via `super()` in subclass constructors.\n- If refactoring becomes complex, you might want to consider Replace Constructor with Factory Method instead.\n\n## Extract Subclass/Extract Superclass\n\n### Extract Subclass Triggers:\n- A class has behavior used for some instances of the class and not for others. \n- Type Code, but you don't have a subclass yet to do Replace Type Code with Subclasses/Strategy/State\n\n### Extract Superclass Triggers:\n- Two classes doing similar things\n\n### Alternative: Extract Class\nA choice between **Delegation** and **Inheritance**.\n\n## Extract Interface\n### Triggers\n- Use of only a particular subset of a class's responsibilities by a group of clients. \n- A class needs to work with any class that can handle certain requests.\n\n## Collapse Hierarchy\nWhen the superclass and subclass are doing the same thing.\n\n## Form Template Method\nThis was covered in CompSci718 and has been very handy.\nInheritance is a powerful tool for eliminating duplicate behavior. \n\n## Replace Inheritance with Delegation\n\n<img src=\"delegation.png\" alt=\"\" width=\"600px\"/>\n\n\"Has a\" relationship: The `Stack`(delegate) has a `Vector` object in its field, and \"speaks\" on behalf of `Vector`.\n\nThis relates to the concepts of **Proxy Pattern** that I learnt from CompSci711. Here's the link to my [learning note about Proxy Pattern](https://niuniuanran.github.io/2020/08/16/Understanding-Proxy-pattern/). They are different because:\n- There is not inheritance/implementation hierarchy in Delegation itself.\n- The `Stack` class cannot pretend it is a `Vector`.\nBut they are similar because:\n- The \"delegator\" hides the details of the delegated class from the client.\n- Applying the delegation/proxy reduces the burden of re-implementing the same functionality of the existing delegated/subject.\n\n## Replace Delegation with Inheritance\n**From \"has a/depend\" relationship to \"extends\" relationship.**\n\nIf you are tired of writing the delegating methods.\n\nYou should not make a subclass that does not inherit all superclass's behaviours. Other options:\n- Extract Superclass/Interface to extract the common parts first\n- Let the client call the delegated subject directly by using Remove Middle Man.\n\n# ======= 19/8/2020: Big Refactorings =======\n\n# Tease Apart Inheritance\n\n<img src=\"tease-apart-inheritance.png\" alt=\"\" width=\"600px\"/>\n\nCreate two hierarchies and use delegation to invoke one from the other.\n\nThe `PizzaStore` nightmare I had with CompSci718 was a good example on this. By teasing apart the region styles and the flavour, the code was more alterable whenever I need to add a new flavour/region: It is a nice **matrix**.\n\n# Convert Procedural Design to Objects\n\n<img src=\"Convert-procedural-to-objects.png\" width=\"600px\" alt=\"\"/>\n\nI had a vivid experience on this refactoring with my CompSci711 assignment (Echo - sequential). \nI got more flexibility to achieve distributed echoing if I encapsulate the procedural communications into `Node` objects and let them maintain their own state. \n\n\"Using objects\" is more than calling constructors. The typical situation is long procedural methods on a class with little data and dumb data objects with nothing more than accessors.\n\nScenarios when you do need **classes with behaviours/procedures and little/no data**: Strategy objects! These are dedicated for varying behaviours. They are usually small and are used when we have a particular need for flexibility.\n\nKey process:\n- Take each long procedure and apply **Extract Method** and the related refactorings to break it down. \n- As you break down the procedures, use **Move Method** to move each one to the appropriate dumb data class.\n\n# Separate Domain from Presentation\n\nSome GUI classes contain domain logic. Take the domain logic out as a separate class.\n\n<img alt=\"domain-from-pre.png\" alt=\"\" width=\"600px\"/>\n\nAfter the refactoring, your presentation class will know about the domain class, but the domain class does not need to know about presentation.\n\n## MVC Pattern\nKey value of MVC: separation between the code for user interface (View) and the domain logic (Model). The presentation class in only responsible for user interface; The domain class only contains business logic.\n\n# Extract Hierarchy\n\nWhen you have a class doing a lot of jobs, and part of the jobs are done with lots of conditional statements.\n\n<img alt=\"\" src=\"extract-hierarchy.png\" width=\"600px\"/>\n\nIn evolutionary design(I made a separate node learning about evolutionary design [here](https://niuniuanran.github.io/2020/08/20/What-is-Evolutionary-Design/)), it is common to think of a class as implementing one idea and come to realize later that it is really implementing two or three or ten.\n\n# ======= 20/8/2020:  Refactoring, Reuse, and Reality =======\nThis chapter is written by William Opdyke to discuss what refactor does in the reality of developer's works.\n\n# The cost of refactoring?\nPeople may argue that \"I pay for new functions of the programme\".\nRefactoring might seem expensive for no new features, but it proves itself by saving the efforts and time in other stages of programme development.\n\n# Refactor securely\nThe intuitive definition: \"Secure refactoring\" is refactoring that do not cause damages to the programme. The purpose of refactoring is to change the structure of the programme without changing its behaviour.\n\n**William claims this section to be the most valuable part of this chapter.**\n\nThe options you have:\n- believe in your coding ability\n- believe in your compiler\n- believe in your testing suite\n- believe your code reviewers\n\nBut people make mistakes. Compilers miss semantic bugs. Tests do not cover all cases.\n\nWilliam's suggestion: **define and prototype a refactoring tool to check whether a refactoring can be safely applied to a program and, if it is, refactor the program.**\n\n# Refactoring Tools\nBy the sound of what \"refactoring browser\" does, what we have with Jetbrain suite/Visual Studio is pretty powerful as refactoring tools nowadays.\n\nHere's my screenshot of what Rider has to offer:\n\n<img alt=\"\" src=\"refactoring-tools.png\" width=\"400px\"/>\n\n\n## Technical Criteria for a Refactoring Tool\n### Program Database\nCreating a database requires using **semantic** analysis (parsing) to determine the \"part of speech\" of every token in the program.\n\nThis must done at:\n- class definition level, to determine instance variable and method definitions, \n- the method level, to determine instance variable and method references.\n\n### Parse Trees\nA parse tree is a data structure that represents the internal structure of the method itself. \n\nHere's a parse tree example provided by Martin:\n\n<img src=\"parse-tree.png\" alt=\"\" width=\"600px\"/>\n\n### Accuracy\nThe refactorings implemented by a tool must reasonably preserve the behavior of programs. \n\n## Practical Criteria for a Refactoring Tool\n### Speed\nIf the analysing of the programme takes too much time upfront, developers might not find it an efficient tool.\nA simple solution: Ask programme for the information the tool will need. Now programmers shoulder some responsibilities for potential errors, but it makes the tool more usable.\n### Undo\nYes I want this so much!\n### Integration with other tools\nThe accessibility of the refactoring tool makes a lot of differences.\n\n# 🎈🎈🎈🎈🎈 Done! (20 Aug 2020) 🎈🎈🎈🎈🎈\nToday I finish reading the book! \n\nImportant things I learnt:\n- Why I refactor\n- When I should refactor\n- Why I should not hurry to optimise my programme too early\n- What are the bad smells calling for refactoring\n- How design patterns are applied in refactoring\n- That I should be aware of my \"two hats\" - I should not suddenly become the developer and start changing the code behaviour while I'm wearing the refactoring hat.\n\nImpressive refactoring techniques I learnt:\n- Adding delegation, and removing it when not needed.\n- Introducing Method Object\n- Extract Methods, for the purpose of filling the semantic gap between what the method does and how it does it.\n- Replace conditional with polymorphism + Replace type code with subclass\n\nImpressive bad smells that I am now more aware of:\n- Dumb data class\n- Shotgun Surgery\n- Speculative Generality\n- Refused Bequest\n- Feature Envy\n\nI want to work on:\n- I am having confusion on how much (if any) I should leave spaces for future changes in my programme, or should I just focus on what is currently needed.    \n    - I read the assignment2-implementation1 code and I see myself through it: trying so hard to give spaces for future changes that hazards are introduced for the current purposes.\n","tags":["Maintainable Software","Good Practices"]},{"title":"Decide on a CIDR block for VPC","url":"/2020/08/19/Decide-on-a-CIDR-block-for-VPC/","content":"\n# Choose from the Private Address Space\n\nThe Internet Assigned Numbers Authority (IANA) has reserved the following three blocks of the IP address space for private internets:\n\n- 10.0.0.0 - 10.255.255.255  (10/8 prefix)\n- 172.16.0.0 - 172.31.255.255  (172.16/12 prefix)\n- 192.168.0.0 - 192.168.255.255 (192.168/16 prefix)\n\nAWS recommends that you specify a CIDR block from the private (non-publicly routable) IP address ranges as specified in RFC 1918; \nfor example, 10.0.0.0/16, or 192.168.0.0/16.\n\nMore details on private address assignment available at [rfc1918](http://www.faqs.org/rfcs/rfc1918.html).\n\nMake sure the IP ranges from different regions don't overlap. You shouldn't have a 172.31.0.0/16 in us-west eu-ireland, for example. It will make VPN between those two regions a problem requiring double-NAT to solve. \n\n# Subnet CIDR block\nThe CIDR block of a subnet can be the same as the CIDR block for the VPC (for a single subnet in the VPC), or a subset of the CIDR block for the VPC (for multiple subnets). The allowed block size is between a /28 netmask and /16 netmask. \nIf you create more than one subnet in a VPC, the CIDR blocks of the subnets cannot overlap.\n\nSome tips:\n- Consider making a distinction between private and public subnets, eg\n    - private 10.0.1.0/24 (3rd byte < 129)\n    - public 10.0.129.0/24 (3rd byte > 128)\n- Don't under-allocate CIDR blocks. If you use a load of Elastic Load Balancers, remember that they will also consume available ip addresses on your subnets. This is particularly true if you use ElasticBeanstalk.\n\n\n\n","tags":["AWS","AWS Lab"]},{"title":"What is Communicating Sequential Processes","url":"/2020/08/18/What-is-Communicating-Sequential-Processes/","content":"\n> In computer science, communicating sequential processes (CSP) is a formal language for describing patterns of interaction in concurrent systems.  It is a member of the family of mathematical theories of concurrency known as process algebras, or process calculi, based on **message passing via channels**.\n\nC#: [System.Threading.Channels](https://devblogs.microsoft.com/dotnet/an-introduction-to-system-threading-channels/)\n\nGO Routines: related to the coroutine concept.\n\nJavaScript and Python have also similar facilities and packages. Briefly, this a vast, interesting, and actual topic, very useful to learn. \n","tags":["Distributed Algorithms"]},{"title":"Understanding Proxy pattern","url":"/2020/08/16/Understanding-Proxy-pattern/","content":"\nProxy pattern is commonly used in distributed algorithms.\n\n# What it looks like\nLet's start with this example provided by [Wikipedia: Proxy pattern](https://en.wikipedia.org/wiki/Proxy_pattern):\n```\n    interface ICar\n   {\n       void DriveCar();\n   }\n   \n   // Real Object\n   public class Car : ICar\n   {\n       public void DriveCar()\n       {\n           Console.WriteLine(\"Car has been driven!\");\n       }\n   }\n   \n   // Proxy Object\n   public class ProxyCar : ICar\n   {\n       private Driver driver;\n       private ICar realCar;\n   \n       public ProxyCar(Driver driver)\n       {\n           this.driver = driver;\n           this.realCar = new Car();\n       }\n   \n       public void DriveCar()\n       {\n           if (driver.Age < 16)\n               Console.WriteLine(\"Sorry, the driver is too young to drive.\");\n           else\n               this.realCar.DriveCar();\n        }\n   }\n   \n   public class Driver\n   {\n       public int Age { get; set; }\n   \n       public Driver(int age)\n       {\n           this.Age = age;\n       }\n   }\n   \n   // How to use above Proxy class?\n   private void btnProxy_Click(object sender, EventArgs e)\n   {\n       ICar car = new ProxyCar(new Driver(15));\n       car.DriveCar();\n   \n       car = new ProxyCar(new Driver(25));\n       car.DriveCar();\n   }\n```\n\nBased on the code I could draw this UML:\n\n<img src=\"CarProxy.png\" alt=\"\" width=\"500px\"/>\n\n# When to use it\n\nUse proxy when you want to add new functions/change how components communicate but do not want to change the existing component (the Real Subject).\n\n- A proxy is a wrapper or agent object that is being called by the client to access the real serving object behind the scenes.\n- For the client, usage of a proxy object is similar to using the real object, because both implement the same interface.\n\n","tags":["Distributed Algorithms","Design Patterns"]},{"title":"EC2 instances, stop/start v.s. reboot","url":"/2020/08/13/EC2-instances-stop-start-v-s-reboot/","content":"\n# Reboot\nHere's the [AWS documentation about rebooting your instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-reboot.html)\nAn instance reboot is equivalent to an operating system reboot. In most cases, it takes only a few minutes to reboot your instance. When you reboot an instance, it keeps its public DNS name (IPv4), private IPv4 address, IPv6 address (if applicable), and any data on its instance store volumes.\nRebooting an instance doesn't start a new instance billing period (with a minimum one-minute charge), unlike stopping and starting your instance.\n\n# Stop/Start\nHere's the [AWS documentation about stopping/starting your instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html)\n\n# What happens to IP addresses of the instance?\n## Stop/Start\nWhen an EC2 instance is stopped, AWS will take back both its private and public IP addresses. \nWhen we start it again, AWS will re-assign private and public IP addresses to the instance. \nThis is when Elastic IP address becomes necessary if you want to be consistently available.\n\n## Reboot\nYou keep your IP addresses.\n\n# What happens to the hardware hosting the instance?\n## Stop/Start\nAWS will take back the hardware hosting your instance, and AWS might start another EC2 instance on it.\nYour instance is now just an \"ID\".\n## Reboot\nThe instance stays on the hardware.\n\n# What happens to instance storage (ephemeral)?\nThe data in an instance store persists only during the lifetime of its associated instance. \n## Stop/Start\nAny contents on the instance’s former ephemeral storage were wiped when you stop it.  You are given fresh ephemeral storage when you start your instance.\n\nTherefore, You can only stop an Amazon EBS-backed instance. Instances with instance storage as its root volume can only be terminated.\n\n## Reboot\nIf an instance reboots (intentionally or unintentionally), data in the instance store persists. \n\n# Other behaviours for stopped instances\n## Billing\nYou can leave an instance stopped for as long as you like and not get charged for run time.\nYou do get charged at a much lower rate for the EBS volume storage.\nA fresh billing hour is started for the instance when you start it again. \n\n## Change EC2 instance type\nYou can change EC2 instance type only after you've stopped it.\n\n## Enlarge EBS root volume\nYou can do this either when the instance is stopped, or while it is running.\n(Shrinking your EBS root volume is much more trouble though, you have to move your data manually into a smaller one, and configure the new volume to be your instance's root volume.)\n","tags":["AWS"]},{"title":"Graph Theory Crash Learn","url":"/2020/08/10/Graph-Theory-Crash-Learn/","content":"\nNeed to pick up some graph knowledge to be more ready for my Parallel and Distributed Computing course.\n\n# Defining Graph\n\nI watched a quick video, [Graph Theory - An Introduction](https://www.youtube.com/watch?v=HmQR8Xy9DeM), where I took all the images I used in this part.\n\nGraph is a nonempty finite set of vertices V, along with a set E of 2-element subsets of V.\nThe elements of V are called vertices The elements of E are called edges.\n\n<img alt=\"\" width=\"300px\" src=\"define.png\"/>\n\n>- Formally, a graph G is an ordered pair of disjoint sets (V, E), where E Í V × V. Set V is called the vertex or node set, while set E is the edge set of graph G. \n>- Typically, it is assumed that self-loops (i.e. edges of the form (u, u), for some u Î V) are not contained in a graph.  -- Definition from[personal.kent.edu](http://www.personal.kent.edu/~rmuhamma/GraphTheory/MyGraphTheory/defEx.htm)\n\n\n## Graph Order/Graph Cardinality\n|G| = |V| = number of vertices or graph order.\n\n## Graph Size\n|E| = number of edges or graph size.\n\n## Degree of a vertex\nNumber of vertices coming out of edges from the vertex. \ndeg (v1) = 3. \n\nv1's neighbours: v2, v3, v4\n\n# Presentation of Graph\n## Edge Set\n<img alt=\"\" src=\"set.png\" width=\"300px\"/>\n\n## Adjacency List\n<img alt=\"\" src=\"list.png\" width=\"300px\"/>\n\n## Adjacency Matrix\n<img alt=\"\" src=\"matrix.png\" width=\"300px\"/>\n\n# Tree\nGraph with no circuit(loop): if you travel from v1 to v4, the only way to come back to v1 is through **backtrack**.\n\n## Isomorphic graph\n> In graph theory, an isomorphism of graphs G and H is a bijection （双射） between the vertex sets of G and H. such that any two vertices u and v of G are adjacent in G if and only if f(u) and f(v) are adjacent in H.\n\n# Concepts related to graph theory\n## Digraph\nDirected Graph. \nA directed graph (or digraph) is a graph that is made up of a set of vertices connected by edges, where the edges have a direction associated with them.\n\n### Arcs\nIn one restricted but very common sense of the term,[5] a directed graph is an ordered pair G=(V,E) comprising:\n\n- V, a set of vertices (also called nodes or points);\n- <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/410d2e8a388594591b3aa8f63b0cb4ad69063d01\" width=\"20px\"/>\na set of edges (also called directed edges, directed links, directed lines, arrows or arcs) which are ordered pairs of vertices (that is, an edge is associated with two distinct vertices).\n\n## Path\nA path is a sequence of vertices with the property that each vertex in the sequence is adjacent to the vertex next to it.\n### Simple Path\nA path that does not repeat vertices is called a simple path. \n### Circuit\nA circuit is path that begins and ends at the same vertex.\n\n## Weights/Costs\nIn many applications, each edge of a graph has an associated numerical value, called a **weight**.\n\n- Weighted graphs may be either directed or undirected.\n- The weight of an edge is often referred to as the \"**cost**\" of the edge.\n- In applications, the weight may be a measure of \n    - the length of a route, \n    - the capacity of a line, \n    - the energy required to move between locations along a route, etc.\n\n## Shortest Path\nThe shortest path problem is about finding a path between vertices in a graph such that the total sum of the **edges weights** is minimum.\n\n## Distance \nGeodesic distance between a pair of nodes = minimum **length** of a connecting path, aka **length of a shortest path** (hop count in unweighted graphs).\n\n> How we define distance is directly related with how we define shortest path.\n>- For unweighted graph, or for a weighted graph but we choose to ignore the weight, the shortest path/distance are both looking at the **hop count**\n>- For weighted graph and we choose to care about the weight/cost, the shortest path/distance are both looking at the total weight/cost and do not care about the count of edges.\n\n## Eccentricity of a vertex\nThe eccentricity of a vertex v is the **greatest distance** between v and any other vertex.\n\n## Diameter\nDiameter = maximum distance between any pair of nodes.\n- i.e. the maximum distance among the shortest paths connecting each pair of nodes in the graph. \n- i.e. the maximum eccentricity of any vertex in the graph\n\n## Radius\nRadius = minimum maximum distance, for any node to any other node (minimum attained at centers).\n- i.e. the minimum eccentricity of any vertex in the graph\n- i.e. the eccentricity of the graph's centre\n\n## Spanning Tree\nA spanning tree T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G, with **a minimum possible number of edges**.\n\n### Minimum Spanning Tree\nA minimum spanning tree (MST) or minimum weight spanning tree is a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with the **minimum possible total edge weight**.\n\n# Dijkstra's Shortest Path Algorithm\nSee page 5 of [this presentation](http://courses.cs.vt.edu/~cs3114/Fall10/Notes/T22.WeightedGraphs.pdf) for implementation details.\nDijkstra's SSAD Algorithm only works for graphs with non-negative weights.\n\nWe assume that there is a path from the source vertex s to every other vertex in the graph. \n- Let S be the set of vertices whose minimum distance from the source vertex has been found. \n    - Initially S contains only the source vertex. \n    - The algorithm is iterative, adding one vertex to S on each pass. \n- We maintain a table D such that for each vertex v, D(v) is the minimum distance from the source vertex to v via vertices that are already in S (aside possibly from v itself). \n- Greed: on each iteration, add to S the vertex v not already in S **for which D(v) is minimal**.\n\n# Jarnik-Prim MST Algorithm\nBy modifying Dijkstra’s SSAD Algorithm to \n\n- build a list of the edges that are used as vertices are added, and \n- storing the distance from nodes to the current tree (rather than from nodes to the source)\n\nWe obtain Prim’s Algorithm (V Jarnik, 1930 and R C Prim, 1957).\n\n## Difference with Dijkstra \n- Dijkstra's SSAD Algorithm not necessarily find a minimum-weight spanning tree, because each steps only greedily finds the shortest path from the current node to the **source node**.\n- By contrast, Jarnik-Prim's algorithm greedily finds the path from the current node to **any node in the current tree**\n\n# DFS\n\n## DFS - Types of Edges\nImage shot from [Video: DFS - Types of Edges | Edge Classification | Tree Edge, Back Edge, Forward Edge, Cross Edge\n](https://www.youtube.com/watch?v=Y78KivF-hm0)\n\n### Digraph \n\n<img src=\"dfs-types-of-edges.png\" alt=\"\" width=\"700px\"/>\n\n- Tree Edge: Part of the tree\n- Forward Edge: edge from ancestor to grandchildren\n- Cross Edge: edge connecting two nodes with different parents\n- Back Edge: edge from grandchildren to ancestor\n\nThe numbers beside each node is the [pre/post time](https://www.youtube.com/watch?v=2UvzjtJb-WQ). Now we can see the relationship between the pre/post time and the types of edges:\n- For Tree Edge [u,v]: \n    - pre[u] < pre[v] < post[v] < pre[u]\n    - u is the parent, and v is the children.\n- For Forward Edge [u,v]: \n    - pre[u] < pre[v] < post[v] < post[u]\n    - u is the ancestor, and u is the grandchildren. \n- For Back Edge [u,v]: \n    - pre[v] < pre[u] < post[u] < post[v]\n    - v is the ancestor, and u is the children.\n- For Cross Edge [u,v]: \n    - pre[v] < post[v] < pre[u] < post[u] \n    - v gets visited and then u. \n\n### Undirected Graph\nThere will only be two types of edges:\n- Tree Edge\n- Back Edge\n\n<img src=\"undirected-type.png\" alt=\"\" width=\"600px\"/>\n\nWhy no cross edge? In undirected graph, what would have been \"cross edge\" would be taken as tree edge. \nFor example, the edge c-> e (now c-e) was taken when we traverse to c.\n\n\n# Shortest path tree v.s. Min-cost tree\n\n<img src=\"short-path-min-cost.png\" alt=\"\" width=\"400px\"/>\n\n- Shortest path tree must have a **root**. Then the tree is one that all node's path the the root is the shortest possible path.\n- Min cost tree does not need a root to define it, it just looks for a tree that has the minimum total weight of edges.\n\n\n\n","tags":["Distributed Algorithms","Algorithms","Graph Theory"]},{"title":"Parallel and Distributed Computing Intro Notes","url":"/2020/08/10/Parallel-and-Distributed-Computing-Learning-Notes/","content":"# Parallel vs Distributed Computing\n\n- Coupling\n    - Parallel computing: tight coupling between tasks, that cooperate on **shared memory**\n    - Distributed computing: loose coupling between nodes, that cooperate by **messaging**\n- Algorithm implementation\n    - In classical algorithms, the problem is **encoded** and given to the (one single) processing element\n    - In parallel algorithms, the problem is **encoded and partitioned**, and then its chunks are given to the processing elements.\n    - In distributed algorithms, the problem is given by the **network** itself and solved by **coordination protocols**\n\n**Distributed systems** are groups of networked computers which share a common goal for their work.\n\n# Lynch's Discussion of how Distributed Algorithms differ\nThere are many different kinds of distributed algorithms. Some of the attributes by which they differ include：\n\n## The Inter-process Communication (IPC) Method\nDistributed algorithms run on a collection of processors, which need to communicate somehow. They could communicate through:\n- Accessing Shared Memory\n- Point-to-Point or Broadcast Messaging\n- Executing Remote Procedure Calls\n\n## Timing Model\nTiming of events in the system, reflecting the different types of timing information that might be used by algorithms.\n- Completely synchronous: performing communication and computation in Lock-step synchrony\n- Completely asynchronous: each node can take steps at arbitrary speeds and in arbitrary orders.\n- Partially asynchronous: in between, some timing restrictions but not completely lock-step. For example, each node could \n    - have bounds on their relative speeds, or \n    - have access to approximately synchronized clocks.\n\n## Failure Model\n- The algorithm can assume that there will be no node failures\n- The algorithm can also be designed to tolerate limited amount of faulty behavior, such as:\n    - Processors might just stop, with or without warning; \n    - Processors might fail transiently (转瞬即逝地); \n    - Processors might exhibit more severe Byzantine failures, where a failed processor can behave arbitrarily. \n    - Failures of the communication mechanisms, including \n        - message loss\n        - duplication\n\n## Problem Addressed\nThe typical problems that are considered are those that arise in the application areas mentioned above. \nTypical problems：\n- resource allocation, \n- communication, \n- consensus among distributed processors, \n- database concurrency control, \n- deadlock detection, \n- global snapshots, \n- synchronization\n\n# Typical scenario in distributed computing\n\n- computing nodes have local memories and unique IDs\n- nodes are arranged in a network: graph, digraph, ring...\n- neighbouring nodes communicate by message passing (This is a typical IPC method)\n- independent failures are possible: nodes, communication channels (Failure model)\n- the network topology (size, diameter, neighbourhoods) and other characteristics (e.g. latencies) are often unknown to **individual nodes**  \n- the network may or may not dynamically change\n- nodes solve parts of a bigger problem or have individual problems but still need some sort of **coordination** - which is achieved by distributed algorithms.\n\n\n# Rounds and Steps\nNodes work in rounds (macro-steps), which have three sub-steps:\n- 1 Receive sub-step: receive incoming messages\n- 2 Process sub-step: change local node state\n- 3 Send sub-step: send outgoing messages\n\n> Note: some algorithms expect null or empty messages, as an explicit con\frmation that nothing was sent\n\n# Synchronise model as a particular case of asynchronous models\nWe describe timing model with the rounds and steps above.\n\n- Synchronous model， version 1\n    - all nodes: *process* takes 1 time unit\n    - all messages: transit time (*send* -> *receive*) takes 0 time units\n- Synchronous model, version 2\n    - all nodes: *process* takes 0 time units\n    - all messages: transit time (*send* -> *receive*) takes 1 time units\n\n> The second (and equivalent) version ensures that **synchronised models are a particular case of asynchronous models**\n\n# Asynchronous model\n- all nodes: *process* takes 0 time units\n- each message (individually): transit time (send -> receive) takes **any real number of time units** (for synchronous it is 1 time unit)\n    - or, normalised: any real number in the [0; 1] interval\n\n- often, a FIFO guarantee, which however may affect the above timing assumption (see next slide)\n\n## async time complexity >=\u0015 sync time complexity\nTime complexity (**worst-case**) = supremum of all possible normalised async runs.\n\nSync run is just one of the all possible runs, where all transit is 1 time unit.\n\n## Asynchronous model with FIFO channels\n- The FIFO guarantee: faster messages cannot overtake earlier slower messages sent over the same channel\n- Congestion (pileup) may occur and this should be accounted for\n- The timing assumption: (each message (individually): transit time (send -> receive) takes **any real number of time units** (for synchronous it is 1 time unit), or, normalised: any real number in the [0; 1] interval) only applies to the top (oldest) message in the FIFO queue\n- After the top message is delivered, the next top message is delivered after an additional arbitrary delay.\n- Thus, **a sequence of n messages may take n time units** until the last is delivered\n\n# Non-determinism\nBoth sync and async timing models are non-deterministic.\n- Choice of request (sync and async)\n- Message delay may vary (async)\n\nHowever, all executions must arrive to a valid decision (not necessarily the same, but valid).\n\n## Confluent system\nIn computer science, confluence is a property of rewriting systems, describing which terms in such a system can be rewritten in more than one way, to yield the same result. \n","tags":["Distributed Algorithms","Algorithms"]},{"title":"Null-Coalescing Operator ?? in C sharp","url":"/2020/08/08/Null-Coalescing-Operator-in-C-sharp/","content":"I was taking the [LINQ course on Udemy](https://www.udemy.com/course/the-ultimate-linq-with-csharp-masterclass-basics-to-advanced/learn/lecture/9475736#overview) and I came across new operators: [?? and ??=](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/null-coalescing-operator).\n\n# ??\n\n> The null-coalescing operator ?? returns the value of its left-hand operand if it isn't null; otherwise, it evaluates the right-hand operand and returns its result. The ?? operator doesn't evaluate its right-hand operand if the left-hand operand evaluates to non-null.\n\n\n# ??=\n\n> Available in C# 8.0 and later, the null-coalescing assignment operator ??= assigns the value of its right-hand operand to its left-hand operand only if the left-hand operand evaluates to null. The ??= operator doesn't evaluate its right-hand operand if the left-hand operand evaluates to non-null.\n\n\n# Right-Associative\n\nThe null-coalescing operators are right-associative. That is, expressions of the form\n```\na ?? b ?? c\nd ??= e ??= f\n```\n\nAre evaluated as\n``` \na ?? (b ?? c)\nd ??= (e ??= f)\n```\n\n\n# Usage\n\n## Giving a default value when null-conditional operations evaluate to null\nIn expressions with the null-conditional operators [?. and ?[]](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/member-access-operators#null-conditional-operators--and-), you can use the ?? operator to provide an alternative expression to evaluate in case the result of the expression with null-conditional operations is null.\n\nThis is how the Udemy course uses it:\n\n<img src=\"coalesce.png\" alt=\"\" width=\"700px\"/>\n\nAnother example from [Microsoft Docs](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/null-coalescing-operator#examples):\n``` \ndouble SumNumbers(List<double[]> setsOfNumbers, int indexOfSetToSum)\n{\n    return setsOfNumbers?[indexOfSetToSum]?.Sum() ?? double.NaN; //?[] will get null if setOfNumbers is null \n}\n\nvar sum = SumNumbers(null, 0);\nConsole.WriteLine(sum);  // output: NaN\n\n```\n\n## Work with nullable value types\nWhen you work with nullable value types (`T?`) and need to provide a value of an underlying value type, use the ?? operator to specify the value to provide in case a nullable type value is null:\n```\nint? a = null;\nint b = a ?? -1;\nConsole.WriteLine(b);  // output: -1\n```\n\n## Throw exception on null values\nBeginning with C# 7.0, you can use a throw expression as the right-hand operand of the ?? operator to make the argument-checking code more concise:\n```\npublic string Name\n{\n    get => name;\n    set => name = value ?? throw new ArgumentNullException(nameof(value), \"Name cannot be null\");\n}\n```\n\n## Assign value if null\n\nBeginning with C# 8.0, you can use the ??= operator to replace the code of the form\n\n``` \nif (variable is null)\n{\n    variable = expression;\n}\n```\n\nBy:\n``` \nvariable ??= expression;\n```","tags":["C#"]},{"title":"Reading Note - Effect of Clinical Decision-Support Systems: A Systematic Review","url":"/2020/08/07/Effect-of-Clinical-Decision-Support-Systems-A-Systematic-Review/","content":"This is the reading note for HlthInfo730 Week1 reading, [Effect of Clinical Decision-Support Systems: A Systematic Review](https://www.acpjournals.org/doi/full/10.7326/0003-4819-157-1-201207030-00450).\n\n# Motivation\nTo find evidence of the effects of Clinical Decision-Support Systems(CDSS), to support widespread use of it. \n\n# Defining CDSS\n A CDSS is “any **electronic system** designed to aid directly in clinical decision making, in which **characteristics of individual patients** are used to generate **patient-specific** assessments or recommendations that are then presented to **clinicians** for consideration”\n \n# Classic CDSSs\n\n- Classic CDSSs have\n     - alerts, \n     - reminders, \n     - order sets, \n     - drug-dose calculations,\n     \nto automatically remind the **clinician** of a specific action, or care summary dashboards that provide performance feedback on quality indicators. \n-  **Information retrieval tools**, such as an “infobutton” embedded in a clinical information system, are designed to aid clinicians in the search and retrieval of context-specific knowledge from information sources based on patient-specific information from a clinical information system.\n- **Knowledge resources**, such as UpToDate, Epocrates, and MD Consult, consist of distilled primary literature that allows selection of content germane to a specific patient to facilitate decision making at the point of care or for a specific care situation.\n\n\n# Assessing Outcomes of CDSS\n\nThe research looks for the following types of CDSS's outcomes:\n- Clinical Outcomes \n    - length of stay, \n    - morbidity (CDSSs improved morbidity outcomes), \n    - mortality (CDSSs have low impact),\n    - health-related quality of life, \n    - adverse events (low impact)\n- Health care process \n    - Recommended Preventive Care Service Ordered or Completed (significant impact) \n    - clinical study, \n    - treatment ordered or completed \n- User workload and efficiency (user knowledge, number of patients seen, clinician workload, and efficiency),\n- Relationship-centered (patient satisfaction), \n- Economic (cost and cost-effectiveness), \n- Use and implementation by a health care provider (acceptance, satisfaction, use, and implementation).\n\n\n","tags":["Health Informatics"]},{"title":"IAM Policies: Inline-Managed, Resource Based-Identity Based ","url":"/2020/08/06/IAM-Policies-Inline-Managed-Resource-Based-Identity-Based/","content":"\nA policy is an object in AWS that, when associated with an identity or resource, defines their permissions. \n\n# Identity-based and resource-based policy\n\nWhen you create a permissions policy to restrict access to a resource, you can choose [an identity-based policy or a resource-based policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html).\n\n## Identity-based policy\nIdentity-based policies are attached to an IAM user, group, or role.\n\n## Resource-based policy\nResource-based policies are attached to a resource.  \n\nThe IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role. **[An IAM role is both an identity and a resource that supports resource-based policies]**. \nFor that reason, you must attach **both a trust policy and an identity-based policy to an IAM role**. \n\n\n\n# Managed Policies and Inline Policies\nWhen you need to set the permissions for an identity in IAM, you must decide whether to use [an AWS managed policy, a customer managed policy, or an inline policy.](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html)\n\n## AWS Managed Policies\nAn AWS managed policy is a standalone policy that is created and administered by AWS. Standalone policy means that the policy has its own Amazon Resource Name (ARN) that includes the policy name. \n\nAWS managed policies are designed to provide permissions for many common use cases. \n\nYou cannot change the permissions defined in AWS managed policies. AWS occasionally updates the permissions defined in an AWS managed policy. When AWS does this, the update affects all principal entities (users, groups, and roles) that the policy is attached to. \n\n## Customer Managed Policies\n\nYou can create standalone policies that you administer in your own AWS account, which we refer to as customer managed policies. You can then attach the policies to multiple principal entities in your AWS account. When you attach a policy to a principal entity, you give the entity the permissions that are defined in the policy. Each policy also have its own ARN.\n\nA great way to create a customer managed policy is to start by copying an existing AWS managed policy. That way you know that the policy is correct at the beginning and all you need to do is customize it to your environment.\n\n## Inline Policies\n- For Identity-based policy:\n    - An inline policy is a policy that's **embedded in an IAM identity** (a user, group, or role). \n    - The policy is an inherent part of the identity. You can create a policy and embed it in a identity, either when you create the identity or later.\n\n- For Resource-based policy:\n    - Resource-based policies are inline only, not managed.\n\n# What a policy looks like\nA policy defines what actions are allowed or denied for specific AWS resources. This policy is granting permission to List and Describe information about EC2, Elastic Load Balancing, CloudWatch and Auto Scaling. This ability to view resources, but not modify them, is ideal for assigning to a Support role.\n\nThe basic structure of the statements in an IAM Policy is:\n- **Effect** says whether to Allow or Deny the permissions.\n- **Action** specifies the API calls that can be made against an AWS Service (eg cloudwatch:ListMetrics).\n- **Resource** defines the scope of entities covered by the policy rule (eg a specific Amazon S3 bucket or Amazon EC2 instance, or which means any resource*).\n\n[AmazonEC2ReadOnlyAccess](https://console.aws.amazon.com/iam/home?region=us-east-1#/policies/arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess$serviceLevelSummary):\n```\n{\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Action\": \"ec2:Describe*\",\n         \"Resource\": \"*\"\n       },\n       {\n         \"Effect\": \"Allow\",\n         \"Action\": \"elasticloadbalancing:Describe*\",\n         \"Resource\": \"*\"\n       },\n       {\n         \"Effect\": \"Allow\",\n         \"Action\": [\n           \"cloudwatch:ListMetrics\",\n           \"cloudwatch:GetMetricStatistics\",\n           \"cloudwatch:Describe*\"\n         ],\n         \"Resource\": \"*\"\n       },\n       {\n         \"Effect\": \"Allow\",\n         \"Action\": \"autoscaling:Describe*\",\n         \"Resource\": \"*\"\n       }\n     ]\n   }\n```","tags":["AWS","AWS Lab Session Preparation"]},{"title":"Read from stdin with C#","url":"/2020/08/02/Read-from-stdin-with-C/","content":"Just a quick note on how to read from a file with stdin.\n\n```C#\n// program.cs\n\nusing System;\n   \n   namespace EchoAndSize\n   {\n       class Program\n       {\n           static void Main(string[] args)\n           {\n               String line = Console.ReadLine();\n               Console.WriteLine(line);\n           }\n       }\n       class Program2\n       {\n   \n       }\n   }\n```\n\nIn command line:\n``` mono Program.exe < echo.txt\n```\n\n","tags":["C#"]},{"title":"Using mono to run C# program on Mac terminal","url":"/2020/08/02/Using-mono-to-run-C-program-on-Mac-terminal/","content":"\nI needed a way to run C# console programmes in my Mac Terminal, as I needed to load file input through `stdin`.\n\nI learnt online that [mono](https://www.mono-project.com/) is the framework to do it:\n> Mono is a software platform designed to allow developers to easily create cross platform applications part of the .NET Foundation.\n\nSo I downloaded the VS release from [here](https://www.mono-project.com/download/stable), and then hopped to the [Mono Basics](https://www.mono-project.com/docs/getting-started/mono-basics/) getting started guide to compile and run my first terminal C# program.\n\n# Adding the path to mono into bash PATH\n\nAs with `Go` I was told by terminal `bash command not found`. \nI have been messing with `PATH` several times so I thought this should be the time that I figure out a robust way to add a new path and document it here.\n\n[This stackexchange post](https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path) solved my problem. The steps I followed are:\n\n1. Find out where the `bin` for the new command is installed. For mono it is `/Library/Frameworks/Mono.framework/Versions/Current/bin`.\n\n2. open the `~/.bash_profile` file \n```\nsudo nano ~/.bash_profile\n```\n\n3. Here's what the `~/.bash_profile` file look like:\n```\n  GNU nano 2.0.6        File: /Users/anran/.bash_profile                        \nPATH=\"/Library/Frameworks/Python.framework/Versions/3.7/bin:${PATH}\"\nexport PATH\nexport PATH=/Library/Frameworks/Python.framework/Versions/3.7/bin:/usr/local/bi$\nexport PATH=\"$PATH:/usr/local/smlnj/bin\"\nexport PATH=\"$HOME/.composer/vendor/bin:$PATH\"\nalias composer=\"php /usr/local/bin/composer.phar\"\nexport PATH=\"/usr/local/opt/php@7.3/bin:$PATH\"\nexport PATH=\"$PATH:/usr/local/smlnj/bin\"\n[[ -s \"$HOME/.rvm/scripts/rvm\" ]] && source \"$HOME/.rvm/scripts/rvm\" # Load RVM$\n```\n\n`export` explicitly changes the `PATH` env variable. Each `export PATH=\"$PATH:<new_path>` command adds a new path to `PATH`.\n\nAdd a new line just before the line to load RVM:\n```\nexport PATH=\"$PATH:/Library/Frameworks/Mono.framework/Versions/Current/bin\"\n```\n\nSave the file and exit.\n\n4. Reopen the terminal. \n\nTo verify the path is correctly added, run `echo $PATH` to see the new `PATH`'s content.\n\n5. Now `mono` and `csc` are available directly through Terminal. To compile and execute my program:\n\n```\n$ csc Program.cs\nMicrosoft (R) Visual C# Compiler version 3.6.0-4.20224.5 (ec77c100)\nCopyright (C) Microsoft Corporation. All rights reserved.\n   \n$ mono Program.exe\nHello World!\n```\n\n\n\n","tags":["C#","bash"]},{"title":"What is gRPC","url":"/2020/08/01/What-is-gRPC/","content":"\nRPC: Remote Procedure Calls\n\nOpen sourced on [github](https://github.com/grpc/grpc).\n\n> In gRPC, a client application can directly **call** a method on a server application on a different machine as if it were a local object, making it easier for you to create distributed applications and services.\n\n# How gRPC works\n- As in many RPC systems, gRPC is based around the idea of **defining a service**, specifying the methods that can be called remotely with their parameters and return types.\n-  On the server side, the server implements this interface and runs **a gRPC server** to handle client calls.\n- On the client side, the client has a **stub** (referred to as just a client in some languages) that **provides the same methods as the server**.\n- gRPC clients and servers can run and talk to each other in a variety of environments - from servers inside Google to your own desktop - and can be written in any of gRPC’s supported languages.\n\n# Four types of class from gRPC\n## Unity calls\nSend request as a client, get a single response.\n## Server and client streaming\n- Send as many request as I want, and get one final response from the server; \n- Send one initial request and get multiple responses from the server\n## Full bidirectional streaming\nboth directions are streaming (multiple messages)\n\n# Protocol Buffers\n**Protobuf is the IDL, Interface Definition Language, used by gRPC.**\n\n## Server Side\nThis is the `greet.proto` file built by `.NET`'s gRPC **service** template:\n\n<img src=\"grpc-proto.png\" alt=\"\" width=\"400px\"/>\n\n`service Greeter` defines what service is available. `rpc` defines the \"process\" and the request/reply that the process takes in and returns.\n\nProto will generate C# code for services.\n\n### Server side `StartUp`\n- `configureServices` makes a call to `AddGrpc`\n- `configure` builds the app's routing endpoints.\n    - in here you should **register** all of your services. \n\n## Client Side\nFor the client side we need a `using var channel = GrpcChannel.ForAddress(\"https://localhost:5001\");`\nThen we create a client with `var client = new GreeterClient(channel);`\nNow we can involve the task that is available from the server, and pass it the request: `var request = new HellowRequest {Name = \"anran\" };` `var response = await client.SayHellowAsync(request);` \nTo print the message we can now do:  `Console.WriteLine(response.Message);`\n\n## gRPC is an opinionated protocol\nMany things are built in into the protocol. e.g. You don't need to write down the start path of the Service, as long as the Client knows the existence of the endpoint.","tags":["Distributed Algorithms"]},{"title":"Echo: a fundamental diffusing algorithm","url":"/2020/07/29/Echo-a-fundamental-diffusing-algorithm/","content":"\n# Starting and Terminating options for Distributed algorithms\nFor distributed algorithms you need a way to let your distributed system know when it is time to start or terminate the computation.\n## Start Options\n- Single source/ Centralised/ Diffusing. Echo is a typical diffuing algorithm, starting from one single source.\n- Many source/ Decentralised\n## Terminate Options\n- Sometimes one node (often the initiator) takes the final decision and can notify the rest (usually **omitted phase**)\n- In general, this can be very challenging and requires sophisticated control algorithms for termination detection.\n\n# Echo Algorithm\nEcho is a fundamental **diffusing** algorithm. \n\n## Two phases\nEcho combines two phases:\n- A broadcast: \"top-down\", builds child-to-parent links in a **spanning tree**\n- ConvergeCast: \"bottom-down\"/echo phase, confirms that the node has finished its part.\n\n## Links(pointers)\nChild-to-parent links built at broadcast. Parent-to-child links built either at convergecast or by additional confirmation messages immediately after the broadcast.\n\n## Termination\nAfter receiving echo tokens from all its neighbours, the source node **decides the termination**. Optionally, the source node can start a third phase to broadcast this termination to other nodes.\n\n## Sync Mode\nIn Sync mode, you get a BFS spanning tree in broadcast phase (Echo also known as SyncBFS)\n\n### Time complexity analysis\n\nTime complexity: \nTime Units <= 2D + 1\n\nD is the depth of the BFS spanning tree. \n- In the broadcast stage, in each time unit, the messages go deeper into the tree by 1. It takes at most D time units to reach all nodes.\n- For the leaf nodes to know that their neighbours have all received a message, it takes 1 extra time unit for all leaf nodes to send messages to neighbours. Now all leaf nodes are ready to ConvergeCast.\n- For the root node to receive message for neighbours, nodes at different lengths will need to ConvergeCast back, coming one level up the depth at each time unit. This process takes at most D time units too.\n\n### Message complexity analysis\n\nMessages = 2|E|\n\nBecause each node will expect to receive from all its neighbours, each edge will have two messages (-> and <-) going through it. \n\n## Async Mode\nIn async mode, you get a spanning tree, but not necessarily a BFS spanning tree.\n\n### Time complexity \n\nTime complexity: \nTime Units = O(|V|)\n\nThere are fast and slow messages. \nEach slow message takes 1 (normalised) time unit. Each fast message only takes `Epsilon`. \n\nAsync is a kind of an eager process. \n- You build a spanning tree eagerly, taking whichever path is fast for you - the spanning tree gets built pretty fast, ~ |V|*`Epsilon` time. \n- Now what is left for you is the slow edges. To finalise the echo, each node will need to finish their ConvergeCast on a slower node. \n\n### Message complexity\nThe message number remains the same as sync model, \nmessage number = 2E = O(|E|)\n\n## Time complexity: Async v.s Sync \n- Sync is a special case of Async. \n    - Async's time complexity is a **supremum over all possible normalised executions** (delivery time in [0; 1]), inclusing sync.\n- In Sync, all edges takes 1 time unit to travel.  \n    - Waiting for all messages to arrive before making a decision and proceed to the next step helps us build the optimised BFS tree.  \n    - With a BFS tree, we can ensure that echo finishes in 2|D| + 1 time.\n- In Async, the eager broadcast means we might not choose the overall efficient tree. All graphs could behave like a line. \n    - Actually, with a line, |V| = |D| and O(|V|)=O(|D|), async and sync has the same complexity.\n\n### What is supremum?\nFor set (0,1), the supremum is 1. There is no maximum.\nFor set [0,1], the supremum and the maximum are both 1.\n","tags":["Distributed Algorithms","Algorithms"]},{"title":"Clinical Decision Support System introduction","url":"/2020/07/29/Clinical-Decision-Support-System-introduction/","content":"\n# Diagnosis & Treatment\n## Diagnosis (Dx)\nRecognizing a disease or condition by its outward presentation (signs and symptoms) and assessments (labs, X-Ray etc.)\n\n## Differential diagnosis\nThe process of weighing the probability of various other conditions accounting for a patient's condition.\n- process of narrowing a list of all possible diagnoses that could explain what’s medically wrong with the patient using tests to exclude various diagnoses\n- The differential diagnosis of rhinitis (a runny nose) includes allergic rhinitis (hayfever), the abuse of nasal decongestants and, of course, the common cold.\n\n## Treatment (Tx): providing or prescribing a remedy based on a patient’s diagnosis (most of the time!)\nThe doctor doesn’t need to narrow the differential Dx to one option – just need to be precise enough to decide a course of action (e.g., a drug treatment, or just ‘watchful waiting’).\n\n<img src=\"dx-process.png\" alt=\"\" width=\"700px\"/>\n\n# Care Planning\nA Care Plan is an agreed set of actions and possibly goals.\n\n- In nursing, this can include recording systematic observations, administering medication, bandage changes, etc.\n– In chronic disease management, this can take the long view and action may be coordinated by the patient\n    - E.g., annual foot examination in diabetes, or adhering to an exercise\nand/or diet regimen\n    - May include targets for blood pressure, HbA1c, BMI, etc.\n\n\n\n\n\n\n","tags":["Health Informatics","Coursework"]},{"title":"Operational Excellence Pillar: WA framework","url":"/2020/07/24/Operational-Excellence-Pillar-WA-framework/","content":"\n# Design Principles\n- Perform Operations with code\n- Annotate document changes: automate the creation of documentation after every build, or you can automatically annotate (注解) hand-crafted documentation. Annotated documentation can be used by people and systems. Use annotations as an input to your operations code\n- Make frequent, small, reversible changes: You want to design workloads to allow components to be updated regularly. \n- Refine standard operations frequently\n- Anticipate failure: pre-mortem (事前验尸)\n- Learn from all operational failures\n\n# Core Services\n\n<img src=\"core-services.png\" alt=\"\" width=\"700px\"/>\n\n- **AWS CloudFormation** is key for operational excellence because it helps you ensure reliability. The service lets you treat infrastructure as code, and it provides templates you can use to replicate your environment.\n- To prepare, **AWS Config** and **AWS Config Rules** can be used to *create standards for workloads*, and to *determine whether environments are compliant with those standards before they are put into production*.\n- **CloudWatch** allows you to *monitor the operational health of a workload*.\n- **Amazon Elasticsearch** Service allows you to *analyze your log data to gain actionable insights quickly and securely*.\n\n# Anti-Patterns\n## Focus only on technology metrics\nYou might not be delivering value to the customer if you’re not paying attention to latency.\nYou want to also have business metrics (which AWS offers)\n\n## Batch Changes\nInstead, make small, reversible changes\n\n## Manual Changes\n\nIt is hard to reproduce errors caused by manual changes\n\n## Stale (outdated) Document\nHaving outdated documentation or no documentation can create problems. Put a process in place to ensure all documentation is up-to-date.\n\n# Prepare\n\n## Monitor the application, platform, and infrastructure components\nYou can use CloudWatch alarms, and send the information from CloudWatch logs to a dashboard to see the health of your infrastructure at any time. You can use this information to **understand the customer experience and customer behaviors**.\n\n## Validate workloads before moving into production\nAsk yourself: are the workloads supported by operations?\n\n## Perform Cloud Operations\n- Use Checklist for standard and required procedures\n- Check that required procedures are adequately captured in runbooks and playbooks\n- Validate trained personnel to make sure everyone is enabled\n\n## Test responses to operational events and failures \nMake sure you test responses to operational events and failures so that you can quickly recover from them.\n\n<img src=\"prepare-services.png\" alt=\"\" width=\"700px\"/>\n\nKey service: AWS Config, AWS Config Rules to define the standards to stick to.\n\n# Operate\n- Achieve business and customer outcomes through the successful operation of a workload.\n- Manage operational events with efficiency and effectiveness. You can do this by:\n    - establishing **baselines** that you use to identify the improvement or degradation of operations, \n    - collecting and analyzing your **metrics**\n    - then validating your understanding of how you **define operational success** and how it changes over time.\n- Communicate the **operational status** of workloads. \n- Consider that operational health includes both the health of the **workload**, and the health and success of the operations that act upon the workload—for example, **deployment and incident response**.\n- Use **dashboards and notifications** so that information can be accessed automatically. The more people have access to information about the health of your infrastructure, the healthier it will be.\n- Take the time to determine the root cause of workload outage.\n\n<img src=\"operate-services.png\" alt=\"\" width=\"700px\"/>\n\n# Evolve\n- Dedicate work cycles to making continuous incremental improvements.\n- Regularly evaluate and prioritize opportunities for improving procedures for both workloads and operations, such as **feature requests, issue remediation, and compliance requirements**. \n- Identify areas for improvement, and include **feedback loops** within your procedures.\n- Share “lessons learned” across teams to share the benefits of those lessons. \n    - Analyze trends within the lessons learned\n    - Perform cross-team retrospective analysis of operations metrics\n    - Identify opportunities and methods for improvement. \n    - Implement changes and evaluate the results \n\n<img src=\"evolve-services.png\" alt=\"\" width=\"700px\"/>\n\n## Evolve with AWS Services - Developer Tools\n\n### AWS CodeStar\n\nEach AWS CodeStar project comes with a **project management** dashboard, including an integrated issue tracking capability powered by Atlassian JIRA Software. \nAWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. \nThere is no additional charge for using AWS CodeStar. \n\n### AWS CodeCommit\nAWS CodeCommit is a fully-managed **source control service** that hosts secure **Git-based** repositories. \n\n### AWS CodeBuild\nAWS CodeBuild is a fully managed **continuous integration** service that compiles source code, runs tests, and produces software packages\n\n### AWS CodeDeploy\nAWS CodeDeploy is a fully managed **deployment** service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\n\n### AWS CodePipeline\nAWS CodePipeline is a fully managed **continuous delivery** service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\n\n### AWS X-Ray\n\nAWS X-Ray[https://aws.amazon.com/xray/] helps **developers** analyze and debug production, distributed applications, such as those built using a microservices architecture. \nWith X-Ray, you can:\n\n- understand how your application and its underlying services are performing \n- identify and troubleshoot the root cause of performance issues and errors\n- have an end-to-end view of requests as they travel through your application\n- see a map of your application’s underlying components. \n\nYou can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.\n\n## Evolve with AWS Services - Amazon Elasticsearch Service\n[Amazon Elasticsearch Service](https://aws.amazon.com/elasticsearch-service/) provides support for open source Elasticsearch APIs, managed Kibana, integration with Logstash and other AWS services, and built-in alerting and SQL querying. \nIt allows you to **analyze your log data** to gain actionable insights quickly and securely.\n\n# What factors drive your operational priorities?\n- Business Needs\n- Compliance Requirements\n- Risk Management\n\n# Determining whether you are ready to support a workload\nBest practices:\n- Continuously improving your **culture**. This best practice governs the way you operate. You must recognize that change is constant, and that you need to continue to experiment and evolve by acting on opportunities to improve.\n- Having a **shared understanding** of the value to the business. Make sure that you have cross-team consensus on the value of the workload to the business, and that you have procedures that you can use to engage additional teams for support.\n-  Ensuring that you have enough personnel so that you can have an appropriate number of **trained personnel** to support the needs of your workload. \n    - Perform regular reviews of workload demands\n    - Train existing personnel or adjust personnel capacity as needed.\n-  Making sure that governance and guidance are documented and accessible.\n    - Ensure that standards are accessible, readily understood, and measurable for compliance. Make sure that you have a way to propose changes to standards, and request exceptions.\n- Using **checklists** to evaluate whether you are ready to operate workloads. These checklists include **operational readiness checklists** and **security checklists**.\n- Having **runbooks** for events and procedures that you understand well. \n- Having **a playbook** for failure scenarios.\n- **Practicing recovery** so that you can identify potential failure scenarios and test your responses—for example, *game days*, and *failure injection*.\n\n# What factors drive your understanding of operational health?\nBest Practices:\n- Defining expected business and customer outcomes. Make sure that you have a **documented** definition of what success looks like for the workload, from business and customer perspectives.\n- Identifying **success metrics**. Define metrics that can be used to measure the behavior of the workload against the **expectations of the business and of customers**.\n- Identifying **workload metrics**. Define metrics that can be used to measure the status and success of the **workload and its components**. (Technology metrics)\n- Identifying **operations metrics**. Define metrics that can be used to measure the execution of **operations activities**, such as runbooks and playbooks.\n- Establishing **baselines** for metrics so that they provide expected values as the basis for comparison.\n- Collecting and analyzing your metrics. Perform **regular, proactive reviews** to identify trends and determine responses.\n- Validating insights. Review the results of your analysis and responses with **cross-functional teams and business owners**. Adjust the responses as appropriate.\n- Taking a business-level view of your operations. Determine whether you are satisfying **customer needs**, and identify areas that need improvement so that you can reach your **business goals**.\n- Determining the priority of operational events based on their **impact on the business**. When multiple events require intervention, priority is based on the business impact.\n- Putting processes in place to handle event, incident, and problem management. \n- Processing each alert. **Any event for which you raise an alert should have a well-defined response**, such as a runbook or playbook. The event should also have a specifically identified **owner**, such as an individual, a team, or a role.\n- Defining **escalation(raise) paths**. **Runbooks and playbooks** should have a definition for what triggers an escalation, a process for escalation, and specifically identify the owners for each action. Escalations might include third parties, such as for example, vendors, AWS Support, and others.\n- Identifying decision makers. \n- Communicating operating status through dashboards.\n- Pushing **notifications to communicate with your users** when the services they consume are being \nimpacted, and when the services return to normal operating conditions, such as via email or SMS.\n- Establishing a **root cause analysis** process that identifies and documents the root cause of an event.\n- Communicating the root cause of an issue or event. Also make sure that you tailor your communications to the target audiences.\n\n\n \n\n\n\n\n\n\n\n\n","tags":["AWS","AWS Well-Architectured Framework"]},{"title":"Build and deploy applications to Azure by using GitHub Actions","url":"/2020/07/22/Build-and-deploy-applications-to-Azure-by-using-GitHub-Actions/","content":"\nThis is my learning note when learning Continuous Delivery to Azure by GitHub Actions through [Microsoft Learn Module: Build and deploy applications to Azure by using GitHub Actions](https://docs.microsoft.com/en-gb/learn/modules/github-actions-cd/) and GitHub Learning Lab, [GitHub Actions: Continuous Delivery with Azure](https://lab.github.com/githubtraining/github-actions:-continuous-delivery-with-azure)\n\n> Continuous Delivery is a software development discipline where you build software in such a way that the software can be released to production at any time.\n\n# Options for triggering a CD workflow\n## ChatOps\nChatOps uses chat clients, chatbots and real-time communication tools to execute tasks. For example, you might leave a specific comment in a pull request that can kick off a bot. That bot might comment back with some statistics or run a workflow.\n## Labels in your pull request\nDifferent labels can start different workflows.  To use labels, your workflow will look like this:\n\n```yaml\non:\n  pull_request:\n    types: [labeled]\n```\n\n# Store credentials with GitHub Secrets\nGitHub Secrets is a secure place to store sensitive information that your workflow will need. \nIn order to deploy to an Azure resource, the GitHub Action must have permission to access the resource. \n\n<img alt=\"\" src=\"https://docs.microsoft.com/en-gb/learn/github/github-actions-cd/media/2-secrets.png\" width=\"800px\"/>\n\nTo access the secret:\n```yaml\nsteps:\n      - name: \"Login via Azure CLI\"\n        uses: azure/login@v1\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n```\n\n# Triggering CD with labelling\nIn the case for the GitHub Learning lab, we'll use labels as triggers for multiple tasks:\n- When someone applies a \"spin up environment\" label to a pull request, that'll tell GitHub Actions that we'd like to set up our resources on an Azure environment.\n- When someone applies a \"stage\" label to a pull request, that'll be our indicator that we'd like to deploy our application to a staging environment.\n- When someone applies a \"destroy environment\" label to a pull request, we'll tear down any resources that are running on our Azure account.\n\n# Job conditionals\nGitHub Actions features powerful controls for when to execute jobs and the steps within them. One of these controls is if, which allows you run a job only when a specific condition is met:\n```yaml\nif: contains(github.event.pull_request.labels.*.name, 'peacock')\n```\n\n# GitHub deploy-staging workflow\nThe [.github/workflows/deploy-staging.yml](https://github.com/niuniuanran/github-actions-continuous-delivery-azure/blob/master/.github/workflows/deploy-staging.yml) defines the logic of the GitHub workflow for staging the deployment. GitHub Actions is cloud agnostic, so any cloud will work with it.\n\n# Azure Configuration\nThe [.github/workflows/spinup-destroy.yml](https://github.com/niuniuanran/github-actions-continuous-delivery-azure/blob/master/.github/workflows/spinup-destroy.yml) defines the logic for configuring Azure resources.\nThis workflow has two jobs:\n- Set up Azure resources will run if the pull request contains a label with the name \"spin up environment\".\n- Destroy Azure resources will run if the pull request contains a label with the name \"destroy environment\".\n\nIn addition to each job, there's a few global environment variables:\n- AZURE_RESOURCE_GROUP, AZURE_APP_PLAN, and AZURE_WEBAPP_NAME are names for our resource group, app service plan, and web app, respectively, which we'll reference over multiple steps and workflows.\n- AZURE_LOCATION lets us specify the region for the data centers, where our app will ultimately be deployed.","tags":["GitHub","DevOps"]},{"title":"Build continuous integration (CI) workflows by using GitHub Actions","url":"/2020/07/20/Build-continuous-integration-CI-workflows-by-using-GitHub-Actions/","content":"\nGitHub Actions can be used to implement continuous integration (CI) for code that is maintained in GitHub repositories. \n- CI is the practice of using automation to build and test software every time a developer commits changes to version control. \n- CI helps teams discover issues early in the development process and fix them quickly.\n\n# What is CI?\n- CI can help you stick to your team’s quality standards by running tests and reporting the results on GitHub. \n- CI tools run builds and tests, **triggered by commits**. \n- The results post back to GitHub in the pull request. \n- This reduces context switching for developers, and improves consistency for testing.\n- The goal is fewer bugs in production and faster feedback while developing.\n\n\n# What are artifacts?\n\nWhen a workflow produces something other than a log entry, it's called an artifact. \n\nStoring an artifact helps to preserve it between jobs. Each job uses a fresh instance of a VM, so you can't reuse the artifact by saving it on the VM. If you need your artifact in a different job, you can upload the artifact to storage in one job, and download it for the other job.\n\n## Artifact storage\nArtifacts are stored in storage space on GitHub. The space is free for public repositories and some amount is free for private repositories, depending on the account. GitHub stores your artifact for 90 days.\n\n\nI took GitHub Learning lab: [GitHub Actions: Continuous Integration](https://lab.github.com/githubtraining/github-actions:-continuous-integration), which builds [this repository](https://github.com/niuniuanran/github-actions-for-ci/).\n\n# Use a templated workflow\nTemplated workflows are available in the [Actions tab](https://github.com/niuniuanran/github-actions-for-ci/actions). It will help you add a workflow file, for example, [.github/workflows/node.js.yml](https://github.com/niuniuanran/github-actions-for-ci/pull/3/commits/10f4732b4db4fc01a37c5e271a916f25fb30a51f#diff-3ac29962efd46b8d2008d26b9d9241a4)\n\n# Examining the workflow file\nThe [.github/workflows/node.js.yml](https://github.com/niuniuanran/github-actions-for-ci/pull/3/commits/10f4732b4db4fc01a37c5e271a916f25fb30a51f#diff-3ac29962efd46b8d2008d26b9d9241a4) defines the CI workflow.\n## on\nThe on: field is what tells GitHub Actions when to run. In this case, we're running the workflow anytime there's a push.\n```yaml\non:\n  push:\n    branches: [ master ]\n  pull_request:\n    branches: [ master ]\n```\n\n## jobs\nThe `jobs:` block defines the core component of an Actions workflow. **Workflows are made of jobs**, and our template workflow defines a single job with the identifier build.\n\nEvery job also needs a specific **host machine** on which to run, the `runs-on:` field is how we specify it. The template workflow is running the build job in the latest version of Ubuntu, a Linux-based operating system.\n\n The template workflow is running the `build:` job in the latest version of Ubuntu, a Linux-based operating system.\n\n```yaml\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        node-version: [10.x, 12.x, 14.x]\n\n    steps:\n    - uses: actions/checkout@v2 \n# actions/checkout@v2 is used to ensure our virtual machine has a copy of our codebase. The checked out code will be used to run tests against.\n    - name: Use Node.js ${{ matrix.node-version }}\n      uses: actions/setup-node@v1\n# actions/setup-node@v1 is used to set up proper versions of Node.js since we'll be performing testing against multiple versions.\n      with:\n        node-version: ${{ matrix.node-version }}\n    - run: npm ci\n# 'run:' In addition to running pre-built actions, the workflow can also execute commands, just as you would if you had direct access to the virtual machine. In this portion of the template workflow, we run some common commands relevant to Node.js projects, like npm install to install dependencies and npm test to run the chosen testing framework.\n    - run: npm run build --if-present\n    - run: npm test\n```\n\n## Accessing artifacts\nWith [this version of CI workflow](https://github.com/niuniuanran/github-actions-for-ci/pull/6/files#diff-3ac29962efd46b8d2008d26b9d9241a4), You may notice build succeeded, but each of the test jobs failed. That's because the build artifacts created in build aren't available to the test job. Each job executes in a fresh instance of the virtual environment. This is due to the design of the virtual environments themselves.\n\nTo solve the problem, first [add the step to upload artifact](https://github.com/niuniuanran/github-actions-for-ci/pull/6/files#diff-3ac29962efd46b8d2008d26b9d9241a4). \nThen the `test` job needs to start after `build` finishes (jobs run in parallel unless specified):\n```yaml\ntest:\n    needs: build\n    runs-on: ubuntu-latest\n```\n\nThe `test` job will download the artifact:\n```yaml\n...\n    steps:\n    - uses: actions/checkout@v2\n    - uses: actions/download-artifact@master\n      with: \n        name: webpack artifacts\n        path: public\n    - name: Use Node.js ${{ matrix.node-version }}\n      uses: actions/setup-node@v1\n      with:\n        node-version: ${{ matrix.node-version }}\n    - name: npm install, and test\n      run: |\n        npm install\n        npm test\n      env:\n        CI: true\n```\n\n# Adding an approval workflow\nThere can be multiple workflows for a repo. [.github/workflows/approval-workflow.yml](https://github.com/niuniuanran/github-actions-for-ci/blob/team-workflow/.github/workflows/approval-workflow.yml) uses a community-created action, [pullreminders/label-when-approved-action@master](https://github.com/abinoda/label-when-approved-action) to take care of labelling a pull request as `\"approved\"` when there have been 2 reviews.\n\n# Use branch protections\n\nIn repo settings, you can create branch protection rules for branches that match a certain name pattern.\nYou can reinforce reviews before merge, require status check and prevent branch deletion etc.","tags":["GitHub","DevOps"]},{"title":"Manage software delivery by using a release based workflow on GitHub","url":"/2020/07/20/Manage-software-delivery-by-using-a-release-based-workflow-on-GitHub/","content":"I was taking the [Microsoft Learn module](https://docs.microsoft.com/en-gb/learn/modules/release-based-workflow-github/): Manage software delivery by using a release based workflow on GitHub. I will Learn to implement a **release based workflow** on GitHub using **project boards, branches, and releases**.\n\n# What is a release based workflow?\nA release based workflow is a set of patterns and policies that focus on releasing software. \nA release based workflow drives three different parts of the release cycle: \n- managing the project, \n- selecting a branching strategy, \n- releasing to customers.\n\n# GitHub Project Boards\nIn GitHub, iterations are managed as **projects**.\nThe dominant feature of a project is its **board**. The board is the central plan of record for the iteration and contains all of the **cards** that are to be resolved. \nA card can represent an issue, a pull request, or even just a generic note.\nThe card's project status is integrated across the repository.\n\nBy using a project board, all stakeholders have an easy way to understand the status and velocity of a project. You can also create boards that are scoped to individual users or a collection of repositories owned by an organization.\n\n# Tracking specific milestones\nMilestones are similar to project tracking in that there is an emphasis on the prioritized completion of issues and pull requests. However, where a project may be focused on the team's process, **a milestone is focused on the product**.\n\n# Selecting a branching strategy\nThere are several strategies for managing releases. Some teams might use long-lived branches, like production, dev, and master. Some teams use simple feature branches, releasing from the master branch.\nNo one strategy is better than another. We always recommend **being intentional about branches and reducing long-lived branches whenever possible**.\n## Working with long-lived branches\nA long-lived branch is a Git branch that is never deleted. \neg:\n- master, release-v1.0, release-v2.0\n- master, dev, production\n\n## Servicing long-lived branches\n\nGit does offer an automated solution to apply certain updates into long-lived branches, without shoving all updates onto them, in the form of its cherry-pick command.\n\n## Protect release branches\nYou can protect release branches. This means you can protect branches from force pushes or accidental deletion. This is already configured in this repository.\n\n# Releasing to consumers\n\nWhen a product version is ready to be released, GitHub simplifies the process of packaging it up and notifying consumers.\n\n# Create a release based workflow: GitHub Learning Lab\n\n[This repository](https://github.com/niuniuanran/release-based-workflow) contains my learning with GitHub Learning Lab.\n\n## GitHub Releases\n\nGitHub Releases point to a specific commit. Releases can include release notes in Markdown, and attached binaries.\n\n# Pull Request shipping from release branch to master\nYou should open a pull request between your release branch and master as early as possible. It might be open for a long time, and that's okay. The pull request corresponds to the work in the project board.\n\nThe pull request description should:\n- Include a brief description of the pull request\n- Include a task list of expected features\n- Propose a ship date\n\n[This](https://github.com/niuniuanran/release-based-workflow/pull/6) is the long-lived pull request created.\nShort-living feature pull requests that have been merged into the `release-v1.0` branch will be visible in the long-living, `release-v1.0 -> master` pull request. \n\nUsually, you delete a branch after merging. The release branch is protected, so it cannot be deleted when you merge this pull request.\n\n# Release Drafter\n[Release Drafter](https://github.com/apps/release-drafter) drafts your next release notes as pull requests are merged into master. It is configured with [.github/release-drafter.yml](https://github.com/niuniuanran/release-based-workflow/blob/master/.github/release-drafter.yml).\n\n<img src=\"drafted-release.png\" alt=\"\" width=\"700px\"/>\n\n# First Release\n\n<img src=\"first-release.png\" alt=\"\" width=\"800px\"/>\n\n[This](https://github.com/niuniuanran/release-based-workflow/releases/tag/v1.0.0) is the first official release. As can been seen, the source code is provided as `.zip` and `tar.gz` packages. I can also choose to upload binary code to the release.\n\n\n\n","tags":["GitHub","DevOps"]},{"title":"Webpack Tree Shaking","url":"/2020/07/20/Webpack-Tree-Shaking/","content":"\n## Intention\n\n- Dead-code elimination\n- Bundle code written in ECMAScript dialects\n\n## Rationale\n- For dynamic languages, all possible execution flows of a program can be represented as a tree of function calls.\n- The popularity of tree shaking in JavaScript is based on the fact that in distinction from CommonJS modules, ECMAScript 6 module loading is *static* and thus the whole dependency tree can be *deduced by statically parsing the syntax tree*.\n\n> In JavaScript, you have to add an entire library even if you only need it for one function, but thanks to tree shaking the Dart-derived JavaScript only includes the individual functions that you need from a library.    -- Chris Buckett\n","tags":["Web Development"]},{"title":"C sharp ref keyword","url":"/2020/07/20/C-sharp-ref-keyword/","content":"The ref keyword indicates a value that is passed by reference. It is used in four different contexts:\n\n- In a method signature and in a method call, to pass an argument to a method by reference. \n- In a method signature, to return a value to the caller by reference. \n- In a member body, to indicate that a reference return value is stored locally as a reference that the caller intends to modify or, in general, a local variable accesses another value by reference. \n- In a struct declaration to declare a ref struct or a readonly ref struct. \n\n# Passing an argument by reference\n\nWhen used in a method's parameter list, the ref keyword indicates that an argument is passed by reference, not by value. The ref keyword **makes the formal parameter an alias for the argument**, which **must be a variable**. \n\n```\nvoid Method(ref int refArgument)\n   {\n       refArgument = refArgument + 44;\n   }\n   \n   int number = 1;\n   Method(ref number);\n   Console.WriteLine(number);\n   // Output: 45 \n```\n\n# Reference return values\n\n```\npublic static ref int Find(int[,] matrix, Func<int, bool> predicate)\n{\n    for (int i = 0; i < matrix.GetLength(0); i++)\n        for (int j = 0; j < matrix.GetLength(1); j++)\n            if (predicate(matrix[i, j]))\n                return ref matrix[i, j];\n    throw new InvalidOperationException(\"Not found\");\n}\n```\n\n# Ref locals\nA ref local variable is used to refer to values returned using `return ref`.\n","tags":["C#"]},{"title":"AWS Cloud Architecturing Knowledge Check notes","url":"/2020/07/20/AWS-Cloud-Architecturing-Knowledge-Check-notes/","content":"\nHere are the questions from the AWS Cloud Architecting Knowledge check that I find worth noting.\n\n# 1\n<img src=\"access-troubleshoot.png\" width=\"700px\" alt=\"\"/>\n\n# 2\n<img src=\"auto-deployment.png\" width=\"700px\" alt=\"\"/>\n\nResource deployment and configuration automation\n## Amazon EC2 Run Command\n[AWS Systems Manager Run Command](https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html) lets you remotely and securely manage the configuration of your managed instances. A managed instance is any EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. \n\nRun Command enables you to **automate** common administrative tasks and perform ad hoc configuration changes at scale. \n\n## AWS Systems Manager\n\n[AWS Systems Manager](https://aws.amazon.com/systems-manager/) gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. \n\nSystems Manager simplifies resource and application management, shortens the time to detect and resolve operational problems, and makes it easy to operate and manage your infrastructure securely at scale.\n\n## AWS OpsWorks\n[AWS OpsWorks](https://aws.amazon.com/opsworks/) is a configuration management service that provides managed instances of **Chef and Puppet**. Chef and Puppet are **automation platforms** that allow you to **use code to automate the configurations of your servers**. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. \n\n# 3\n\n<img src=\"automate-test.png\" alt=\"\" width=\"800px\"/>\n\n# 4\n<img src=\"automatic-instance-placement.png\" alt=\"\" width=\"800px\"/>\n\n## Dedicated Host\n\nTo use a Dedicated Host, you first **allocate hosts** for use in your account. You then launch instances onto the hosts by specifying **host tenancy** for the instance. You must select a specific host for the instance to launch on to, or you can allow it to launch on to any host that has **auto-placement enabled** and matches its **instance type**. \n\nIf you no longer need an On-Demand host, you can stop the instances running on the host, direct them to launch on a different host, and then release the host.\n\nDedicated Hosts are also integrated with AWS License Manager. With License Manager, you can create a host resource group, which is a collection of Dedicated Hosts that are managed as a single entity. \n\n### Auto-Placement\nAuto-placement is configured at the host level. It allows you to manage whether instances that you launch are launched onto a specific host, or onto any available host that has matching configurations.\n\nWhen the auto-placement of a Dedicated Host is disabled, it only accepts Host tenancy instance launches that specify its unique **host ID**. This is the default setting for new Dedicated Hosts.\n\n### Host affinity\nHost affinity is configured at the instance level. It establishes a launch relationship between an instance and a Dedicated Host.\n    \n**When affinity is set to Host, an instance launched onto a specific host always restarts on the same host if stopped**. This applies to both targeted and untargeted launches.\n    \nWhen affinity is set to Off, and you stop and restart the instance, it can be restarted on any available host. However, it tries to launch back onto the last Dedicated Host on which it ran (on a best-effort basis).\n\n## Dedicated Instance\nDedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on **hardware that's dedicated to a single customer**. Dedicated Instances that belong to different AWS accounts are **physically isolated** at a hardware level, even if those accounts are linked to a single payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.\n\nYou can change the tenancy of an instance from `dedicated`(Dedicated Instance) to `host`(Dedicated Host), or from host to dedicated after you've launched it.\n\nIf you launch an instance into **a VPC that has an instance tenancy of dedicated**, your instance is automatically a Dedicated Instance, regardless of the tenancy of the instance.\n\n## A Comparison of Dedicated Host vs Dedicated Instance\n\nThe chart is found [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html).\n\n<img src=\"didicated-vs-host.png\" alt=\"\" width=\"700px\"/>\n\n# 4\n\n<img src=\"automation.png\" alt=\"\" width=\"700px\"/>\n\n# 5\n\n<img src=\"aws-config.png\" alt=\"\" width=\"700px\"/>\n\n## AWS Config\n[AWS Config](https://aws.amazon.com/config/) is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously **monitors and records your AWS resource configurations** and allows you to **automate the evaluation of recorded configurations against desired configurations**. \n\nWith Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines.\n\n## AWS Config Rule\nAn AWS Config rule represents an **AWS Lambda function** that you create for a custom rule or a predefined function for an AWS managed rule. The function **evaluates configuration items** to **assess** whether your AWS resources comply with your desired configurations. \nThis function can run **when AWS Config detects a configuration change to an AWS resource** and **at a periodic frequency** that you choose (for example, every 24 hours).\n\n# 6\n\n<img src=\"caching.png\" alt=\"\" width=\"700px\"/>\n\nOffload popular traffic to fully managed services like CloudFront or S3 => Less IaaS to maintain, scale and pay for\n\n# 7\n\n<img src=\"change-management.png\" alt=\"\" width=\"700px\"/>\n\n# 8\n\n<img src=\"cloud-formation-parameter.png\" alt=\"\" width=\"700px\"/>\n\n# 9\n\n<img alt=\"\" src=\"cloudformation-parameters.png\" width=\"700px\"/>\n\n## Parameters\n\n[AWS CloudFormation Parameters](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html#gettingstarted.templatebasics.parameters)\nYou declare parameters in a template's Parameters object. A parameter contains a list of attributes that define its value and constraints against its value. The only required attribute is Type, which can be String, Number, or an AWS-specific type. You can also add a Description attribute that tells a user more about what kind of value they should specify. \nIt contains a list of attributes that can define default values and allowed values.\n\n# 10\n\n<img alt=\"\" src=\"Cloudformation-dependson.png\" width=\"700px\"/>\n\n## DependsOn Attribute\nWith [DependsOn](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-dependson.html)\nattribute you specify that the creation of a specific resource follows another. When you add a DependsOn attribute to a resource, that resource is created **only after the creation of the resource specified in the DependsOn attribute**.\n\n# 11\n\n<img alt=\"\" src=\"cloudformation-mapping.png\" width=\"700px\"/>\n\n## CloudFormation Mappings\nThe optional [Mappings](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html) section **matches a key to a corresponding set of named values**. \nFor example, if you want to set values based on a region, you can create a mapping that uses **the region name as a key** and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map.\n\nYou **cannot** include parameters, pseudo parameters, or intrinsic functions in the Mappings section.\n\n## Conditions\nThe optional [Conditions](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html) section contains statements that define the circumstances under which entities are created or configured. \nFor example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true. \n\n\n# 12\n\n<img alt=\"\" src=\"cf-distribution.png\" width=\"700px\"/>\n\n## CloudFront distribution\nWhen you create a web distribution, CloudFront assigns a domain name to the distribution, such as d111111abcdef8.cloudfront.net. You can use this domain name in the URLs for your content.\n\n\n# 13\n\n<img alt=\"\" src=\"concierge.png\" width=\"700px\"/>\n\n# 14\n\n<img alt=\"\" src=\"config.png\" width=\"700px\"/>\n\n# 15\n\n<img alt=\"\" src=\"config-inventory.png\" width=\"700px\"/>\n\n## Looking up resources through Config\nYou can use the AWS Config console, AWS CLI, and AWS Config API to look up the resources that AWS Config has taken an **inventory** of, or **discovered**, including deleted resources and resources that AWS Config is not currently recording. \nSee [here](https://docs.aws.amazon.com/config/latest/developerguide/looking-up-discovered-resources.html)\n\n\n# 16\n\n<img alt=\"\" src=\"consumption-model.png\" width=\"700px\"/>\n\n## Adopt a consumption model\nOne of [Cost Optimisation Design Principles](https://wa.aws.amazon.com/wat.pillar.costOptimization.en.html).\nPay only for the computing resources that you require and increase or decrease usage depending on business requirements, not by using elaborate forecasting. \nFor example, development and test environments are typically only used for eight hours a day during the work week. You can stop these resources when they are not in use for a potential cost savings of 75% (40 hours versus 168 hours).\n\n# 17\n\n<img alt=\"\" src=\"db-consideration.png\" width=\"700px\"/>\n\n[Considerations when selecting a database solution](https://wa.aws.amazon.com/wat.question.PERF_4.en.html) include requirements for **availability, consistency, partition tolerance, latency, durability, scalability, and query capability**. \n\n# 18\n\n<img alt=\"\" src=\"dead-letter-queue.png\" width=\"700px\"/>\n\n# 19\n\n<img alt=\"\" src=\"dead-letter-queue-benefit.png\" width=\"700px\"/>\n\n## Dead letter queue\nAmazon SQS supports [dead-letter queue](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html), which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. \n\n# 20\n\n<img alt=\"\" src=\"decoupling.png\" width=\"700px\"/>\n\n# 21\n\n<img alt=\"\" src=\"distribution.png\" width=\"700px\"/>\n\n# 22\n\n<img alt=\"\" src=\"dynamodb-concurrent.png\" width=\"700px\"/>\n\n## DynamoDB Consistency and Conflict Resolution\n\nAny changes made to any item in any replica table are **replicated to all the other replicas** within the same global table. In a global table, a newly written item is usually propagated to all replica tables within seconds.\n\nWith a global table, each replica table stores the same set of data items. DynamoDB does not support partial replication of only some of the items.\n\n- An application can read and write data to any replica table. If your application only uses eventually consistent reads and only issues reads against one AWS Region, it will work without any modification. \n- If your application requires strongly consistent reads, it must perform all of its strongly consistent reads and writes in the same Region. \n\n**DynamoDB does not support strongly consistent reads across Regions.**\nTherefore, **if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region**.\n\n**Conflicts can arise if applications update the same item in different Regions at about the same time**. To help ensure eventual consistency, DynamoDB global tables **use a last writer wins reconciliation between concurrent updates**, in which DynamoDB makes a best effort to determine the last writer. \nWith this conflict resolution mechanism, all the replicas will agree on the **latest update** and converge toward a state in which they all have identical data.\n\n# 23\n\n<img alt=\"\" src=\"dynamodb-model.png\" width=\"700px\"/>\n\n# 24\n\n<img alt=\"\" src=\"dynamodb-performance.png\" width=\"700px\"/>\n\n**Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale.**\n\n# 25\n\n<img alt=\"\" src=\"ec2-type-choice.png\" width=\"700px\"/>\n\n# 26\n\n<img alt=\"\" src=\"elasti-cache.png\" width=\"700px\"/>\n\n## Monitor ElastiCache\n\nThe following CloudWatch metrics offer good insight into ElastiCache performance. In most cases, we recommend that you set CloudWatch alarms for these metrics so that you can take corrective action before performance issues occur:\n\n- CPUUtilization\n- EngineCPUUtilization\n- SwapUsage\n- Evictions\n- CurrConnections\n\nSee [here](https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.WhichShouldIMonitor.html) for details.\n\n# 27\n\n<img alt=\"\" src=\"emr.png\" width=\"700px\"/>\n\n## What does MapReduce do \nAmazon EMR uses Apache Hadoop as its **distributed data processing engine**. Hadoop is an open source, Java software framework that supports data-intensive distributed applications running on large clusters of commodity hardware. Hadoop implements a programming model named “MapReduce,” where **the data is divided into many small fragments of work, each of which may be executed on any node in the cluster**. \n\nInstead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to **analyze massive datasets in parallel** more quickly.\n\n# 28\n\n<img alt=\"\" src=\"evolve.png\" width=\"700px\"/>\n\n# 29\n\n<img alt=\"\" src=\"floating-ip.png\" width=\"700px\"/>\n\n## Floating IP\nA Floating IP is an IP address that can be instantly moved from one Droplet to another Droplet in the same datacenter. \n\n# 30 \n\n<img alt=\"\" src=\"ha-ec2.png\" width=\"700px\"/>\n\n# 31\n\n<img alt=\"\" src=\"infrastructure-event-management.png\" width=\"700px\"/>\n\n## AWS Infrastructure Event Management\n\n[AWS Infrastructure Event Management](https://aws.amazon.com/premiumsupport/programs/iem/) is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you **plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events**.\n\n## Trusted Advisor\n\n[Trusted Advisor](https://aws.amazon.com/premiumsupport/technology/trusted-advisor/) is an online tool that provides you real time guidance to help you provision your resources following AWS best practices. Trusted Advisor checks help optimize your AWS infrastructure, increase security and performance, reduce your overall costs, and monitor service limits. \n\n**AWS Business Support** and **AWS Enterprise Support** customers get access to all 115 Trusted Advisor checks (14 cost optimization, 17 security, 24 fault tolerance, 10 performance, and 50 service limits) and recommendations.\n\n# 32\n\n<img alt=\"\" src=\"inspector.png\" width=\"700px\"/>\n\n## Amazon Inspector\n- [Amazon Inspector](https://aws.amazon.com/inspector/) is an **automated security assessment service** that helps improve the security and compliance of applications deployed on AWS. \n- Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. \n- The AWS security organization is continuously assessing the AWS environment and updating a **knowledge base of security best practices and rules**. Amazon Inspector makes this expertise available to you in the form of a service that simplifies the process of establishing and enforcing best practices within your AWS environment.\n\n# 33\n\n<img alt=\"\" src=\"inspector-rules-packages.png\" width=\"700px\"/>\n\n## Amazon Inspector assessments\n\nAmazon Inspector assessments are offered to you as pre-defined **rules packages** mapped to common security best practices and vulnerability definitions.\n\n# 34\n\n<img alt=\"\" src=\"io-troubleshoot.png\" width=\"700px\"/>\n\n# 35\n\n<img alt=\"\" src=\"kinesis-data-stream.png\" width=\"700px\"/>\n\n# 36\n\n<img alt=\"\" src=\"kinesis-data-stream-use-case.png\" width=\"700px\"/>\n\n# 37\n\n<img alt=\"\" src=\"lambda-performance.png\" width=\"700px\"/>\n\n# 38\n\n<img alt=\"\" src=\"Metris.png\" width=\"700px\"/>\n\n## Metrics\n\nMetrics are uniquely defined by \n- a **name**, \n- a **namespace**, and \n- **zero or more dimensions**. \n\nEach **data point** in a metric has \n- a time stamp, \n- and (optionally) a unit of measure. \n\nYou can retrieve statistics from CloudWatch for any metric.\n\n## Statistics\n- Statistics are metric data aggregations over specified periods of time. \n- CloudWatch provides statistics based on the metric data points provided by your custom data or provided by other AWS services to CloudWatch. \n- Aggregations are made using the:\n    - namespace, \n    - metric name, \n    - dimensions, \n    - the data point unit of measure, \n    - within the time period you specify.\n\n# 39\n\n<img alt=\"\" src=\"middleware.png\" width=\"700px\"/>\n\n<img alt=\"\" src=\"middleware-careful.png\" width=\"400px\"/>\n\n# 40\n\n<img alt=\"\" src=\"monitor-reliability-issues.png\" width=\"700px\"/>\n\n# 41\n\n<img alt=\"\" src=\"op-excellence-priority.png\" width=\"700px\"/>\n\n# 42\n\n<img alt=\"\" src=\"operational-excellence.png\" width=\"700px\"/>\n\nSee this table from [AWS Well-Architectured Whitepaper](https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf):\n\n<img alt=\"\" src=\"wa-framework.png\" width=\"600px\"/>\n\n# 43\n\n<img alt=\"\" src=\"ops-ex-standards.png\" width=\"700px\"/>\n\n# 44\n\n<img alt=\"\" src=\"ops-exc.png\" width=\"700px\"/>\n\n# 45\n\n<img alt=\"\" src=\"ops-excellence-books.png\" width=\"700px\"/>\n\n## Use runbooks to perform procedures\n- [Runbooks](https://wa.aws.amazon.com/wat.concept.runbook.en.html) are documented procedures to achieve specific outcomes. \n- Start with a valid effective manual process, implement it in code and trigger automated execution where appropriate.\n- Consistent and prompt responses to **well understood events**.\n\n## Use playbooks to investigate issues\n- [Playbooks](https://wa.aws.amazon.com/wat.concept.playbook.en.html) are the predefined steps to perform to **identify** an issue. \n- Enable consistent and prompt responses to **issues that are not well understood**. \n\n## Use checklist to prepare\nYou should use a consistent process (including manual or automated checklists) to know when you are ready to go live with your workload or a change.\n\n# 46\n\n<img alt=\"\" src=\"OpsWork.png\" width=\"700px\"/>\n\nOpsWorks has three offerings, \n- AWS Opsworks for Chef Automate,\n- AWS OpsWorks for Puppet Enterprise, and \n- AWS OpsWorks Stacks.\n\n# 47\n\n<img alt=\"\" src=\"per-second-billing.png\" width=\"700px\"/> - very possibly outdated.\n\n## Per-second billing\n\nEC2 usage are billed on one second increments, with a minimum of 60 seconds. Similarly, provisioned storage for EBS volumes will be billed per-second increments, with a 60 second minimum. Per-second billing is available for instances launched in:\n\n- On-Demand, **Reserved** and Spot forms\n- All regions and Availability Zones\n- Amazon Linux and Ubuntu\n\n[This article](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html) explains how reserved instances are billed.\n\n# 48 \n\n<img alt=\"\" src=\"perform-monitor.png\" width=\"700px\"/>\n\n# 49\n\n<img alt=\"\" src=\"prepare.png\" width=\"700px\"/>\n\n## Operational Excellence\n- Organization\n    - OPS 1: How do you determine what your priorities are?\n    - OPS 2: How do you structure your organization to support your business outcomes?\n    - OPS 3: How does your organizational culture support your business outcomes?\n- Prepare   \n    - OPS 4: How do you design your workload so that you can understand its state?\n    - OPS 5: How do you reduce defects, ease remediation, and improve flow into production?\n    - OPS 6: How do you mitigate deployment risks?\n    - OPS 7: How do you know that you are ready to support a workload?\n- Operate\n    - OPS 8: How do you understand the health of your workload?\n    - OPS 9: How do you understand the health of your operations?\n    - OPS 10: How do you manage workload and operations events?\n- Evolve\n    - OPS 11: How do you evolve operations?\n\n# 50\n\n<img alt=\"\" src=\"protect-data.png\" width=\"700px\"/>\n\n# 60\n\n<img alt=\"\" src=\"provisioned-iops-ssd.png\" width=\"700px\"/>\n\n# 61\n\n<img alt=\"\" src=\"rdp-port.png\" width=\"700px\"/>\n\n# 62\n\n<img alt=\"\" src=\"reserve.png\" width=\"700px\"/>\n\n[Amazon ElastiCache Reserved Nodes](https://aws.amazon.com/elasticache/reserved-cache-nodes/) give you the option to make a low, one-time payment for each cache node you want to reserve and in turn receive a significant discount on the hourly charge for that Node.\n\n# 63\n\n<img alt=\"\" src=\"resource-based.png\" width=\"700px\"/>\n\n# 64\n\n<img alt=\"\" src=\"routes.png\" width=\"700px\"/>\n\n# 65\n\n<img alt=\"\" src=\"runbooks.png\" width=\"700px\"/>\n\n# 66\n\n<img alt=\"\" src=\"security.png\" width=\"700px\"/>\n\n# 67\n\n<img alt=\"\" src=\"serverless.png\" width=\"700px\"/>\n\n##  Design Principles for performance efficiency\n- Democratize advanced technologies: Make advanced technology implementation easier for your team by delegating complex tasks to your cloud vendor. Rather than asking your IT team to learn about hosting and running a new technology, consider consuming the technology as a service. \n- Go global in minutes: Deploying your workload in multiple AWS Regions around the world allows you to provide lower latency and a better experience for your customers at minimal cost.\n- Use serverless architectures: Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities.\n- Experiment more often: With virtual and automatable resources, you can quickly carry out **comparative testing** using different types of instances, storage, or configurations.\n- Consider mechanical sympathy: Understand how cloud services are consumed and always use the technology approach that aligns best with your workload goals. For example, consider data access patterns when you select database or storage approaches.\n\n# 68 \n\n<img alt=\"\" src=\"sni.png\" width=\"700px\"/>\n\n## Amazon CloudFront Custom SSL\n\nAmazon CloudFront gives you three options for accelerating your entire website while delivering your content **securely over HTTPS** from all of CloudFront's edge locations.\n\n### Default: CloudFront distribution domain name\nBy default you can deliver your content to viewers over HTTPS by using your CloudFront distribution domain name in your URLs, for example, https://dxxxxx.cloudfront.net/image.jpg.\n\nIf you want to deliver your content over HTTPS using your **own domain name and your own SSL certificate**, you can use one of our Custom SSL certificate support features.\n\n### SNI Custom SSL\n    \nServer Name Indication (SNI) Custom SSL relies on the **SNI extension of the Transport Layer Security protocol**, which allows multiple domains to serve SSL traffic over the same IP address. Amazon CloudFront delivers your content from each edge location and offers the same security as the Dedicated IP Custom SSL feature (see below).\n\n### Dedicated IP Custom SSL\n    \nIf you need to deliver content to browsers that don’t support SNI, you can use the **Dedicated IP Custom SSL feature**. For this feature the Amazon content delivery network **allocates dedicated IP addresses to serve your SSL content at each Edge location**.\n\n# 69\n\n<img alt=\"\" src=\"Prepare-procedures.png\" width=\"700px\"/>\n\nCheck that required procedures are adequately captured in runbooks and playbooks is a best practice for **Prepare**.\n\n# 70\n\n<img alt=\"\" src=\"rto-rpo.png\" width=\"700px\"/>\n\nHighest objective means shortest time.\n\n# 71\n\n<img alt=\"\" src=\"SOA.png\" width=\"700px\"/>\n\n# 72\n\n<img alt=\"\" src=\"spot-instance.png\" width=\"700px\"/>\n\n# 73\n\n<img alt=\"\" src=\"sqs.png\" width=\"700px\"/>\n\n# 74\n\n<img alt=\"\" src=\"visibility-time-out-sqs.png\" width=\"700px\"/>\n\n## Amazon SQS visibility timeout\n\n<img src=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-visibility-timeout-diagram.png\" alt=\"\" width=\"650px\"/>\n\nWhen a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must **delete the message from the queue after receiving and processing it**.\n\nImmediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a *[visibility timeout](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\n)*, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. \n\n# 75\n\n<img alt=\"\" src=\"vpc-endpoint.png\" width=\"700px\"/>\n\n## VPC Endpoint\nA [VPC endpoint](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html) enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. \n\n- Virtual devices. \n- Horizontally scaled, redundant, and highly available VPC components. \n- Allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.\n\n# 76\n\n<img alt=\"\" src=\"well-arch.png\" width=\"700px\"/>\n\n- AWS Well-Architected helps cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications and workloads. \n- AWS Well-Architected provides a consistent approach for customers and partners to evaluate architectures, and implement designs that can scale over time.","tags":["AWS","AWS SAA Test"]},{"title":"Creating Web UI with Razor Pages","url":"/2020/07/19/Creating-Web-UI-with-Razor-Pages/","content":"\nI was learning the Microsoft Learn Module - [Create a web UI with ASP.NET Core](https://docs.microsoft.com/en-gb/learn/modules/create-razor-pages-aspnet-core/).\n\nRazor Pages is a server-side, page-centric programming model for building rich web UI with ASP.NET Core. Razor Pages makes it easy to get started building dynamic web applications when all you need is to define UI logic using a combination of HTML, CSS, and C#.\n\n**Razor pages have the extension `.cshtml`**. Razor syntax is a combination of HTML and C# where the C# code defines the dynamic rendering logic for the page.\n\nIn a webpage that uses the Razor syntax, there can be two kinds of content: client content and server code:\n\n- Client content: Contains what you're used to in webpages: HTML markup (elements), style information such as CSS, maybe some client script such as JavaScript, and plain text.\n- Server code: Razor syntax lets you add server code to your client content. **If there is server code in the page, the server runs that code first, before it sends the page to the browser.** From the browser's perspective, client content that's generated by your server code is no different than any other client content.\n\n# Razor Pages\n\nEach Razor page is a pair of files:\n\n- A .cshtml file that contains markup with C# code using Razor syntax.\n- A .cshtml.cs PageModel class file that defines:\n  - Page handlers for requests sent to the page.\n  - Data used to render the page.\n\n# PageModel (.cshtml.cs)\n\nA model object defines data properties and encapsulates logic or operations related to those data properties. A PageModel is essentially the same thing, but is a model that **more specifically encapsulates the data properties and logic operations scoped just to its Razor page**.\n\n# DataAnnotations\n\nModels in ASP.NET Core often make use of data annotations to **constrain or customize model properties**.\nData annotations are attributes used to specify behavior that you want to enforce on the model properties to which they're applied.\nFor example, a range of minimum and maximum acceptable values.\n\n# The Pages directory structure and routing requests\n\nRazor Pages uses the directory structure within the Pages directory as the convention for **routing requests** by default. An index page located in the root of the Pages directory, for example, is the default page for the app's site.\n\n# Razor Syntax\n\nRazor syntax uses the `@` symbol to transition from HTML to C#.\n\n- If the @ symbol is followed by a Razor reserved keyword, it transitions into Razor-specific markup;\n- otherwise, it transitions to C#. Razor evaluates the C# expressions and renders them in the HTML output.\n\n## Reserved Razor keywords:\n\n### `@page`\n\nThe @page directive is what makes this a Razor page. It indicates this page can handle HTTP requests. The @page directive must be the first directive on a Razor page.\n\n### `@model`\n\nThe @model directive is Razor syntax specifying the type of the model made available to the Razor page.\n\n## C# code\n\nThe `@` character starts single-statement C# blocks. Multi-statement C# blocks can be created when using @{}. For example:\n\n```\n@{\n\tvar pageTitle = \"Home page\";\n\tViewData[\"Title\"] = pageTitle;\n}\n```\n\n# Razor Tag Helpers\n\nHere's the example `Pages/Products/Create.cshtml` file:\n\n```\n@page\n   @model ContosoPets.Ui.Pages.Products.CreateModel\n   @{\n       ViewData[\"Title\"] = \"Create\";\n   }\n\n   <h1>Create Product</h1>\n\n   <form method=\"post\">\n       <div class=\"form-group\">\n           <label asp-for=\"Product.Name\" class=\"control-label\"></label>\n           <input asp-for=\"Product.Name\" class=\"form-control\" />\n           <span asp-validation-for=\"Product.Name\" class=\"text-danger\"></span>\n       </div>\n       <div class=\"form-group\">\n           <label asp-for=\"Product.Price\" class=\"control-label\"></label>\n           <input asp-for=\"Product.Price\" class=\"form-control\" />\n           <span asp-validation-for=\"Product.Price\" class=\"text-danger\"></span>\n       </div>\n       <div class=\"form-group\">\n           <input type=\"submit\" value=\"Save\" class=\"btn btn-primary\" />\n       </div>\n   </form>\n```\n\nTag Helpers are components for automating HTML generation in ASP.NET Core web applications. By design, they're reusable, customizable building blocks that save you time. Most built-in Tag Helpers extend standard HTML elements you're familiar with and provide additional **server-side attributes** for the element, making them more robust.\n\n## The Label Tag Helper\n\n`<label asp-for=\"Product.Name\" class=\"control-label\"></label>`\nThe Label Tag Helper extends the standard HTML element. As is common for many Razor Tag Helpers, it uses an asp-for attribute which takes a specified PageModel property. The PageModel property is defined in C#. In this case, the value of the PageModel Name property will be rendered as the content for an HTML <label> element. The `asp-for` attribute is scoped to the PageModel for the Razor page, so the @ symbol isn't used. The label is dynamic as is needed here, but remains compact and easy to add in your markup.\n\n## The Input Tag Helper\n\nIn `Create.cshtml`:\n`<input asp-for=\"Product.Name\" class=\"form-control\" />`\nGenerated output:\n`<input name=\"Product.Name\" class=\"form-control\" id=\"Product_Name\" type=\"text\" value=\"\" data-val-required=\"The Name field is required.\" data-val=\"true\">`\n\n- It evaluates the Product.Name property defined in the PageModel in C#, adds an id and name based on that property and sets the input type appropriately.\n- It provides client-side validation using JQuery **based on the model's data annotation attributes provided through the PageModel**.\n\n## The Validation Message Tag Helper\n\nThe following markup uses the Validation Message Tag Helper. It displays a validation message for a single property on your model.\n\n```\n<span asp-validation-for=\"Product.Price\" class=\"text-danger\"></span>\n```\n\nGenerated output:\n\n```html\n<input\n  name=\"Product.Price\"\n  class=\"form-control\"\n  id=\"Product_Price\"\n  type=\"text\"\n  value=\"\"\n  data-val-required=\"The Price field is required.\"\n  data-val=\"true\"\n  data-val-range-min=\"0.01\"\n  data-val-range-max=\"9999.99\"\n  data-val-range=\"The field Price must be between 0.01 and 9999.99.\"\n  data-val-number=\"The field Price must be a number.\"\n/>\n```\n\nThe type, data-val-range-min, data-val-range-max and error response are dynamically set by the model's data annotations for the model's Product.Price property.\n\n# PageModel\n\nA Razor Page's PageModel class file defines any page handlers for requests sent to the page, and data used to render the page.\n\n```c#\nusing ContosoPets.Ui.Models;\nusing ContosoPets.Ui.Services;\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.AspNetCore.Mvc.RazorPages;\nusing System.Threading.Tasks;\n\nnamespace ContosoPets.Ui.Pages.Products\n{\n    public class CreateModel : PageModel\n    {\n        private readonly ProductService _productService;\n\n        [BindProperty]\n        public Product Product { get; set; }\n\n        public CreateModel(ProductService productService)\n// Injecting the ContosoPets.UI ProductService service that handles HTTP requests\n        {\n            _productService = productService;\n        }\n\n        public async Task<IActionResult> OnPostAsync()\n        {\n            if (!ModelState.IsValid)\n            {\n                return Page();\n            } // Built-in server-side model validation using ASP.NET Core data annotations\n\n            await _productService.CreateProduct(Product);\n\n            return RedirectToPage(\"Index\");\n        }\n    }\n}\n```\n\n# Typed HTTPClient service architecture\n[Typed HTTPClient service architecture](https://docs.microsoft.com/en-gb/learn/modules/create-razor-pages-aspnet-core/7-lifecycle-httpclient) is responsible for managing HTTP requests to the web API. \n**Provided as a typed service you have the advantage of injecting it as a constructor parameter directly into the PageModel classes in this project.**\nUsing this architecture provides the advantage of letting the framework take on the responsibility of creating an instance of the HttpClient class and disposing of it when it's no longer needed. This is a great feature for a project such as this one which will make use of HttpClient instances for each CRUD operation.\n","tags":["Web Development","ASP.NET"]},{"title":"Automate GitHub by using GitHub Script","url":"/2020/07/18/Automate-GitHub-by-using-GitHub-Script/","content":"I was taking the Microsoft Learn [Automate GitHub by using GitHub Script](https://docs.microsoft.com/en-gb/learn/modules/automate-github-using-github-script/) module. \n\nI took the GitHub learning lab[GitHub Actions: Using GitHub Script](https://lab.github.com/githubtraining/github-actions:-using-github-script), of which the learning happens at [this repo](https://github.com/niuniuanran/write-github-script/).\n\n# Intro\nThe main difference between GitHub Action and octokit in usage is that GitHub Script provides you with a pre-authenticated octokit/rest.js client named github.\n\n[GitHub Script](https://github.com/actions/github-script) enables you to automate common GitHub processes using GitHub Actions workflows.\nThis allows you to use the workflow context to script any API usage that is available through the octokit/rest.js library without the need to build a bulky custom action to do so. See [octokit/rest.js](https://octokit.github.io/rest.js/v18) for the API client documentation.\n\n# Workflow\n\nA workflow file can be thought of as the recipe for automating a task. They house the start to finish instructions, in the form of jobs and steps, for what should happen based on specific triggers.\n \nYour repository can contain multiple workflow files that carry out a wide variety of tasks. It is important to consider this when deciding on a name for your workflow. The name you choose should reflect the tasks being performed.\n\n# GitHub Action\n\nWorkflows is defined in the [`/.github/workflows`](https://github.com/niuniuanran/write-github-script/tree/master/.github/workflows) directory in yaml format. [This](https://github.com/niuniuanran/write-github-script/blob/master/.github/workflows/my-workflow.yml) is the first workflow file I created. \nIt is triggered by new issue, and GitHub Action bot will add a comment to the issues.\n\n```yaml\nname: Learning GitHub Script\n\non:\n  issues:\n    types: [opened]\n\njobs:\n  comment:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/github-script@0.8.0\n        with:\n          github-token: ${{secrets.GITHUB_TOKEN}}\n          script: |\n            github.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: \"🎉 You've created this issue comment using GitHub Script!!!\"\n            })\n```\n\n<img src=\"github-action.png\" alt=\"\" width=\"700px\"/>\n\nJob automated!\n\n# Automatically move issues into a GitHub Project board\n\nThe GitHub Learning Lab guided me to [update the workflow to add a step, adding any newly opened issue to a specified project dashboard column as a card](https://github.com/niuniuanran/write-github-script/pull/4/commits/0926d99f98d5f773adaa6f407ce15f84a5357758).\n\nHere's the outcome:\n\n<img src=\"move-card.png\" alt=\"\" width=\"700px\"/>\n\nHere's the [dashboard](https://github.com/niuniuanran/write-github-script/projects/1) that GitHub Action adds the new issue into.\n\n# Use the FS module to use a templated comment\n\nIn [this unit](https://github.com/niuniuanran/write-github-script/pull/6) of GitHub Learning Lab:\n\n- GitHub Script also grants you access to a full Node.js environment.\n- Instead of writing the guide directly into our workflow, we can use the Node.js File System module to read a file and use it as the body of our issue comment.\n\nThe updated my-workflow file contains instructions to load comment from [.github/ISSUE_RESPONSES/comment.md](https://github.com/niuniuanran/write-github-script/blob/master/.github/ISSUE_RESPONSES/comment.md):\n\nThe fs module of Node.js is used (`readFileSync`) to load the content of `.github/ISSUE_RESPONSES/comment.md` with `UTF8` encoding as the body of the generated issue comment.\n\n```yaml\nsteps:\n   - name: Checkout repo\n     uses: actions/checkout@v2\n   - name: Comment on new issue\n     uses: actions/github-script@0.8.0\n     with:\n        github-token: ${{secrets.GITHUB_TOKEN}}\n        script: |\n           const fs = require('fs')\n           const issueBody = fs.readFileSync(\".github/ISSUE_RESPONSES/comment.md\", \"utf8\")\n           github.issues.createComment({\n           issue_number: context.issue.number,\n           owner: context.repo.owner,\n           repo: context.repo.repo,\n           body: issueBody\n           })\n       \n```\n\n# Conditional step\n```yaml\n- name: Add issue to project board\n        if: contains(github.event.issue.labels.*.name, 'bug')\n        uses: actions/github-script@0.8.0\n        with:\n          github-token: ${{secrets.GITHUB_TOKEN}}\n          script: |\n            github.projects.createCard({\n            column_id: 10065718,\n            content_id: context.payload.issue.id,\n            content_type: \"Issue\"\n            });\n\n```\n\nThis step will be executed if `contains(github.event.issue.labels.*.name, 'bug')`\n\n# Summary\nThings you learned in this course:\n\n- What GitHub Script is\n- How GitHub Script maps to octokit/rest.js\n- How to use the GitHub Script action in your workflow\n    - By commenting when an issue is opened\n    - By adding an issue to a project board when it is opened\n- How to read a file from the repository for use with GitHub Script\n- How to apply expressions to a workflow file\n\n\n\n\n","tags":["GitHub","DevOps"]},{"title":"Automate with GitHub App","url":"/2020/07/18/Automate-with-GitHub-App/","content":"I was taking the [Microsoft Module: Automate DevOps processes by using GitHub Apps](https://docs.microsoft.com/en-gb/learn/modules/automate-devops-github-apps/), which comes with an interactive [GitHub Learning Lab](https://lab.github.com/githubtraining/getting-started-with-github-apps). It is an interesting experience getting started with using GitHub Apps to enhance the team's GitHub experiences.\n\n[Probot](https://probot.github.io/apps/) is where a wide range of GitHub apps could be found and installed.\nOnce installed, GitHub Apps acts like a \"user\" that conduct jobs on their own behalf.\n\nGitHub Apps facilitate [continuous integration](https://github.community/t/scalable-continuous-integration-ci-patterns/13501). It could use webhook or poll patterns, but webhook is prefered.\n[Smee.io](https://smee.io/) is a webhook payload delivery service.\n\n# Configure webhook and GitHub App\n\nThis is the initial behaviour of the [Request info](https://github.com/apps/request-info) GitHub app:\n\n<img src=\"start.png\" alt=\"\" width=\"800px\"/>\n\nThe Request info requires the [/.github/config.yml](https://github.com/niuniuanran/getting-started-github-apps/blob/master/.github/config.yml) for configuration. \n\n[This blank issue](https://github.com/niuniuanran/getting-started-github-apps/issues/4) is a demo of how the customised request info behaves.\n\n# GitHub APIs and Webhooks\n[GitHub APIs](https://developer.github.com/v3/apps/available-endpoints/) and Webhooks go hand in hand, but the distinction between them is important.\n\n- Webhooks are specific \"noise\" interpreters. They listen for specific events to occur as their trigger.\n- When an event is triggered, the vastly more detailed GitHub API gives the bot an excessive amount of information (as a payload). The bot takes this payload, alters it slightly, and hands it back to GitHub's API, which delivers the change back to your repository.\n- The GitHub API can send information that makes changes to the platform, but only when prompted via webhook.\n- The GitHub API and GitHub's webhooks are both key components of GitHub Apps.\n\n# ToDo\nRead this article: [Scalable Continuous Integration (CI) Patterns](https://github.community/t/scalable-continuous-integration-ci-patterns/13501), it looks interesting!\n\n\n\n\n","tags":["GitHub","DevOps"]},{"title":"Add a preview image to my webpage","url":"/2020/07/16/Add-a-preview-image-to-my-webpage/"},{"title":"Powerful Azure Database Tools","url":"/2020/07/14/Powerful-Azure-Database-Tools/","content":"\nI was learning from the Microsoft Learn Module: [Develop and configure an ASP.NET application that queries an Azure SQL database] (https://docs.microsoft.com/en-us/learn/modules/develop-app-that-queries-azure-sql/3-exercise-create-tables-bulk-import-query-data) and it offers some powerful tools for data import and query.\n\n# bcp\n`bcp` is a convenient tool to import bulk data from file to sql database.\n\n```\nbcp [MSA-DB].dbo.MyTable format nul -c -f mytable.fmt -t , -S msa-anran.database.windows.net -U <username> -P <password>\n```\n\nThis generates a format file, `mytable.fmt`, that defines how data should be mapped to the columns of the table, `[MSA-DB].dbo.MyTable`.\nNote that the square brackets are added because the database name, `[MSA-DB]`, contains the character `-`, so that the database name could be properly handled.\n\n```\nbcp \"$DATABASE_NAME.dbo.studyplans\" in studyplans.csv -f studyplans.fmt -S \"$DATABASE_SERVER.database.windows.net\" -U $AZURE_USER -P $AZURE_PASSWORD -F 2\n```\nA command like this will import data from `studyplans.csv` to the table `$DATABASE_NAME.dbo.studyplans`, following the format defined in `studyplans.fmt`.\n\n# The `sqlcmd` utility and Cloud Shell\nThrough [Cloud Shell](https://docs.microsoft.com/en-us/azure/cloud-shell/using-the-shell-window), you could access your Azure resources from anywhere.\nTo use the `sqlcmd` utility, go to [Cloud Shell](https://shell.azure.com/) and run the following command. Replace <server> with the name of the database server that you created, <database> with the name of your database, and <user name> and <password> with your credentials.\n```\nsqlcmd -S <server>.database.windows.net -d <database> -U <username> -P <password>\n```\n\nIf the sign-in command succeeds, you'll see a 1> prompt. You can enter SQL commands on several lines and then type GO to run them:\n``` \n1> SELECT * FROM StudyPlans;  \n2> GO\n```\n\n# `psql` command line tool for PostgreSQL\nThe command-line tool called psql is the PostgreSQL distributed interactive terminal for working with PostgreSQL servers and databases. \n\n# The `System.Data.SqlClient` .NET library\nThe System.Data.SqlClient library is a collection of types and methods that you can use to connect to a SQL Server database that's running on-premises or in the cloud on SQL Database. \n\n# Azure CLI `az webapp` command\nThe [Azure CLI `az webapp` command](https://docs.microsoft.com/en-us/cli/azure/webapp?view=azure-cli-latest) manages web apps.\n\nIn order to build the connection between the course database and the app, the following files are modified, as instructed by the [microsoft learn unit: Connect an ASP.NET application to Azure SQL Database](https://docs.microsoft.com/en-us/learn/modules/develop-app-that-queries-azure-sql/5-exercise-connect-aspnet-to-azure-sql?source=learn):\n- `CoursesAndModules.cs`\n    - The model of the C# object.\n    - has `getter` and constructor for the fields\n- `DataAccessController.cs`\n    - Responsible for the data access logic\n    - An `SqlConnection` object is created which, given the connection string, connects to the database.\n    - A method `GetAllCoursesAndModules` is created, using the connection object to obtain a list of courseAndModules.\n- `Index.cshtml.cs`\n    - This file contains code that the `index.cshtml` runs when it's displayed.\n    - It contains a class, `CoursesAndModulesModel : PageModel`. This class overrides `onGet` method to provide the model needed by the view.\n    - It depends on `DataAccessController.cs`: creates a `DataAccessController` object, and uses it to obtain the list of courseAndModules within `onGet` method.\n    \n\n#","tags":["Database","Azure","SQL"]},{"title":"Using Azure Pipeline Variable Group","url":"/2020/07/12/Using-Azure-Pipeline-Variable-Group/","content":"I was building a toy TypeScript create-react-app App, [PetCalendar](https://github.com/niuniuanran/PetCalendar) that used the [Unsplash Image API](https://unsplash.com/developers) to fetch HD pet images for the user, and deploying it into [Azure Pipeline](https://dev.azure.com/).\n\nThe challenge I met was with using secret environmental variables for the ACCESS KEY of UNSPLASH API. \nFor local built, it was a easy task - adding the variable `REACT_APP_UNSPLASH_ACCESS_KEY` into the `.env` file and create-react-app takes care of accessing the variables.\nFor deployment, I learnt from [this StackOverflow post](https://stackoverflow.com/questions/51313330/how-to-use-environment-variables-in-react-app-hosted-in-azure) that Azure has a feature called \"variable group\" that perfectly suits my need.\nThe documentation for Variable Groups can be found [here at Microsoft docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/library/variable-groups?view=azure-devops&tabs=yaml).\n\n# Creating Variable Group\n\n<img src=\"azure.png\" alt=\"\" width=\"800px\"/>\n\nI could even choose to make the secret variables secret:\n\n<img src=\"variables.png\" alt=\"\" width=\"800px\"/>\n\n# Accessing Variable Group\nIn order to use the variable group from my Pipeline, I need to include the group in the `variables` in the `azure-pipelines.yml` file:\n\n```yaml\nvariables:\n  - group: Unsplash\n  - name: rootDir\n    value: \"pet-calendar\"\n  - name: buildDir\n    value: \"$(rootDir)/build\"\n```\n\nThen use the variable just like how I use other variables.\nAppending custom environment variables for create-react-app could be found in the [create-react-app documentation](https://create-react-app.dev/docs/adding-custom-environment-variables/#linux-macos-bash).\n\n```yaml\n  - script: |\n      cd $(rootDir)\n      npm install\n      REACT_APP_UNSPLASH_ACCESS_KEY=$(REACT_APP_UNSPLASH_ACCESS_KEY) REACT_APP_NAME=$(REACT_APP_NAME) npm run build\n      cd ..\n    displayName: \"npm install and build\"\n```\n\n\n","tags":["DevOps","CI/CD","Azure"]},{"title":"Nullable value types and Type Testing with Value Matching in C sharp","url":"/2020/07/08/Nullable-value-types-in-C-sharp/","content":"\n\n[A nullable value type](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/builtin-types/nullable-value-types) `T?` represents all values of its underlying value type T and an additional null value. \n\nFor example, you can assign any of the following three values to a `bool?` variable: `true`, `false`, or `null`. \n\nAn underlying value type T cannot be a nullable value type itself.\n\n\n# Test Typing with Pattern Matching\nBeginning with C# 7.0, the is operator also tests an expression result against a pattern. In particular, it supports the type pattern in the following form:\n\nExample combining **nullable value type** and pattern matching:\n \n``` \nint i = 23;\nobject iBoxed = i;\nint? jNullable = 7;\nif (iBoxed is int a && jNullable is int b)\n{\n    Console.WriteLine(a + b);  // output 30\n}\n```\n\n\nRead further [here at Microsoft Docs for type testing and cast](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/type-testing-and-cast)","tags":["C#"]},{"title":"MS learn note: Build a simple website using HTML, CSS, and Javascript","url":"/2020/07/07/MS-learn-note-Build-a-simple-website-using-HTML-CSS-and-Javascript/","content":"[Build a simple website using HTML, CSS, and Javascript](https://docs.microsoft.com/en-us/learn/modules/build-simple-website/) is a very basic course offered by microsoft, but there are some useful tips that are worth noting down for speedy coding.\n\n# Use VS Code\n- In Visual Studio Code, you can also create files with the command code index.html main.css app.js. These files are immediately opened in the editor, saving you a step, but also requires you to save each file.\n\n- Assuming that you're using Visual Studio Code, type code . in the command line to open the directory in that application. \n\n- In VS Code, select the index.html page and type html:5, then hit Enter (Return). It will generate the html skeleton.\n\n# noscript element\n The <noscript> element can be used to show a message if JavaScript is deactivated. Using the <noscript> element is an example of fault tolerance or graceful degradation. We can detect and plan for when a feature isn't supported or available.\n\n# Strict Mode\n As you get started with JavaScript, the initial focus is often working with numbers, math, text manipulation, dates, and storing information. Sometimes JavaScript makes assumptions about the type of data you enter; assignment, math, or logical equality can give you unexpected results. JavaScript tries to be friendly, make your code work, and provide you with a solution even if the result should be an error. To combat these shortcomings, you can activate strict mode, which reduces silent errors, improves performance, provides more warnings, and fewer unsafe features. You enter strict mode by adding `'use strict';` at the beginning of you js file.\n\n# Git\n## Git status\n```\ngit status\n```\ndisplays current git status, such as what untracked files you have.\n## add\nWhen you add files with Git, you're preparing to record or stage the changes. Although you could add a single file, usually it's best to stage all changes with the following command.\n``` \ngit add .\n```\n## commit\nAfter staging your changes, you can save them by committing. You add a message to the commit, so you know what changes occurred.\n```\ngit commit -m \"Add new posts to source\"\n```\n","tags":["Web Development","Microsoft Learn","VS Code"]},{"title":"Architecting AWS Module 01: Design your environment","url":"/2020/07/07/Architecting-AWS-Module-01-Design-your-environment/","content":"# Choose Region\nFour factors:\n- Data sovereignty\n- Latency\n- New service/feature\n-cost\n# How many AZs\nBest practice: start with two AZ, so that if resources in one AZ are unreachable, your application won't fail.\nFor heavy usage (Amazon DynamoDB) it may be beneficial to use more than two AZs.\n\n## Applications that heavily use Amazon EC2 spot instances\nTwo or more AZs to access more price options\n## Application with MySQL, MS SQL Server, Oracle data sources\nTwo availability zones for active/passive\n## Data sources Cassandra or MongoDB\n2 or more for extremely high availability\n\n# VPC\n## Using One VPC\n- High-performance computing: lower latency\n- Microsoft Active Directory for identity management: better security\n- Small, single applications\n\nFor most user cases, there are two primary patterns:\n\n<img src=\"multi-pattern.png\" alt=\"\" width=\"800px\"/>\n\n## Multi-VPC\neasier to maintain, suited for single team/organisations; sometimes required for governance or compliance\n## Multi-Account\nManaging access and standards can be challenging in more complex organisations.\n- Larger organisation\n- multiple IT teams\n- medium-sized organisation anticipating fast growth.\n\n### AWS Organisation\nHierarchical grouping of accounts\nOrganisation permissions overrule account permissions.\n","tags":["AWS","AWS SAA Test"]},{"title":"AWS SAA Exam Readiness note","url":"/2020/07/06/AWS-SAA-Exam-Readiness-note/","content":"\nThe new version of SAA test is based around the well-architectured framework.\n\nValidate the person's ability to:\n- Define a solution using architectual design principles, based on customer requirements\n- Provide implementation guidance based on best practices to the organisation **throughout the lifecycle of the project**.\n\n# Question Breakdown\n- Resilient: 34%\n- Performant: 24%\n- Secure: 26%\n- Cost-optimised: 10%\n- Operationally Excellent: 6%\n\n# Design Resilient Architectures\n\n## Choose reliable/resilient storage.\n### Ephemeral volumes\ninstance storage is ephemeral\n- only certain EC2 types\n- fixed capacity\n- disk type and capacity depends on EC2 instance\n- application-level durability\n- good for caching/temporary storage.\n### EBS\nattachable storage\nconnects to one EC2 instance at a time\n- different types\n- encryption\n- snapshots\n- provisioned capacity\n- independent lifecycle than EC2 instance\n- multiple volumes striped to create larger volumes\nThink as EBS volume as durable, attachable storage for EC2 instance.\nFour types of EBS:\nSDD is good for random access and HDD is good for sequential access.\n- gp2\n- io1\n- st1: throughput optimised HDD\n- sc1: Cold HDD\n\nSDD is more expensive than HDD.\n\nFurther reading: [White Paper: AWS Storage Overview](https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf)\n\n### EFS\n- Also mounted as a disk into EC2 instance like EBS, but could be shared among instances.\n- PB scale.\nElastic capacity.\nSupports NFS v4.0, v4.1 protocol.\nCompatible with Linux-based AMIs for Amazon EC2. Not currently supported for windows.\n\n#### EFS process:\nYou create an Amazon EFS volume and then create a mount point for it in a particular VPC. The EFS volume can only attach to a single VPC at a time.\nWithin the VPC, you get **mount target** that your EC2 instance can connect to.\n\n### S3\n#### Consistency model\nYou always want to ask the consistency for a distributed model.\n- Strong consistency for new objects\n- Eventual consistency for updates: because it is a scalable, distributed system.\n#### Storage classes\nS3 Standard\nS3 Standard-IA\n#### Encryption at rest\nSSE-S3: S3 manages keys\nSSE-KMS: KMS managed enveloped keys\nSSE-C: customer manage keys to encrypt on the server side\n#### Encryption at transit\nHTTPS\n#### Versioning\nGreat way to protect from accidental delete and overwrite.\n#### Access Control \nControl S3 access through:\n- IAM user policies\n- bucket policies\n- Amazon S3 access control lists (ACLs) enable you to manage access to **buckets and objects**.\n#### Multi-part upload\n#### Internet-API accessible\n#### Regional Availability\nRegional scoped.\n### Amazon Glacier\n- Encrypt data by default\n- Regional\n- Retrieval types: expedited( 1–5 minutes), standard(3-5 hours), bulk(5-12 hours)\n- 11 9's durability\n\n## Decouple\nEnsures if one component fails, the others stay functional.\n### SQS queue\n- asynchronous interaction \n- data persistence when service fails\n- allow for scalability in individual component\n### Decouple from Identity (e.g. IP) of Components\n- Elastic IP address: EIP could be moved to the new server. \n- Elastic Load Balancing\n\n## mult-tier architectures\nmult-tier architecture is naturally decoupled\n## high availability and fault tolerance\n\"Everything fails, all the time\"\nService failure should be treated as operational events. You want to design you system to be resilient to instance/code failures.\n### Fault tolerance\nThe more loosely your system is coupled, the more easily it scales and the more fault-tolerant it can be.\nFault tolerance means the user doe\ns not experience any impact from a fault and the SLA is met. It is **a higher requirement than high availability** - HA means the service is up and available, but could be running in a degraded status. \n\n<img src=\"high-a.png\" alt=\"\" width=\"800px\"/>\n\n<img src=\"fault-tolerance.png\" alt=\"\" width=\"800px\"/>\n\n## CloudFormation\nEnhances resilience: you can re-launch your system whenever you want.\n- Template (Declarative definition of resources to create) -> Stack (Collection of AWS resources)\n- Templates do not need to be region-specific.\n- You can use **mappings** to specify the base AMI since AMI IDs are different in each region.\n\n## Lambda\nProvide stateless code for AWS to deploy and manage - not vulnerable to instance failure. \nYou pay for invocation.\n- You can access Lambda print statement outputs through **CloudWatch Logs**.\n\n## RTO and RPO\n- RTO: recovery time objective, time taken for system to recover back to service.\n- RPO: recovery point objective, how much data is lost if the system fails (can be measured either in MB/GB or time units).\n\n## Test Axioms\n- Expect \"Single AZ\" will never be a right answer.\n- Using AWS managed services should always be preferred.\n- **Fault tolerance and high availability are not the same thing**: fault tolerant is a higher requirement, conceals the service from the user with **no loss**\n- Expect that everything will fail at some point and design accordingly.\n\n# Design Performant Architectures\n## Storage and databases\n### EBS\n- EBS trade-offs between SSD (General purpose/ provisioned IOPS) and HDD (throughput-optimised/cold HDD)\n- EBS volumes are automatically replicated within an Availability Zone.\n### S3\n- You should **offload static storage from your server instance to S3**: This dramatically improves web server performance by freeing up the server to use all of the CPU/memory for serving dynamic content.\n- Buckets are always tied to a region, although you do not need to specify the region in the URL (names are globally unique).\n\n#### Amazon S3 Payment Model\nPay for what you use, three components:\n- GBs per month\n- Transfer out of region\n- PUT, COPY, POST, LIST, GET requests\nFree for:\n- Transfer into S3\n- Transfer out from Amazon S3 to Amazon CloudFront\n- Transfer out from S3 to the same region\n\nIf you have higher availability requirement, use cross-region replication.\n\nObjects are immutable - the only way to change a single byte is to replace the object.\n\n### RDS or NoSQL?\n#### Use Amazon RDS when:\n- Complex transactions or complex queries\n- A medium-to-high query/write rate\n- No more than a single worker node/shard\n- High durability: won't be lost from machine fails (Also with DynamoDB)\n#### Do not use Amazon RDS when:\n- Massive read/write rates (eg. 150K write/second)\n- Sharding: DynamoDB automatically shards your data into multiple servers. This is why is scales so well.\n- Simple GET/PUT requests and queries -> DynamoDB\n- RDBMS customisation\n\n#### RDS Read Replicas\nImprove read performance, take workload off master instance.\nWhich RDS engines support Read Replica?\nRead Replica is supported for all of the  engines, **except** Microsoft SQL Server and Oracle. So these are:\n- MySQL\n- PostgreSQL\n- Aurora\n- MariaDB\n\n#### DynamoDB: Provisioned throughput\nDynamoDB grows in size as your data footprint changes.\nYou do specify your **throughput**\n\nThe units to define DynamoDB capacity are RCU and WCU.\n- Read capacity unit (for item up to 4KB in size).\n- One strongly consistent read per second.\n- Two eventually consistent reads per second. (If you don't care about strong consistency, each RCU gives you two reads)\n- Write capacity unit (for an item up to 1KB in size)\n- One write per second.\n\n## Caching\nYou can cache data of your application at different levels.\n### CloudFront\nWhen requested, the request gets directed to the optimal edge location. If the edge location does not have the content, the request goes to the origin, and the origin content is transferred to CloudFront edge location for caching.\n\nProtected with WAF and AWS Shield.\n\n### ElastiCache\nUse ElastiCache to cache what would have been repeatedly fetched from the database(DynamoDB/RDS/MongoDB)\n#### Memcached and Redis\n- Memcached: simpler, easier to set up, multi-thread, low maintenance, easy horizontal scalability with Auto Discovery.\n- Redis: more sophisticated data structure support, atomic operations, pub/sub messaging, read replicas/failover, cluster mode/sharded clusters, persistence\n#### Design decision\nGood candidates to store in a cache:\n- Session state\n- shopping cart\n- product catalog\n\nDo not cache data that needs to be as fresh as possible.\n\n## Elasticity and Scalability\n### Horizontal Scaling vs Vertical Scaling\nmight be tested on whether you want to scale up/down or scale out/in.\n\n### Auto Scaling\nAuto Scaling, Elastic Load Balancer and CloudWatch work together to enable auto scaling of EC2 instances.\n\nAuto Scaling uses a **Launch Configuration** to launch a fully configured instance automatically. Contains:\n- AMI ID, \n- instance type, \n- storage configuration,\n- key pair, \n- user data, \n- security group\n\nAuto Scaling Group\n- Points to the launch configuration\n- Specifies min, max, desired size of Auto Scaling group\n- May reference an ELB\n- Health Check Type\n\nAuto Scaling policy\n- Specifies how much to scale in/out\n- **One or more** policies could be attached to one group.\n\nCloudWatch Metrics\nCan monitor:\n- CPU\n- Network\n- Queue Size\nDefault EC2 metrics: CPU, network, disk\nMemory is not a native CloudWatch event: CloudWatch does not have access to this level.\n\n<img src=\"scaling.png\" alt=\"\" width=\"800px\"/>\n\nRemember: auto scaling takes time. If you know that a spike will happen at a known time, a **scheduled** auto scaling is better than scaling upon a CPU utilisation alarm. Here we assume that the new instances come up into full capacity in 20 minutes.\n\n<img src=\"auto-scaling-az.png\" width=\"800px\" alt=\"\"/>\n\n\"Must have at least 6 running instances to maintain minimally acceptable performance for a short period of time\":\nIt does not mean you want to scale down - 6 is the **minimally acceptable** number. It means you want your system to stay at least 6 when things fail!\n\nThe health status of an Auto Scaling instance is either **healthy or unhealthy**. All instances in your Auto Scaling group start in the healthy state. **Instances are assumed to be healthy unless Amazon EC2 Auto Scaling receives notification that they are unhealthy**. This notification can come from one or more of the following sources: **Amazon EC2, Elastic Load Balancing (ELB), or a custom health check**.\n\nEBS is responsible for **sending traffic to healthy instances**. \n\nIncreasing instance size will not increase availability. \n\n## Test Axioms \n- If data is unstructured, Amazon S3 is generally the storage solution.\n- Use caching strategically to improve performance\n- Know when and why to use Auto Scaling\n- Choose the instance and database type that makes the most sense for your workload and performance need.\n\n# Security\n- [White paper: AWS Security Best Practices](https://d1.awsstatic.com/whitepapers/Security/AWS_Security_Best_Practices.pdf)\n- [White paper: overview of security processes](https://d0.awsstatic.com/whitepapers/aws-security-whitepaper.pdf)\n- [IAM best practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)\n\n## Shared Responsibility Model\nManaged services move the line higher.\nBoth client-side and server-side data encryption configurations are responsibilities of customers.\n\n## Identities\nIAM integrates with Microsoft Active Directory and AWS Directory Service using SAML identity federation.\nIdentities in AWS have the following forms:\n- IAM users: Users created within an account\n- Roles: Temporary identities used by EC2 instances, Lambdas, and **external users**.\n- Federation: Users with Active Directory identities or other corporate credentials have role assigned in IAM\n- Web Identity Federation: Users with web identities from Amazon.com or other Open ID provider have role assigned using Security Token Service (STS).\n\nIt is impossible to put an IP restriction on root user logins.\n\n<img src=\"IAM-policies.png\" alt=\"\" width=\"800px\"/>\n\nPolicies control actions on AWS resources.\n\n## Secure Data\n### Data in transit\n#### Data transferring in and out of AWS infrastructure\n- SSL over web\n- IPsec for VPN\n- IPsec over AWS Direct Connect\n- Import/Export/Snowball\nThese are tampering-resistant and encrypted.\n#### Data sent to the AWS API\nAWS API calls use HTTPS/SSL by default\n### Data at rest\n#### Access Control\nData stored in Amazon S3 is private by default - requires AWS credentials for access.\n- Access over HTTP or HTTPS\n- Audit of access to all objects\n- Supports ACL on:\n    - Buckets\n    - Prefixes\n    - Objects\n    \n#### Encryption\n- Server side: SSE-S3, SSE-KMS, SSE-C\n- Client side: encrypt the data before sending in to S3: CSE-KMS, CSE-C(customer managed master encryption keys)\nIn some cases, client-side encryption is required for compliance and regulations.\nIn general, server-side encryption has better performance and is easier.\n\n#### Where to store keys\n- Key Management Service\n    - Customer software-based key management\n    - Integrated with many AWS services: EBS, S3, RDS, Redshift, Elastic Transcoder, WorkMail, EMR\n    - Use directly from application\n- AWS CloudHSM\n    - Hardware-based key management\n    - Use directly from application\n    - FIPS 140-2 compliance\n    - Dedicated applicance for key management\n\n## Define the network infrastructure for a single VPC application\n- Use subnets to define Internet accessibility.\n- Security Groups: Use security groups to control traffic in, out of and **between** resources.\n\n<img src=\"sg-vs-nacl.png\" alt=\"\" width=\"800px\"/>\n\nSecurity groups are stateful. Therefore, to allow users to access your web server, you do not have to allow outbound access on port 80 for the security group. You do need to allow outbound traffic for the NACL.\n\n### Services to get traffic in or out of your VPC\n- Internet Gateway: Connect the Internet\n- Virtual private gateway: VPN\n- AWS Direct Connect: Dedicated pipe\n- VPC peering: Connect to other VPCs\n- NAT Gateway: allow internet traffic from private subnets\n\n## Test Axioms\n- Lock down the root user\n- Security groups only allow. Network ACLs also deny.\n- Prefer IAM Roles to access keys.\n\n# Cost Optimisation\n## AWS Pricing\n- Pay as you go\n- Pay less per unit by using more\n- Pay less when you reserve\nYou pay for:\n- Compute\n- Storage\n- Data Transfer\n## EC2 pricing\n- Clock hours of server time\n- Machine configuration\n- Machine purchase type\n- Number of instances\n- Elastic Load balancing: charged by time\n- Detailed monitoring\n- Auto Scaling: AWS Auto Scaling is **free** to use, and allows you to optimize the costs of your AWS environment.\n- Elastic IP addresses\n- Operation systems and software package (marketplace)\n- **instance storage is free but ephemeral.**\n- EC2 instance pricing factors:\n    - EC2 instance family\n    - Tenancy (default/dedicated)\n    - Pricing model (reserved/spot/on demand)\n\n### Using spot instances \n- Use **[hibernate](https://aws.amazon.com/blogs/aws/new-hibernate-your-ec2-instances/)** to pause your instance (saves the in-memory data) and resume later.\n- **Spot blocks** allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted while your job completes.\n\n## S3 Pricing\n- Storage class\n- Storage\n- Requests\n- Data transfer\n\n**EFS does not support public files**\n\n## EBS\n- Volumes: Solid State Drives(SDD, more expensive, good for random access) and Hard Disk Drives(HDD, good for continuous access)\n- Input/output operations per seconds\n- Snapshots taken and restored\n- Data transfer\n\n## User serverless architecture to save cost\nBetter utilisation of resources by paying only when you use:\n- Lambda\n- DynamoDB\n- Amazon S3\n- API Gateway: attache your REST endpoints to Lambda, so that you can invoke the Lambda from the browser or from any HTTP client.\n\n## CloudFront \nBenefit on both cost and performance!\nUse cases\n- Content: static OR dynamic\n- Origins: Amazon S3, EC2, Elastic Load Balancing, HTTP servers\nReduce data transfer cost with CloudFront\n- There is no data transfer charge between S3 and CloudFront.\n- You can also use CloudFront to offload the work for EC2 instances.\n\n### CloudFront Pricing\n- Traffic distribution\n- Requests\n- Data transfer out\n\n## Test Axioms\n- If you know it will be on, reserve\n- Any unused CPU is a waste of money\n- Use the most cost-effective data storage service and class\n- Determine the most cost-effective EC2 pricing model and instance type for each workload.\n\n# Operational Excellence\nMain idea: make the system automated and adapting to circumstance changes.\n\nOperational Excellence: The ability to run and monitor systems to deliver business value and continually improve supporting processes and procedures. \nKey practices:\n- Prepare\n- Operate\n- Evolve\n\n## Operational Excellence Design Principles\n- Perform operations with code\n- Annotate documentation\n- Make frequent, small, reversible changes\n- Refine operations procedure frequently\n- Anticipate failure\n- Learn from all operational failures\n\n## AWS Services for Operational Excellence\n- AWS Config: track resources such as EBS volumes and EC2 instances. verifies that resources comply to configuration rules.\n- AWS Cloud Trail: logs API calls\n- AWS CloudFormation: code -> stack\n- AWS Inspector: check EC2 instances for security vulnerabilities\n- AWS Trusted Advisor: check account for best practices on security, reliability, cost, performance and service limits\n- VPC Flow Logs: logs network traffic. **capture layer 3 and layer 4 IP-level logs. could not do things about layer 7 errors like 404 errors.**\n- AWS CloudWatch: can help extract patterns by converting log lines into metrics.\n\n## Test Axioms\n- IAM roles and safer than keys and passwords.\n- Monitor metrics across the system.\n- Automate responses to metrics where appropriate.\n- Provide alerts for anomalous conditions.\n\n\n\n","tags":["AWS","AWS SAA Test"]},{"title":"Putting the 404 Express middleware in the right place","url":"/2020/07/06/Putting-the-404-Express-middleware-in-the-right-place/","content":"Adding the below middleware (and possibly rendering a self-defined error page) makes Express give the 404 error when all the routes provided before it does not match the current URL. If this middleware is added before a valid route, that route will be blocked and 404 error will be returned.\n```javascript\n        //404 Not Found Middleware\n        app.use(function (req, res, next) {\n            res.status(404)\n                .type('text')\n                .send('Route Not Found');\n        });\n```\n\n# Async Issue\nI met with an async issue when adding this middleware when doing the [issue tracker fCC project](https://github.com/niuniuanran/boilerplate-project-issuetracker).\n```javascript\n//Routing for API\napiRoutes(app);\n\n//404 Not Found Middleware\napp.use(function(req, res, next) {\n  res.status(404)\n    .type('text')\n    .send('Not Found');\n});\n```\nIt looks like the api routes are added before 404 middleware, but the `apiRoutes` function needs to build a database connection before adding the routes:\n```javascript\n/**apiRoutes*/\nmodule.exports = function (app) {\n    MongoClient.connect(CONNECTION_STRING, (err, client) => {\n        const db = client.db(\"issues\");\n        app.post('/api/issues/:project', function (req, res) {\n            //  ...\n})})}\n```\n\nBecause of this delay, the traffic is caught by the 404 middleware instead, and I always get a 404 error when making a request to `/api/issues/:project`.\n\nTo solve this problem, I put the `apiRoutes(app)` just before `app.listen`, and put the 404 middleware INSIDE the callback of the db connection. This means all the api routes will be finished before the 404 middleware is added.\n\n# chai-http routes\nWhen running integration test (functional test) with `chai.request(app)`, because the request is sent right after the app is initiated, the api routes are not properly loaded either:\n```javascript\ntest('Every field filled in', function (done) {\n            chai.request(app)\n                .post(\"/api/issues/test\")\n                .send({\n                    issue_title: 'Title',\n                    issue_text: 'text',\n                    created_by: 'Functional Test - Every field filled in',\n                    assigned_to: 'Chai and Mocha',\n                    status_text: 'In QA'\n                })\n                .end(function (err, res) {\n                    assert.equal(res.status, 200, \"Status should be 200!\"); \n// assert fails with 404, because the db connection is not successful yet and api routes are not loaded to the server.\n})})\n```\n\nThis is how the code provided by fCC solves the problem:\n```javascript\nif(process.env.NODE_ENV==='test') {\n    console.log('Running Tests...');\n    setTimeout(function () {\n      try {\n        runner.run();\n      } catch(e) {\n        const error = e;\n          console.log('Tests are not valid:');****\n          console.log(error);\n      }\n    }, 3500);}\n```\nIt waits for 3.5 seconds for the test to start, to ensure that the tests are run upon a fully loaded server.","tags":["Web Development","Javascript","HTTP","Express"]},{"title":"Express: response resolved does not mean function terminated","url":"/2020/07/06/Express-response-resolved-does-not-mean-function-terminated/","content":"This is my code:\n```javascript\n MongoClient.connect(CONNECTION_STRING, (err, client) => {\n        const db = client.db(\"issues\");\n        app.post('/api/issues/:project', function (req, res) {\n            const project = req.params.project;\n            const title = req.body.issue_title;\n            const text = req.body.issue_text;\n            const creator = req.body.created_by;\n            if (!(title && text && creator)) {\n                res.status(400).json({err: \"missing required field\"});\n            } \n\n            const assigned = req.body.assigned_to || '';\n            const statusText = req.body.status_text || '';\n            const createdOn = new Date();\n\n            db.collection(project).insertOne({\n                \"issue_title\": title, \"issue_text\": text, \"created_by\": creator,\n                \"assigned_to\": assigned, \"status_text\": statusText, \"created_on\": createdOn,\n                \"updated_on\": createdOn, \"open\": true\n            }, {}, (err, doc) => {\n                if (err) {\n                    return res.status(500);\n                } else res.json(doc.ops[0]);\n            })\n\n        });\n```\nThis is the very interesting error I got:\n```\n Uncaught Error [ERR_HTTP_HEADERS_SENT]: Cannot set headers after they are sent to the client\n```\n\nThis is because the below piece of code has sent the response to user, but the function keeps executing, and attempts to send another request to user.\n```javascript\nif (!(title && text && creator)) {\n  res.status(400).json({err: \"missing required field\"});\n} \n```\n","tags":["Web Development","Javascript","HTTP","Express"]},{"title":"Auto-increment in MongoDB","url":"/2020/07/02/Auto-increment-in-MongoDB/","content":"\nI'm working on the [URL Shortener API project by freeCodeCamp](https://www.freecodecamp.org/learn/apis-and-microservices/apis-and-microservices-projects/url-shortener-microservice). To get a robustly unique auto-increment shortened URL, I want to implement auto-increment at MongoDB.\n\nMy implementation could be find at [this GitHub repository](https://github.com/niuniuanran/boilerplate-project-urlshortener).\n\n# What the data structure looks like\n\nThe `UrlPair` is the model to store the pairing between original URL and the short URL.\nThe `Counter` is the model to keep track of auto-incrementation. Each counter has a `for` value stating what it is counting for. \n```javascript\nconst Schema = mongoose.Schema;\nconst urlPairSchema = new Schema({\n    \"original\": {type: String, required: true},\n    \"short\": {type: String, required: true, unique: true}\n});\nconst UrlPair = mongoose.model(\"URL_Pair\", urlPairSchema);\nconst counterSchema = new Schema({\n    \"for\": {type: String, required: true, unique: true},\n    \"counter\": {type: Number, default: 0, required: true}\n});\nconst Counter = mongoose.model(\"Counter\", counterSchema);\n```\n# How to achieve incrementation\nThe query looks as follows:\n```javascript\nCounter.findOneAndUpdate({\"for\": \"URL_Pair\"}, {$inc: {\"counter\": 1}}, {upsert: true})               \n```\n\n[`findOneAndUpdate`](https://mongoosejs.com/docs/api.html#model_Model.findOneAndUpdate) takes three argument, the condition, the update and the options (and then the callback).\n`$inc` is the query to increment the specified value.\n\n# How to insert the initial doc\n`{upsert:true}` option instructs `findOneAndUpdate` to insert a new doc if the queried doc does not exist.\n\n","tags":["Web Development","MongoDB","Mongoose"]},{"title":"Gained my API and Microservices certificate at FCC","url":"/2020/07/01/Finished-my-API-and-Microservices-certificate-at-FCC/","content":"Today I gained my [API and Microservices certificate](https://www.freecodecamp.org/certification/anran/apis-and-microservices)!\n\n# Frameworks and tools I used\n- MongoDB Atlas\n- Mongoose\n- Express\n- multer\n- body parser\n\n# Patterns I learnt\n- The callback patterns of Express\n- Scheme - Model - Query of mongoose\n\n# Challenges I met\n- Manually implement auto-increment in MongoDB\n- Validating a Javascript `Date` object. If `new Date()` is passed with an invalid argument, the output is an `Invalid Date` object which is also an instance of `Date`. The invalid date could be identified through `isNaN(invalidDate)`.\n- Chaining queries. The user could choose whether they want to add a `from` and `to` filter into their query, as well as a `limit` field. See [here](https://github.com/niuniuanran/boilerplate-project-exercisetracker/blob/gomix/server.js) for what I did for the GET router `'/api/exercise/log?[userId]&{from}&{to}&{limit}'`.\n- Handling file uploads with [Express multer middleware](http://expressjs.com/en/resources/middleware/multer.html)\n","tags":["Web Development","Javascript","RESTful API"]},{"title":"Middleware functions","url":"/2020/06/28/Middleware-functions/","content":"Today I first learnt to use Express middleware.\nExpress.js provides a sound documentation [here](https://expressjs.com/en/guide/using-middleware.html).\n\n# Middleware functions\nMiddleware functions are functions that take 3 arguments: the request object, the response object, and the next function in the application’s request-response cycle. \n\nThese functions execute some code that can have side effects on the app, and usually add information to the request or response objects. \n\nThey can also end the cycle by sending a response when some condition is met. \n\nIf they don’t send the response when they are done, they start the execution of the next function in the stack. This triggers calling the 3rd argument, next().\n\n# Bind application-level middleware\nBind application-level middleware to an instance of the app object by using the app.use() and app.METHOD() functions, where METHOD is the HTTP method of the request that the middleware function handles (such as GET, PUT, or POST) in lowercase.\n\n# Loading multiple middleware functions\n```javascript\napp.use('/user/:id', function (req, res, next) {\n  console.log('Request URL:', req.originalUrl)\n  next()\n}, function (req, res, next) {\n  console.log('Request Type:', req.method)\n  next()\n})\n```","tags":["Web Development","Express"]},{"title":"Errors identified in CISSP Guide to Security Essentials, 2nd Edition, Gregory","url":"/2020/06/27/Errors-identified-in-CISSP-Guide-to-Security-Essentials-2nd-Edition-Gregory/","content":"I am reading the 'CISSP Guide to Security Essentials, 2nd Edition' written by Peter H. Gregory, published by Cengage Learning. This is a very informative book that gives a wide range of knowledge in information security, but there are some . \n\nI will note down here any errors that I identify for people's future reference.\n\n# IEEE 802.11 a/b/g/n (Chapter 10)\n[IEEE 802.11 a/b/g/n](https://standards.ieee.org/standard/802_11-2016.html) is a family of wireless network standards.\nThe book prints this standard family as `IEEE 802.1 a/b/g/n` on all its appearances, including page 417, page 414, page 388 in Chapter 10 (Telecommunications and network security).\n\n# Vulnerability to frequency analysis (Chapter 5)\nOn page 179, second graph of \"Transposition\" section, the book states: `This makes a transposition cipher - as well as a substitution cipher - vulnerable to frequency analysis.`\n\nOn page 179, at the end of the \"Monoalphabetic\" section, the book states: `Like a transposition cipher, a monoalphabetic cipher is subject to a frenquency analysis attack.`\n\nThese two statements look incorrect to me. \n The characters in ciphertext by transposition are as they are, so frequency analysis won't really get any extra information, therefore I believe Transposition is **NOT** subject to frequency analysis attack.\n\n# Countermeasures to reduce ALE (Chapter 1)\nOn beginning of page 7, the book discusses a range of countermeasures to reduce ALE.\nThe third measure: `Changes in single loss expectancy` should instead be **Annualized rate of occurrence(ARO)** to make sense with the context.\n\n`Changes in single loss expectancy` is equivalent to the second countermeasure listed here, `changing the EF`, as long as the Asset Value stays the same.\n\n# Single Point of Failure (Chapter 1)\nOn the bottom of page 11 - Single Point of Failure, the discussion on Figure 1-2 stated that `The firewall is a single point whose failure will cause the failure of the entire system's objectives`. Based on the figure and the context, the single point of failure should instead be the **Gateway**(GW).\n\n# Information Labeling (Chapter 1)\nIn the first paragraph Information Labeling section of page 19, it states: `When others are aware of the` **classification level** `of a particular set of data, they are more apt to be aware of the` **classification level** `and handle the data properly`.\nThis sentence contains redundancy and does not make a lot of sense. \nBased on the context, it might want to say:\n`When others are aware of the` **classification level** `of a particular set of data, they are more apt to be aware of the` **sensitivity level** `and handle the data properly`\n\n\n\n\n","tags":["Security","Reading notes"]},{"title":"AWS High Availability","url":"/2020/06/25/AWS-High-Availability/","content":"\n# Avoid Single Point of Failure\nIf two application servers use the same database server, that database server is the single point of failure. A common solution is to have a secondary database. \nNotice that when the main database goes offline, the application servers need to automatically send their requests to the secondary database. This goes back to **High Availability Best Practice #2: treat resources as disposable, and design your applications to support changes in hardware**.\n\nImplement redundancy where possible to prevent single failures.\nNot neccessarily duplicated components. You can use **automated solutions** that only launch when needed, or a **managed service** where AWS automatically replaces malfunctioning underlying hardware for you\n\n# High Availability Factors\nFault tolerance, recoverability, and scalability are three primary factors that determine the overall availability of your application.\n## Fault Tolerance\nThe built-in redundancy of an application's components\n## Recoverability\nThe process, policies, and procedures related to restoring service after a catastrophic event.\n## Scalability\nThe ability of an application to accommodate growth without changing design.\n\n# AWS Services and HA\n\n<img src=\"HA-services.png\" alt=\"\" width=\"900px\"/>\n\nRoute 53 is one of our only services that has a 100% availability service-level agreement.\n\n**AWS RDS, CloudFront, EBS, ELB are inherently highly available**\n\n- You can make Amazon Elastic Compute Cloud—or Amazon EC2—highly available by having two or more Amazon EC2 instances or placing Amazon EC2 instances in an Auto Scaling group. \n- With Amazon Redshift, you can set up the clusters and have a copy so that if something happens to one of the nodes, the service could be started from somewhere else. \n- For AWS Direct Connect, you can either have two direct connections, or use VPN as a backup.\n\n# Elastic Load Balancing\nThree types of ELB:\n\n<img width=\"800px\" src=\"ELB.png\" alt=\"ELB\"/>\n\n## ALB\n- Content-based routing\n- WebSocket, HTTP, HTTPS\n- Containers or EC2 instances\n- Advanced request routing, supports microservices and container-based applications\n- Sticky sessions are a mechanism to route requests from the same client to the same target. Application Load Balancer supports sticky sessions using load balancer generated cookies. If you enable sticky sessions, the same target receives the request and can use the cookie to recover the session context. Stickiness is defined at a target group level.\n\n## NLB\n- high throughput\n- ultra low latency\n- route connections based on IP protocol data\n- optimized to handle sudden and volatile traffic patterns, while using a single static IP address per Availability Zone\n- Network Load Balancer can now distribute requests regardless of Availability Zone with the support of cross-zone load balancing. \n\n# Sticky Cookie\nBy default, a load balancer routes each request independently to the application instance with the smallest load. However, you can use sticky session features—which is also known as session affinity—that enable the load balancer to bind a user’s session to a specific application instance.\nThis takes the responsibility of maintaining sessions from the server instance to the load balancer.\n\n# Elastic IP address\nElastic IP addresses are very important because they allow us to mask the failure of an instance or software by allowing users and clients to use the same IP address with replacement resources.\n\n# High availability with the right architecture. \n- you can make Amazon Elastic Compute Cloud—or Amazon EC2—highly available by having two or more Amazon EC2 instances or placing Amazon EC2 instances in an Auto Scaling group. \n- With Amazon Redshift, you can set up the clusters and have a copy so that if something happens to one of the nodes, the service could be started from somewhere else. \n- For AWS Direct Connect, you can either have two direct connections, or use VPN as a backup.\n    - Direct Connect (DX) and Hardware virtual network (VPN) use virtual private gateway in an Amazon VPC.\n    - Direct Connection(DX) is highly available when there are two ports open. \n    - DX also allows for a connection from remote servers to the AWS cloud at any time.\n\n# ELB Connection Draining – Remove Instances From Service With Care\nYou can use Elastic Load Balancing on its own, or in conjunction with Auto Scaling. When combined, the two features allow you to create a system that automatically adds and removes EC2 instances in response to changing load.\n\nIn order to provide a first-class use experience, you’d like to avoid breaking open network connections while taking an instance out of service, updating its software, or replacing it with a fresh instance that contains updated software. Imagine each broken connection as a half-drawn web page, an aborted file download, or a failed web service call, each of which results in an unhappy user or customer.\nYou can now avoid this situation by enabling the new **Connection Draining feature** for your Elastic Load Balancers. \n\nConnection Draining is **enabled by default for load balancers that are created using the Console**.\nWhen Connection Draining is enabled and configured, the process of **deregistering an instance from an Elastic Load Balancer** gains an additional step. For the duration of the configured timeout, the load balancer will **allow existing, in-flight requests made to an instance to complete**, but it will **not send any new requests to the instance**. **During this time, the API will report the status of the instance as InService, along with a message stating that “Instance deregistration currently in progress.”** **Once the timeout is reached, any remaining connections will be forcibly closed**.\n\n# RDS scaling\n## With scaling on RDS you can:\n- Scale up/down with resizable instance types\n- Scale storage up with a few clicks or via the API\n    - Easy conversion from standard to Provisioned IOPS storage\n- Offload read traffic to Read Replicas to increase read performance\n    \n## Putting a cache in front of Amazon RDS\n- ElastiCache for Memcached or Redis\n- Your preferred cache solution, self-managed on EC2\n\n## Scaling Amazon RDS Writes with Database Sharding\n Sharding is a technique for improving the performance of writing with multiple database servers. Fundamentally, sharding is when you prepare databases with identical structures, and divide them—using appropriate table columns as keys—to distribute writing processes.\n \n## Horizontal scaling with Read Replicas\n\n# Autoscaling considerations\n- Avoid thrashing\n    - scale out easily\n    - scale in slowly\n- Use lifecycle hooks: Define lifecycle hooks to perform custom actions when Auto Scaling launches or terminates instances. \n- Stateful applications require additional automatic configuration of instances that are launched into Auto Scaling groups. Remember that instances can take several minutes after launch before they are fully usable.\n\n# EC2 Auto Recovery\nIf you need identical number of instances, you can use EC2 Auto Recovery instead of ASG.\n","tags":["AWS","AWS SAA Test"]},{"title":"Cypress conditional testing","url":"/2020/06/19/Cypress-conditional-testing/","content":"\nI was writing end-to-end tests for TripTime's authorisation function, and as the system will block user from registering a new account if the email is occupied, I used the [conditional testing feature offered by Cypress](https://docs.cypress.io/guides/core-concepts/conditional-testing.html#Definition). \n```\n       .get('body')\n       .then($body => {\n         if ($body.text().includes('Sorry')) {\n           cy.get('header')\n             .contains('Log In')\n             .click()\n             .get('[type=\"email\"]')\n             .type('new-user@cypres.com')\n             .get('[type=\"password\"]')\n             .type('testpassword')\n             .get(\"[type='submit']\")\n             .click();\n         } else {\n           cy.get(\"[type='submit']\").click();\n         }\n       })\n```\n\n> The problem with conditional testing is that it can only be used when the state has stabilized. In modern day applications, knowing when state is stable is oftentimes impossible.\n\nThis problem applies to me, as it takes time for the request to validate the email to resolve, and Cypress might have already taken the wrong path by then. \nOn the other hand, waiting for XHR is not very robust either as I do not have control over when the validating request is to be returned.\nI settled on waiting for 1s for now in this specific scenario. On other scenarios I seek to see if I could wait for the XHR to resolve instead of hard coding the waiting time.","tags":["Web Development","End-to-end Testing","Test Driven Development"]},{"title":"AWS Reliability and Disaster Recovery","url":"/2020/06/19/AWS-Reliability-and-Disaster-Recovery/","content":"Well-Architectured Pillar 3: Reliability \n# Principles of the Reliability pillar\nReliability is the ability of a system to recover from infrastructure or service failures, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues.","tags":["AWS","Architecture Methodology"]},{"title":"Testing xhr in cypress","url":"/2020/05/24/Testing-xhr-in-cypress/","content":"[Cypress](https://www.cypress.io/) is a nice Javascript end-to-end testing framework.\nIt could also spy into what http requests have been sent, wait for their response and assert the response. The documentation for this functionality can be found on the [Network Requests](https://docs.cypress.io/guides/guides/network-requests.html#Testing-Strategies) doc page.\n\nFor me it was very useful as my machine is really slow hosting the server in the docker, and I have to allow a waiting time before going to the next step. `cy.wait(8000)` is an easy walk around but is neither reliable nor elegant. So instead, I spied on the requests and wait for it to get a response, and proceed to the next step when I get the response wanted.\n\nHere is the code I used to spy on requests:\n```javascript\ncy.server()\n        .route({\n        method: 'GET',\n        url: 'http://localhost:8080/api/user',\n      })\n        .as('getUser')\n        .route({\n          method: 'POST',\n          url: 'http://localhost:8080/api/login',\n        })\n        .as('logIn')\n        .route({method: 'POST', url: 'http://localhost:8080/api/trips'})\n        .as('newTrip')\n```\n\n`.as()` gives the request an alias that could be referred to later.\n\n```javascript\ncy.visit('http://localhost:3000')\n        .wait('@getUser') // wait for the request for user to get a response\n        .should('have.property', 'status', 401) // user is not log in, should be 401\n        .get('header') // now the top bar knows that the user is not logged in\n        .contains('Log In') // and renders the log in link\n        .click()\n        .get(\"[value='Log in']\")\n        .get(\"[type='email']\")\n        .type('test@test.com')\n        .get(\"[type='password']\")\n        .type('testtest')\n        .get(\"[type='submit']\")\n        .click()\n        .wait('@logIn') // wait for the request to log in to get a response\n        .should('have.property', 'status', 200) // response success\n        .wait('@getUser') // get current user info\n        .should('have.property', 'status', 200) // success\n        .get('header') //now the header renders for an existing user\n        .get(\"[href='/newtrip']\") // the link for new trip is available\n         .click()\n```","tags":["Web Development","End-to-end Testing","Test Driven Development"]},{"title":"Solving React State Hook setter not working","url":"/2020/05/18/Solving-React-State-Hook-setter-not-working/","content":"\nLet me put the [Stack Overflow post](https://stackoverflow.com/questions/54163796/react-usestate-hook-setter-has-no-result-with-array-variable) that solved my problem here first.\n\nI was doing essentially the same thing, but with activity editing. This is a function that I provided in the `TripContext`:\n```javascript\n/** src/spa/contexts/TripContext.js **/\n\n function updateOneActivity(activityPatch, activityId) {\n    const tripID = router.query.id;\n    axios\n      .patch(`${hostName}/api/trip/${tripID}/activities`, {\n        id: activityId,\n        ...activityPatch,\n      })\n      .then(res => {\n        setActivities(\n          activities.map(activity =>\n            activityId === activity.id ? res.data.activity : activity,\n          ),\n        );\n      })\n      .catch(err => {\n        setDialogError({\n          title: 'Activity Update Failed',\n          message: `Sorry, we failed to update the activity ${\n            activityPatch.name\n          } because: ${\n            err.response && err.response.data && err.response.data.message\n              ? err.response.data.message\n              : 'An internal error happened'\n          }`,\n        });\n        setDialogErrorDisplay(true);\n      });\n  }\n```\nNote that I was trying to update activity by passing an array to `setActivities`.\n\n\nThis is how I used it in `ActivityCard`:\n- Component:\n```javascript\n/** src/spa/components/cards/ActivityCard.js, part of return statement **/\n<DateTimePicker\n     value={activity.start}\n     ampm={false}\n     onChange={start =>\n       handleEdit({ start }, updateOneActivity)\n     }\n     open={startChanging}\n     onClose={() => setStartChanging(false)}\n     TextFieldComponent={() => null}\n     showTodayButton\n   />\n```\n- Handler:\n```javascript\n/** src/spa/components/cards/ActivityCard.js **/\n  const handleEdit = (activityPatch, updateOneActivity) => {\n    if (checkTimeValid(activityPatch)) {\n      updateOneActivity(activityPatch, activityID);\n    } else {\n      setTimeErrorDisplay(true);\n    }\n  };\n```\n\nWhen I edit the activity, the database is updated but I dont get a page update.\nIf I log the `activities` in the console after the `setActivity` is called, I can see that the `activities` array remain the old value.\n\nIt is to be kept in mind that the setter defined with `useState` has the same behaviour as `setState` in class components. Have a look at the [setState documentation](https://reactjs.org/docs/react-component.html#setstate).\n\n> setState() enqueues changes to the component state and tells React that this component and its children need to be re-rendered with the updated state.\n\n> Think of setState() as a request rather than an immediate command to update the component. For better perceived performance, React may delay it, and then update several components in a single pass. React does not guarantee that the state changes are applied immediately.\n\n> **setState() does not always immediately update the component. It may batch or defer the update until later**. This makes reading this.state right after calling setState() a potential pitfall. Instead, use componentDidUpdate or **a setState callback (setState(updater, callback))**, either of which are guaranteed to fire after the update has been applied. If you need to set the state based on the previous state, read about the updater argument below.\n\nSo my solution would be to pass `setActivity` with an updater function`(state, props) => stateChange`.\n\nHere's my updated code in the `TripContext`:\n```javascript\n/** src/spa/contexts/TripContext.js **/\n\n function updateOneActivity(activityPatch, activityId) {\n    const tripID = router.query.id;\n    axios\n      .patch(`${hostName}/api/trip/${tripID}/activities`, {\n        id: activityId,\n        ...activityPatch,\n      })\n      .then(res => {\n        setActivities(\n         activities=> activities.map(activity =>\n            activityId === activity.id ? res.data.activity : activity,\n          ),\n        );\n      })\n      .catch(err => {\n        setDialogError({\n          title: 'Activity Update Failed',\n          message: `Sorry, we failed to update the activity ${\n            activityPatch.name\n          } because: ${\n            err.response && err.response.data && err.response.data.message\n              ? err.response.data.message\n              : 'An internal error happened'\n          }`,\n        });\n        setDialogErrorDisplay(true);\n      });\n  }\n```\n\nWorks smooth like silk. Now when I update activities in `ActivityCard` or anywhere else, the Timeline of the trip will be updated.","tags":["Web Development","React","Javascript","TripTime"]},{"title":"Test React Component at Different States","url":"/2020/05/17/Test-React-Component-at-Different-States/","content":"\nThe user Dashboard of TripTime will show a loading state to the user while it is fetching data from the API, and display cards of the trips once the loading have been completed.\nHere's what it looks like:\n\n<img src=\"loading.gif\" width=\"800px\" alt=\"loading effect\"/>\n\nNow here's the problem: how do I test that the UI is working properly at these two different stages?\nLuckily, we can call `setState` on the instance rendered by the React Test Renderer.\n\nHere's what I did for dashboard that is at the loading state:\n\n```javascript\n/** src/spa/__test__/components/homepage/DashboardLoading.test.js */\n\nimport TestRenderer from 'react-test-renderer'\nimport React from 'react'\nimport Dashboard from '../../../components/dashboard/Dashboard'\nimport TripList from '../../../components/dashboard/TripList'\nimport axios from 'axios'\n\ndescribe('Test Dashboard Loading', () => {\n  const DATE_TO_USE = new Date(2020, 11, 17, 23, 24, 0)\n  const _Date = Date\n  global.Date = jest.fn(() => DATE_TO_USE)\n  global.Date.UTC = _Date.UTC\n  global.Date.parse = _Date.parse\n  global.Date.now = _Date.now\n  axios.get.mockResolvedValue({ data: [] })\n  const userHomePageRenderer = TestRenderer.create(\n    <Dashboard name={'Tester'} />\n  )\n  test('Check if Dashboard renders three TripList', () => {\n    expect(userHomePageRenderer.root.findAllByType(TripList).length).toBe(3)\n  })\n  test('Check if Dashboard displays three loading bars when the trips are loading', () => {\n    userHomePageRenderer.root.instance.setState({\n      currentLoading: true,\n      pastLoading: true,\n      planningLoading: true,\n    })\n    expect(userHomePageRenderer.toJSON()).toMatchSnapshot()\n  })\n})\n```\n\nAnd then for the loaded status:\n\n```javascript\n/** src/spa/__test__/components/homepage/DashboardLoaded.test.js */\n\ntest('Check if Dashboard displays TripCards when it is loaded', () => {\n  userHomePageRenderer.root.instance.setState({\n    currentLoading: false,\n    pastLoading: false,\n    planningLoading: false,\n  })\n  expect(userHomePageRenderer.root.findAllByType(TripCard).length).toBe(12)\n  expect(userHomePageRenderer.toJSON()).toMatchSnapshot()\n})\n```\n\nUnluckily, Jest does not support having multiple SnapShots for the same test suit yet. which is why these two states are tested in two separate test files, and generates `src/spa/__tests__/components/homepage/__snapshots__/DashboardLoaded.test.js.snap` and `src/spa/__tests__/components/homepage/__snapshots__/DashboardLoading.test.js.snap` respectively.\n\nAnother thing worth noting is that `import axios from 'axios';` did not import the node modules, but imports `src/spa/__mocks__/axios.js`:\n\n```javascript\n/** src/spa/__mocks__/axios.js */\n\nconst mockAxios = jest.genMockFromModule('axios')\n\nmockAxios.create = jest.fn(() => mockAxios)\n\nexport default mockAxios\n```\n\nThis mocks axios so that we can conduct `axios.get.mockResolvedValue({ data: [] });`.\n","tags":["Web Development","React","TripTime","Jest"]},{"title":"Created my first Promise object","url":"/2020/05/14/Created-my-first-Promise-object/","content":"This is a baby step.\nI was trying to write a dummy function to mock the behaviour of an API get request, so it needs to be a promise that resolves to an array.\nThis is the code:\n```javascript\nconst pretendStr = 'i-am-a-sharable-link-for-trip';\nfunction generateJoinString(tripID) {\n  return new Promise(function(resolve) {\n    setTimeout(resolve.bind(null, `${pretendStr}${tripID}`), 400);\n  });\n}\n```\n\nThis Promise will resolve to `${pretendStr}${tripID}` after 400ms.\n\nWill work further on dealing with Promise objects later.","tags":["Javascript"]},{"title":"AWS well-architecture framework","url":"/2020/05/13/AWS-well-architectured-framework/","content":"The AWS Well-Architected Framework helps cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. \n\n# The 5 Pillars of the AWS Well-Architected Framework\n## Operational Excellence \nThe operational excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures. \nThere are six design principles for operational excellence in the cloud:\n- Perform operations as code\n- Annotate documentation\n- Make frequent, small, reversible changes\n- Refine operations procedures frequently\n- Anticipate failure\n- Learn from all operational failures\n## Security\nThe security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. \nThere are six design principles for security in the cloud:\n- Implement a strong identity foundation\n- Enable traceability\n- Apply security at all layers\n- Automate security best practices\n- Protect data in transit and at rest\n- Prepare for security events\n\n## Reliability\nThe reliability pillar includes the ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues. \nThere are five design principles for reliability in the cloud:\n- Test recovery procedures\n- Automatically recover from failure\n- Scale horizontally to increase aggregate system availability\n- Stop guessing capacity\n- Manage change in automation\n\n# Performance Efficiency\nThe performance efficiency pillar includes the ability to use computing resources efficiently to meet system requirements and to maintain that efficiency as demand changes and technologies evolve. \nThere are five design principles for performance efficiency in the cloud:\n- Democratize advanced technologies\n- Go global in minutes\n- Use serverless architectures\n- Experiment more often\n- Mechanical sympathy\n\n# Cost Optimization\nThe cost optimization pillar includes the ability to avoid or eliminate unneeded cost or suboptimal resources.\nThere are five design principles for cost optimization in the cloud:\n- Adopt a consumption model\n- Measure overall efficiency\n- Stop spending money on data center operations\n- Analyze and attribute expenditure\n- Use managed services to reduce cost of ownership\n\n\n# four best practice areas for performance efficiency\nThere are four best practice areas for performance efficiency in the cloud:\n- Selection: The optimal solution for a particular system will vary based on the kind of workload you have, often with multiple approaches combined.\n- Review: When architecting solutions, there is a finite set of options that you can choose from. However, over time new technologies and approaches become available that could improve the performance of your architecture.\n- Monitoring: After you have implemented your architecture you will need to monitor its performance so that you can remediate any issues before your customers are aware. \n- Tradeoffs: When you architect solutions, think about tradeoffs so you can select an optimal approach. Depending on your situation you could trade consistency, durability, and space versus time or latency to deliver higher performance.","tags":["AWS","AWS SAA Test"]},{"title":"Material-UI framework","url":"/2020/05/12/Material-UI-framework/","content":"This is a framework that I feel I should have known earlier... \nHere's the [homepage](https://material-ui.com/) for future experiments.\nAt this moment the exciting things that I know it can do are [Dialog](https://material-ui.com/components/dialogs/#dialog) and [Date/Time pickers](https://material-ui.com/components/pickers/#date-time-pickers).\nIt is going to make impressume so much nicer!\n\n ","tags":["Web Development","React"]},{"title":"Generate PDF on front end with react-pdf","url":"/2020/05/11/Generate-PDF-on-front-end-with-react-pdf/","content":"\nI am working to achieve the function for user to generate their trip plan into a pdf that they can carry around with them. [react-pdf](https://react-pdf.org/) is a very powerful tool for me to achieve this goal.\n\n# Prevent SSR in Next.js \nAs the web rendering functions of react-pdf only work on client side, the first problem I faced was to prevent server side rendering. I wrote [another blog](https://niuniuanran.github.io/2020/05/07/Prevent-Next-js-Server-Side-Rendering/) on this issue.\n\n# Code for a nice pdf\nSo here comes the big part: create a nice pdf for user's trip plan. There are limited options on what [components](https://react-pdf.org/components) I could use: \n- Page\n- Link\n- Text\n- View\n- Image\n- Note\n- Canvas\nIt is to be noted that `View` can be nested within another `View`.\n\n## Creat a fixed header that appears on each page\n```\nimport { Text, View, Link } from '@react-pdf/renderer';\n\n<View fixed>\n    <Text\n     style={{\n       fontSize: 10,\n       textAlign: 'right',\n       margin: 5,\n       color: '#ff4200',\n     }}\n    >\n     Presented to you by{' '}\n     <Link src={'http://triptime.cc'}>TripTime</Link>\n    </Text>\n</View>\n```\nHere's the generated header:\n\n<img src=\"header.png\" width=\"800px\" alt=\"a fixed header on top of each page\"/>\n\n## Styled Components\n```\nimport styled from '@react-pdf/styled-components';\n\nconst GrayText = styled.Text`\n     color: #666;\n   `;\n```\nThen I can use`<GrayText>{activity.description}</GrayText>` to generate gray Text components.\n\n\n## Nesting Views\nViews can be nested.\n\nDayView:\n```\nimport _ from 'underscore';\nimport { Text, View} from '@react-pdf/renderer';\n\nexport default function DayView(props) {\n     const day = props.dailyRecord.day;\n     const events = _.sortBy(props.dailyRecord.events, 'start');\n   \n     return (\n       <View>\n         <Text style={{ color: '#ff6400', fontSize: 16 }}>{day}</Text>\n         {events.map((event, index) => {\n           if (event.type === 'travel')\n             return <TravelView travel={event} key={index} />;\n           else return <ActivityView activity={event} key={index} />;\n         })}\n       </View>\n     );\n   }\n```\n\nActivityView:\n```function ActivityView(props) {\n     const activity = props.activity;\n     return (\n       <View style={{ fontSize: 14, margin: 5 }} wrap={false}>\n         <Text>Activity: {activity.name}</Text>\n         <View style={{ paddingLeft: 10, fontSize: 12 }}>\n           <GrayText>{activity.description}</GrayText>\n           {activity.address && (\n             <Text>\n               <GrayText>At:</GrayText> {activity.address}\n             </Text>\n           )}\n           <Text>\n             <GrayText>From:</GrayText> {formatTimeString(activity.start)}{' '}\n             <GrayText>To:</GrayText> {formatTimeString(activity.end)}\n           </Text>\n         </View>\n       </View>\n     );\n   }\n```\n\nHere's the look of the trip plan pdf, organised by days:\n\n<img src=\"days.png\" width=\"800px\" alt=\"activities and travels organised in days\"/>","tags":["Web Development","React","TripTime","Next.js"]},{"title":"Group activities with underscore library","url":"/2020/05/10/Group-activities-with-underscore-library/","content":"This will be a nice feature of TripTime: \n> As a user, I want to be able to generate and download a pdf for the plan of my trip, so that I can carry my plan around and share with my friends.\n\nThe best organisation of the pdf would be in the order of date. So, I have an array of activities and an array of travels. I want them to be grouped by dates, and these groups should be in order as well. Sounds like a couple of intimidating iterations to me on first thought.\n\nLuckily there's a js package, [underscore](https://www.npmjs.com/package/underscore) that provides a range of functional programming helpers. \n\n```javascript\nfunction groupByDay(events) {\n             const occurrenceDay = function(event) {\n               return moment(event.start)\n                 .startOf('day')\n                 .format('dddd Do MMMM, YYYY');\n             };\n             const groupToDay = function(group, day) {\n               return {\n                 day: day,\n                 events: group,\n               };\n             };\n             return _.chain(events)\n               .groupBy(occurrenceDay)\n               .map(groupToDay)\n               .sortBy('day')\n               .value();\n           }\n```\n\n# [chain](https://underscorejs.org/#chain)\nReturns a wrapped object. Calling methods on this object will continue to return wrapped objects until value is called.\n\n# [groupBy](https://underscorejs.org/#groupBy)\nSplits a collection into sets, grouped by the result of running each value through iteratee. If iteratee is a string instead of a function, groups by the property named by iteratee on each of the values.\n\n# [map](https://underscorejs.org/#map)\nProduces a new array of values by mapping each value in list through a transformation function (iteratee).\n\n# [sortBy]((https://underscorejs.org/#sortBy))\nReturns a (stably) sorted copy of list, ranked in ascending order by the results of running each value through iteratee. iteratee may also be the string name of the property to sort by (eg. length).","tags":["Web Development","Javascript","TripTime","Functional Programming"]},{"title":"Prevent Next.js Server Side Rendering","url":"/2020/05/07/Prevent-Next-js-Server-Side-Rendering/","content":"When I was trying to generate a `PDFDownloadLink` for TripTime, I got this error:\n> PDFDownloadLink is a web specific API. Or you're either using this component on Node, or your bundler is not loading react-pdf from the appropiate web build.\n\nAs the author of react-pdf explained in [this issue](https://github.com/diegomura/react-pdf/issues/613):\n> This happens when next.js renders the page server side. You cannot do that, since what <PDFDownloadLink /> does is creating a blob file on the browser with the PDF content. \n\nUp to now I have learnt two ways to prevent Next.js Server Side Rendering:\n\n# Use next js dynamic import with ssr disabled\n\nAccording to [Next.js document](https://nextjs.org/docs/advanced-features/dynamic-import):\n> Next.js supports ES2020 dynamic import() for JavaScript. With it you can import JavaScript modules (inc. React Components) dynamically and work with them. They also work with SSR.\n\nThis is what my teammate used for importing the leaflet map too:\n```\nimport dynamic from 'next/dynamic';\n\nconst TripMapNoSSR = dynamic(() => import('../components/map/TripMap'), {\n     ssr: false,\n   });\n```\n\n# Use hooks to prevent SSRing\nThis is shared by a [alexcaulfield](https://github.com/alexcaulfield) in the same [issue](https://github.com/diegomura/react-pdf/issues/613#issuecomment-557327989) and I reckon this could be used in many scenarios to prevent SSR, not limited to importing.\nSo I wrote my code based on his idea:\n\n```\nimport React, {useEffect, useState} from 'react';\n\nimport MyDocument from '../../../components/pdf/TripDocument';\nimport {PDFDownloadLink, PDFViewer} from \"@react-pdf/renderer\";\n\nexport default function Pdf() {\n  const [isClient, setIsClient] = useState(false);\n  useEffect(() => {\n    setIsClient(true)\n  }, []);\n\n  return <>\n    {isClient && <PDFDownloadLink document={<MyDocument/>} fileName='somename.pdf'>\n      {({blob, url, loading, error}) =>\n        loading ? 'Loading document...' : 'Download now!'\n      }</PDFDownloadLink>\n    }\n  </>\n}\n```\n\nworked nice:\n\n<img src=\"pdf.gif\" width=\"800px\" />","tags":["TripTime","Next.js"]},{"title":"Mounting a new EBS volume to EC2","url":"/2020/05/07/Mounting-a-new-EBS-volume-to-EC2/","content":"\n# The `df` tool and `df -h` command\n`df -h` Using ' -h ' parameter with (df -h) will show the file system disk space statistics in “human readable” format, means it gives the details in bytes.\n\n# Create an ext3 file system on the new volume\n ```shell script\n mkfs [options] [-t type fs-options] device [size]\n```\n \n `-t` or `-type` specify the type of file system to be built. If not specified, the default file system type (currently ext2) is used. Here we specify `ext3`.\n \n ```shell script\n sudo mkfs -t ext3 /dev/sdf\n```\nCreate an ext3 file system on the new volume, `/dev/sdf`\n\n# Create directory for mounting the volume\n\nCreate a directory for mounting the new storage volume:\n\n```shell script\nsudo mkdir /mnt/data-store\n```\n\n# Mount the new volume\n```shell script\nsudo mount /dev/sdf /mnt/data-store\n```\n\n# Configure the Linux instance to mount this volume whenever the instance is started\n\n```shell script\necho \"/dev/sdf   /mnt/data-store ext3 defaults,noatime 1 2\" | sudo tee -a /etc/fstab\n```\n\n## tee\n`tee` is a command in command-line interpreters (shells) using standard streams which reads standard input and writes it to both standard output and one or more files, effectively duplicating its input.\n\n`-a` Appends the output to each file, rather than overwriting it.\n\n## fstab\nThe `fstab` (or file systems table) file is a system configuration file commonly found at `/etc/fstab` on Unix and Unix-like computer systems.\n\n\n\n\n\n\n","tags":["AWS"]},{"title":"AWS global, regional and one-AZ services","url":"/2020/05/06/AWS-global-regional-and-one-AZ-services/","content":"\n# Global service\n## IAM\nUsers are global entities, like an AWS account is today. No region is required to be specified when you define user.\n\n# One Region, Multiple Availability Zone automatically\n## Amazon Elastic File System\nAmazon EFS is a **regional service** storing data within and across **multiple Availability Zones (AZs)** for high availability and durability.\n\n## SNS\nAll messages published to SNS are stored redundantly across multiple Availability Zones.\n\n## SQS\nAmazon SQS stores all message queues and messages within a single, highly-available AWS region with multiple redundant Availability Zones (AZs), so that no single computer, network, or AZ failure can make messages inaccessible.\n\n# Multiple Availability Zone automatically, Go to multiple regions with additional feature\n## Amazon Aurora\n Amazon Aurora is designed to offer greater than 99.99% availability, replicating 6 copies of your data across 3 Availability Zones and backing up your data continuously to Amazon S3. \n ### Amazon Aurora Global Database \nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. \n\n## Amazon S3\nYou specify an AWS Region when you create your Amazon S3 bucket. For S3 Standard, S3 Standard-IA, and S3 Glacier storage classes, your objects are automatically stored across multiple devices spanning a minimum of three Availability Zones, each separated by miles across an AWS Region. \nCross-Region replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions.\n\n## Amazon DynamoDB \nAmazon DynamoDB global tables provide a fully managed solution for deploying a multiregion, multi-master database, without having to build and maintain your own replication solution. \n\n# One Availability Zone by default, can go to multiple availability zones\n## Amazon RDS\nWhen you provision a **Multi-AZ DB Instance**, Amazon RDS synchronously replicates the data to a standby instance in a different Availability Zone (AZ). \n\n\n","tags":["AWS","AWS CP Test"]},{"title":"Teardrop with CSS","url":"/2020/05/06/Teardrop-with-CSS/","content":"This is just a single tip on how to create a teardrop (or a map pin when upside-down) with CSS.\n\n<img src=\"target.gif\" width=\"500px\"/>\n\nI really like what I made for the loading effect and for the map bubbles!\n\nThe shape was made by:\n\n```css\n.loading {\n    width: 15rem;\n    height: 15rem;\n    border-radius: 15rem 15rem 0;\n    transform: rotate(45deg);\n}\n```\nThe only issue is that the content of the div will be rotated too. So:\n```css\n.loading div{\n    transform: rotate(-45deg);\n}\n```\n","tags":["TripTime","CSS","Web Beautify"]},{"title":"React Refs to Components, and finding out methods available to the Component","url":"/2020/05/05/React-Refs-to-Components-and-finding-out-methods-available-to-the-Component/","content":"Target:\n<img src=\"target.gif\" width=\"450px\"/>\n\nSo I want to be able to let the user see the other end of the travel when they click \"Go to the departure (destination) point\". This means I need to have reference to the `Marker` of the other end of the travel.\n\nMy first failed naive attempt:\n```\ntravel.fromMarker = <Marker\n          position={travel.from}\n          icon={generateTravelIcon(travel.travel_rgb, travel.mode, true)}\n        >\n          <Popup>\n           ...\n            <a href='#' onClick={()=>{travel.toMarker.click()}}>\n              Go to destination point\n            </a>\n          </Popup>\n        </Marker>\n```\nThis did not even feel right!\n> Keep in mind, however, that the JSX doesn't return a component instance! It's just a ReactElement: a lightweight representation that tells React what the mounted component should look like. \n-- From [Refs to Components](https://zhenyong.github.io/react/docs/more-about-refs.html)\n\n# React createRef() and ref\nWhat I need is refs:\n> Refs provide a way to access DOM nodes or React elements created in the render method.\n-- From [Refs and the DOM](https://reactjs.org/docs/refs-and-the-dom.html)\n\nHere's what I did to make the pair of Markers:\n```\nclass TravelMarkerPair extends React.Component {\n  static propTypes = {\n    travel: PropTypes.object.isRequired,\n  };\n\n  constructor(props) {\n    super(props);\n    this.fromMarker = React.createRef();\n    this.toMarker = React.createRef();\n  }\n\n  toggleFocus(clickTo) {\n    clickTo\n      ? this.fromMarker.current.fireLeafletEvent('click')\n      : this.toMarker.current.fireLeafletEvent('click');\n  }\n\n  render() {\n    const travel = this.props.travel;\n    return (\n      <>\n        <Marker\n          position={travel.to}\n          icon={generateTravelIcon(travel.travel_rgb, travel.mode, true)}\n          ref={this.toMarker}\n        >\n          <Popup>\n            ...\n            <a href='#' onClick={() => this.toggleFocus(true)}>\n              Go to departure point\n            </a>\n          </Popup>\n        </Marker>\n        <Marker\n          position={travel.from}\n          icon={generateTravelIcon(travel.travel_rgb, travel.mode, true)}\n          ref={this.fromMarker}\n        >\n          <Popup>\n            ...\n            <a href='#' onClick={() => this.toggleFocus(false)}>\n              Go to destination point\n            </a>\n          </Popup>\n        </Marker>\n      </>\n    );\n  }\n}\n```\n\nSo I am able to refer to the component with `this.toMarker` and `this.fromMarker`, and then I can fire the leaflet event of clicking them by `this.toMarker.current.fireLeafletEvent('click')` when user clicks the link from its sibling.\n\n# Finding out the methods available to the component\nI did not find the method `fireLeafletEvent` very easily. I tried `click()` but was told that `this.toMarker.current.click` is not a function. I then thought if it was because React component did not have `click()`, and tried this provided in a [stackoverflow post](https://stackoverflow.com/questions/40091000/simulate-click-event-on-react-element):\n```javascript\nconst mouseClickEvents = ['mousedown', 'mouseup'];\nfunction simulateMouseClick(element){\n  mouseClickEvents.forEach(mouseEventType =>\n    element.dispatchEvent(\n      new MouseEvent(mouseEventType, {\n          view: window,\n          bubbles: true,\n          cancelable: true,\n          buttons: 1\n      })\n    )\n  );\n}\n```\nI was told that `this.toMarker.current.dispatchEvent` was not a function either.\n\nYihao helped me with logging all the properties available of a Javascript object.\n\nThis will print ALL the properties, including inherited ones and own ones:\n```javascript\nfor (const p in component) {\nconsole.log(p)\n}\n```\n\nThis will not print inherited properties:\n```javascript\nconsole.log(Object.getOwnPropertyNames(component));\n```\n\nThis will only print functions:\ntypeof object[property] == 'function'\n```javascript\nfor (const p in component) {\nif (typeof component[p] === \"function\")\nconsole.log(p)\n}\n```\nThis is how I found there is a `fireLeafletEvent` method that suits my need.","tags":["Web Development","React","Javascript","TripTime"]},{"title":"Generate nice random colors","url":"/2020/05/05/Generate-nice-random-colors/","content":"\nI was trying to generate some random numbers for the Markers on the map:\n\n<img src=\"usage.png\" width=\"140px\"/>\n\nThe colours need to be diverse for each travel pairs for easy distinction, and cannot be too light. \nI want to use an array of three numbers so that I'll have the flexibility to use `rgba()` for background opacity alternation:\n```javascript\nconst rgb = props.rgb;\n  const rgbString = `${rgb[0]},${rgb[1]},${rgb[2]}`;\n  const toStyle = {\n    border: `2px solid rgb(${rgbString})`,\n    color: `rgb(${rgbString})`,\n    backgroundColor: `rgba(256, 256, 256, 0.8)`,\n  };\n  const fromStyle = {\n    border: `2px solid white`,\n    color: `white`,\n    backgroundColor: `rgba(${rgbString}, 0.8)`,\n  };\n```\n\n# Random RGB numbers\nThe naive approach is just to generate random numbers for rgb:\n```javascript\nfunction random_rgb() {\n    var o = Math.round, r = Math.random, s = 255;\n    return [o(r()*s), o(r()*s), o(r()*s)];\n}\n```\nThis gets me totally random numbers that could be very light or dark.\n\n# Using HSV\n> Hue, Saturation, and Value (HSV) is a color model that is often used in place of the RGB color model in graphics and paint programs. In using this color model, a color is specified then white or black is added to easily make color adjustments. HSV may also be called HSB (short for hue, saturation and brightness).\n\n- S: 0(not colorful) -> 1(fully colourful)\n- V: 0(black) -> 1(white) \nSo I want the saturation and value stable, but hue random.\n\n\n```javascript\nfunction generateRandomRGB() {\n  let h = Math.random();\n  return HSVtoRGB(h, 0.95, 0.7);  \n}\n\nfunction HSVtoRGB(h, s, v) {\n  var r, g, b, i, f, p, q, t;\n  if (arguments.length === 1) {\n    (s = h.s), (v = h.v), (h = h.h);\n  }\n  i = Math.floor(h * 6);\n  f = h * 6 - i;\n  p = v * (1 - s);\n  q = v * (1 - f * s);\n  t = v * (1 - (1 - f) * s);\n  switch (i % 6) {\n    case 0:\n      (r = v), (g = t), (b = p);\n      break;\n    case 1:\n      (r = q), (g = v), (b = p);\n      break;\n    case 2:\n      (r = p), (g = v), (b = t);\n      break;\n    case 3:\n      (r = p), (g = q), (b = v);\n      break;\n    case 4:\n      (r = t), (g = p), (b = v);\n      break;\n    case 5:\n      (r = v), (g = p), (b = q);\n      break;\n  }\n  return [Math.round(r * 255), Math.round(g * 255), Math.round(b * 255)];\n}\n```\n\n# Improve the distribution using golden ratio\n[This article](https://martin.ankerl.com/2009/12/09/how-to-create-random-colors-programmatically/) recommended using golden ratio to achieve more evenly distributed hue.\nThe algorithm for this is extremely simple. Just add 1/Φ and modulo 1 for each subsequent color.\n\n```javascript\nfunction generateRandomRGB() {\n  const goldenRatioConjugate = 0.618033988749895;\n  let h = Math.random();\n  h += goldenRatioConjugate;\n  h %= 1; //0.5 % 1 = 1, 1.5 % 1 = 1\n  return HSVtoRGB(h, 0.95, 0.7);\n}\n```\n\n# Reinforcing even distribution (not random)\nHere's another guy's code using the new CSS functions `hsl()` and `hsla()` to colour an array of elements:\n```javascript\nvar hue = 0;\nArray.prototype.slice.call(document.querySelectorAll('.myClass')).forEach(function(mc) {\n    mc.style.color = 'hsla(' + hue + ', 75%, 50%, 0.5)';\n    hue += 222.5;\n});\n```\n`hsl()` and `hsla()` take care of normalizing the hue to the [0, 360) range for you. 222.5 is approximately 360/Φ\n\n\nNeat!","tags":["Web Beautify"]},{"title":"Debouncing event handling","url":"/2020/05/04/Debouncing-event-handling/","content":"I was implementing the sign up function for TripTime, and added a `onChange` handler for the email input to send an API request to check if the email address has already been taken.\n\n<img src=\"immediate.png\" width=\"900px\" alt=\"immediate checking\"/>\n\nAnd the API server is dying.\n\nThere needs to be a delay after the user stops typing before the event handler gets invoked, and my teammate suggested that debounce is the solution.\n> The debounce function delays the processing of the keyup event until the user has stopped typing for a predetermined amount of time. \n\n# Read the `debounce` code\nFirst let's have a look at what a debounce function should look like:\n\n```javascript\n// Credit David Walsh (https://davidwalsh.name/javascript-debounce-function)\n\n// Returns a function, that, as long as it continues to be invoked, will not\n// be triggered. The function will be called after it stops being called for\n// N milliseconds. If `immediate` is passed, trigger the function on the\n// leading edge, instead of the trailing.\nfunction debounce(func, wait, immediate) {\n  var timeout;\n\n  return function executedFunction() {\n\n// First we save the context of this and the contents of the arguments passed to executedFunction.\n    var context = this;\n    var args = arguments; // In JavaScript, you can call a function with an arbitrary number of parameters even if they aren’t in the function definition, and arguments will still capture them.\n\t    \n// The callback function, later,  is executed after the end of the debounce timer.\n    var later = function() {\n//The timeout is set to null which means the debounce has ended\n      timeout = null;\n// Then it checks to see if we want to call the debounced func on the tail end. If we do, then it executes func.apply(context, args). \n      if (!immediate) func.apply(context, args);\n    };\n\n// A callNow === true event will cause func to be executed immediately and then prevent any subsequent calls unless the the debounce timer has expired.\n    var callNow = immediate && !timeout; \n\t\n// Next, we clearTimeout which had prevented the callback from being executed and thus restarts the debounce\n    clearTimeout(timeout);\n\n// we (re-)declare timeout which starts the debounce waiting period\n    timeout = setTimeout(later, wait);\n\t\n// If the full wait time elapses before another event, then we execute the later callback function.\n    if (callNow) func.apply(context, args);\n  };\n}\n```\n> A debounce is a higher-order function, which is a function that returns another function (named executedFunction here for clarity). This is done to form a closure around the func, wait, and immediate function parameters and the timeout variable on line 9 so that their values are preserved.\n\nHere are the meaning of each variable:\n`func`: The function that you want to execute after the debounce time\n`wait`: The amount of time you want the debounce function to wait after the last received action before executing func. \n`immediate`: This determines if the function should be called on the leading edge and not the trailing. This means you call the function once immediately and then sit idle until the wait period has elapsed after an action. After the idle time has elapsed, the next event will trigger the function and restart the debounce.\n`timeout`: The value used to indicate a running debounce.\n\n# Usage of `debounce` \n> Common scenarios for a debounce are **resize, scroll, and keyup/keydown events**. In addition, you should consider wrapping any interaction that triggers **excessive calculations** or **API calls** with a debounce.\n\n\nHere is an experiment:\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Title</title>\n    <script>\n        function debounce(func, wait, immediate) {\n            // debounce function\n        }\n        function updateDisplay(e) {\n            console.log(\"called\");\n            document.querySelector(\"#text-display\").innerText = e.target.value;\n        }\n\n        window.onload=function () {\n            >>>>>>> a\n            document.querySelector(\"#text-input\").addEventListener(\"input\", debounce(updateDisplay, 400));\n            >>>>>> b\n            document.querySelector(\"#text-input\").addEventListener(\"input\", updateDisplay);\n            >>>>>> c\n            document.querySelector(\"#text-input\").addEventListener(\"change\", debounce(updateDisplay, 400));\n        }\n\n    </script>\n</head>\n<body>\n<form>\n    <label>\n        Type your input:\n        <input type=\"text\" id=\"text-input\"/>\n    </label>\n</form>\n\nDisplay:\n<p id=\"text-display\">\n</p>\n\n</body>\n</html>\n```\n- Outcome of version a: <img src=\"version-a.gif\" width=\"300px\"/>\nAs can be seen, only after I have stopped typing for 400ms will the display be updated.\n- Outcome of version b:\n<img src=\"version-b.gif\" width=\"300px\"/>\nAs can be seen, the display is updated immediately.\n- Outcome of version c:<img src=\"version-c.gif\" width=\"300px\"/>\nAs can be seen, the display is updated only when the input box loses focus. \nThis has to do with the difference between `input` event and `change` event\n    - oninput event occurs when the **text** content of an element is changed through the user interface. Triggers immediately, unlike change. \n    - onchange occurs when the selection, the checked state or the contents of an element have changed. **In some cases, it only occurs when the element loses the focus**\n\n# Using react-debounce-input package for React debounce\nThere are issues related to using debounce in React as it uses event pooling (will get on that later). \nThe package [react-debounce-input](https://www.npmjs.com/package/react-debounce-input) deals with debouncing events, and this is what I used for signup:\n\n```\n// import { DebounceInput } from 'react-debounce-input';\n<DebounceInput\n  type='email'\n  name='email'\n  required={true}\n  debounceTimeout={400}\n  onChange={event => {\n    handleEmailInput(event);\n    checkEmailOccupied(event.target.value);\n  }}\n  value={userEmail}\n/>\n```","tags":["Web Development","React","Javascript","TripTime"]},{"title":"Information Security Basic Terms and Concepts","url":"/2020/04/28/Information-Security-Basic-Terms-and-Concepts/","content":"\n# Mission, Objective, Goals\nThey are set up by organisation management. Security specialists should act under the mission, objectives and goals, and be involved in key activities.\nSecurity specialist should give information about risk management.\n\n# Risk Management\nThree parts: \n- determining the maximum acceptable level of overall risk\n- using **risk assessment** techniques to determine the initial level of risks\n- If this is excessive, develop **risk treatment** strategies to ameliorate risks\n\n## Risk Assessment\n### Qualitative Risk Assessment\nFor a given scope of assets, identify:\n- Vulnerabilities: characteristic of the object/process/asset, for example, a poor lock\n- Threats: burglar\n- Threat probability \n- Attack: when a person tries to use the vulnerability and when the threat comes true\n- Impact: if the attack happens, what will the loss be\n- Countermeasures: how to prevent such an attack\n\n### Quantitative Risk Assessment\nExtension of qualitative risk assessment.\n- Asset value\n- exposure factor (EF)\n- single loss expectancy SLE = EF * Asset\n- annualized rate of occurrence (ARO)\n- annual loss expectancy ALE = SLE * ARO\n\n### Quantifying Countermeasures\nGoal: reduction of ALE\nImpact of countermeasures:\n- Cost of countermeasure\n- Changes in Exposure Factor (EF)\n- Changes in Single Loss Expectancy (SLE)\n\n### Geographic Considerations\n- Replacement and repair costs of assets may vary by location\n- Exposure Factor may vary by location\n- Impact may vary by location\n\n### Risk Assessment Methodologies\nNot to be tested by listing the content. \n- NIST 800-30\n    -Risk Management Guide for Information Technology Systems\n- OCTAVE\n    -Operationally Critical Threat, Asset, and Vulnerability Evaluation\n- FRAP\n    -Facilitated Risk Analysis Process – qualitative prescreening\n- Spanning Tree Analysis\n    -visual, similar to mind map\n\nThese assessment methodologies above are very expensive and heavy in labour.\n\n## Risk Treatment\n- Risk acceptance: live with that\n- Risk avoidance: discontinue risk-related activity\n- Risk reduction: mitigate. \n- Risk transfer: buy insurance\n\n# Security Management Concepts\n## Security Controls\n- Detective: give us information about the attack, for example, security alarm\n- Preventive: lower the risk of the threat happening, for example, better lock\n- Deterrent: Act on the psychology of the attacker, for example, a CCTV sign\n- Administrative: arrange procedure of which way someone will be allowed to enter a premise\n- Compensating: transfer the risk, buy an insurance.\n## CIA Triad\nThree pillars of security:\n- Confidentiality: information and functions can be accessed only by properly authorized parties\n- Integrity: Information and functions can only be added, altered or removed only by authorized persons and means. Does not mean it matches the truth. It only defines who can operate on them. \n- Availability: systems, functions and data must be available on-demand according to any agreed-upon parameters regarding levels of service. \n\nRemember: each of the terms is related to the specific need of the scenario. \n## Defence in Depth\nA layered defence in which two or more layers or controls are used to protect an asset:\n- Heterogeneity（异质性）: the different controls should be **different types**, so as to better resist attack\n- Entire protection: each control completely protects the asset from most or all threats\n> Defense in depth reduces or eliminates the risks associated by single points of failure, fail open, malfunctions, and successful attacks on individual components\n\n<img src=\"defence-in-depth.jpeg\" width=\"500px\" alt=\"defence in depth layers\"/>\n\n## Single Points of Failure\nA single point of failure (SPOF): weakness in a system where the failure of a single component results in the failure of the entire system\n\n## Fail open, fail closed\nWhen a security mechanism fails, there are usually two possible outcomes:\n- Fail open: the mechanism **permits** all activity\n- Fail closed: the mechanism **blocks** all activity.\n\nPrinciples:\n- Different types of failures will have different results\n- Both fail open and fail closed are undesirable, but sometimes one or the other is catastrophic!\n\nSee [here](https://www.ixiacom.com/company/blog/fail-closed-fail-open-fail-safe-and-failover-abcs-network-visibility) for fail open, fail closed, fail safe, and failover(Failover implies recovery of functionality, achieved through redundancy)\n\n## Privacy\nDefined: the protection and proper handling of sensitive personal information.\nRequires proper technology for protection\nRequires appropriate business processes and controls for appropriate handling\nIssues\n- Inappropriate uses\n- Unintended disclosure to others\n\n# Security Management\n## Security Executive oversight\n- Support and enforcement of policies\n- Allocation of resources\n- Prioritisation of activities\n- Risk treatment\n## Governance\n“Security governance is the set of responsibilities and practices exercised by the board and executive management with the goal of providing strategic direction, ensuring that objectives are achieved ascertaining that risks are managed appropriately and verifying that the enterprise's resources are used responsibly.”\n- Steering Committee oversight\n- Resource allocation and prioritization\n- Status reporting\n- Strategic decisions\n## Policy, requirements, guidelines, standards, and procedures\n- Policies: constraints of behavior on systems and people. Defines what, but not how.\n- Requirements: required characteristics of a system or process\n- Guidelines: defines how to support a policy\n- Standards: what products, technical standards, and methods will be used to support policy\n- Procedures: step by step instructions\n## Roles and responsibilities\nFormally defined in security policy and job descriptions\nThese need to be defined:\n- Ownership of assets\n- Access to assets\n- Use of assets\n- Managers responsible for employee behavior\n## Service level agreements\nRoles and responsibilities are the main parts of SLA.\nSLAs define a formal level of service.\nSLAs for security activities:\n- Security incident response: what we should do there's an incident? Many organisation don't have this sort of agreement/policy on what to do. Often there are substantial losses because there is no proper security incident response.\nFlying a plane is very easy with well programmed regular procedures, but pilots are well paid because they are mainly for handling any unforeseen situation. We cannot programme what happens to a flight. A pilot need to know what to do.\n- Security alert / advisory delivery: how to discover emergent situation\n- Security investigation\n- Policy and procedure review\n\n## Secure Outsourcing\nOutsourcing risks:\n- Control of **confidential information**\n- Loss of **control** of business activities\n- **Accountability** - the organisation that outsources activities is still accountable for their activities and outcomes: still responsible for the error of contractors.\n\n## Data classification and protection\nComponents of a classification and protection program\n- Sensitivity levels: \"confidential\", \"restricted\", \"secret\", etc.\n- Marking procedures: how to indicate sensitivity on various forms of information\n- Access procedures\n- Handling procedures: e-mailing, faxing, mailing, printing, transmitting, destruction\n### Security Labels\nthree essential labels:\n- Security level\n- label's owner\n- data of expiration\nOnly with these three labels can we set a productive and functional system.\nIt tells the current position of a given document\n### Security Clearance\nSecurity clearance is the security label for user of data.\nSecurity clearance must be higher or equal to security label: the fundamental component of information security.\n\n## Certification and accreditation\nTwo-step process for the formal evaluation and approval for user of a system.\n- Certification: the process of evaluating a system against a set of formal standards, policies, or specifications.\n- Accreditation: the formal approval for the use of a certified system, for the defined period of time (and possibly other conditions).\n\n## Internal audit\n\n\n","tags":["Security"]},{"title":"Authentication with React Hook and React Component","url":"/2020/04/26/Authentication-with-React-Hook-and-React-Component/","content":"\n# Use React Context to manage 'global' state\nHere's the [React document](https://reactjs.org/docs/context.html)\nHere's the code for [AuthContext.js](https://github.com/tantigers/TripTime/blob/develop/src/spa/contexts/AuthContext.js).\n\n# User Provider and Consumer of Context\n[_app.js](https://github.com/tantigers/TripTime/blob/develop/src/spa/pages/_app.js) wraps children with `AuthProvider`\n\n# Use Effect hook to ensure a proper fetching happens \n> What does useEffect do? By using this Hook, you tell React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates. In this effect, we set the document title, but we could also perform data fetching or call some other imperative API.\nThe [effect hook](https://reactjs.org/docs/hooks-effect.html) lets you perform side effects in function components:\n\n# A loading spinner to make the loading process clear and nice\nI used the npm package [react-loading](https://www.npmjs.com/package/react-loading) to render a loading animation.\n\nThe component will have a state to decide whether to show the loading animation. When a blocking process is started or stopped, the state gets toggled.\nHave a look at [TopBar.js](https://github.com/tantigers/TripTime/blob/develop/src/spa/components/layout/TopBar.js) and you'll get what I mean.\n\nHere's the handler of Log Out button.\n```\nonClick={async () => {\n               this.setState(() => ({ loggingOut: true }));\n               await logoutHandler();\n               this.setState(() => ({ loggingOut: false }));\n             }}\n```\nHere's how `this.state.loggingOut` decides whether to show the loading animation:\n```\n<>\n        {this.state.loggingOut && (\n          <PageLoading message='TripTime is logging you out. See you soon:)' />\n        )}\n        <AuthContext.Consumer>\n              {/*the function to render the component*/}\n        </AuthContext.Consumer>\n</>\n```\n`this.state.loggingOut` has to be checked outside the consumer part as it needs `this` to be the `TopBar` instance.","tags":["Web Development","React","Javascript","TripTime"]},{"title":"My React Chat Box","url":"/2020/04/25/My-React-Chat-Box/","content":"\nYesterday I created the front-end of the ChatBox for TripTime :)\nHere's what it looks like:\n<img src=\"chatbox.gif\" width=\"900px\" alt=\"\"/>\n\nI met with a couple of challenges along the way, but happy to see how it turns out to look.\n# Positioning of the bubbles\nUsing the grid display to position the chat bubbles were my first challenge. Here's what the code looks like for the container of message bubbles, which takes up the 100% width of the chat box and takes care of the positioning of avatar, info and chat content:\n```css\n.chatMessageContainer, .myChatMessageContainer {\n    width: 100%;\n    display: grid;\n    padding: 1rem;\n    box-sizing: border-box;\n}\n.chatMessageContainer {\n    grid-template-columns: 2rem 1rem auto 1fr;\n    grid-template-rows: 1rem 1fr;\n    grid-template-areas:  'messageAvatar . messageInfo .' 'messageAvatar . messageContent .';\n}\n\n.myChatMessageContainer {\n    grid-template-columns: 1fr auto 1rem 2rem;\n    grid-template-rows: 1rem 1fr;\n    grid-template-areas: '. messageInfo . messageAvatar' '. messageContent . messageAvatar';\n}\n```\nHere's the grid structure of the containers:\n\n<img src=\"grid.png\" width=\"800px\" alt=\"grid\"/>\n\n<img src=\"grid-right.png\" width=\"800px\" alt=\"grid right\"/>\n\n## Note here\nFor `grie-template-columns`/`grie-template-rows`,\n- `auto` means the width of the child depends on itself.\n- `1fr` means the child takes up one fraction of whatever space is left.\nSo the empty space takes up `1fr`, and the bubble takes up `auto`.\n\n# The little triangle attached to the bubble\n\n<img src=\"triangle.png\" width=\"200px\" alt=\"triangle\"/>\nTo get this tiny triangle, I added `before` pseudo-element for the bubbles:\n\n```css\n.messageContent::before, .myMessageContent::before{\n    content: \" \";\n    height: 0;\n    position: absolute;\n    top: 0.2rem;\n    width: 0;\n    border: solid;\n}\n.messageContent::before{\n    left: -0.7rem;\n    border-width: 0.3rem 0.7rem 0.3rem 0;\n    border-color: transparent var(--trip-orange) transparent transparent;\n}\n.myMessageContent::before{\n    right: -0.7rem;\n    border-width: 0.3rem 0 0.3rem 0.7rem;\n    border-color: transparent transparent transparent var(--trip-gray) ;\n}\n```\n\n# Managing the width of the bubble \nThe width of the bubble were adjusted responsively.\nFirstly, for any screen size:\n```css\n.messageContent, .myMessageContent {\n    min-width: 10rem;\n}\n```\nThen for big screen: \n```css\n@media screen and (min-width: 1000px) {\n    .messageContent, .myMessageContent {\n        max-width: 50vw;\n    }\n}\n```\nFor small screen:\n```css\n@media screen and (max-width: 1000px) {\n    .messageContent, .myMessageContent {\n        max-width: 70vw;\n    }\n}\n```\nWhen the 70vw is bigger than the space available, it will just take up the whole space instead of flowing out, which is pretty handy.\n\n# Keep at the bottom of container\nTo keep the ChatBox showing the newest message, I used an npm package [react-scroll-to-bottom](https://www.npmjs.com/package/react-scroll-to-bottom):\n> React container that will auto scroll to bottom or top if new content is added and viewport is at the bottom, similar to tail -f. Otherwise, a \"jump to bottom\" button will be shown to allow user to quickly jump to bottom.\n\n```\nimport ScrollToBottom from 'react-scroll-to-bottom';\n\nexport default class ChatMessageList extends React.Component {\n  render() {\n    const messages = this.props.messages;\n    return (\n      <ScrollToBottom className={styles.messageListContainer}>\n        {messages.map((message, index) => (\n          <ChatMessage\n            chatMessage={message}\n            key={index}\n            isMine={message.author.id === this.props.userID}\n          />\n        ))}\n      </ScrollToBottom>\n    );\n  }\n}\n```\n\n# Handling incoming messages\nOnly the `ChatBox` component knows about the chat message list API.\n```\n// inside ChatBox class:\nhandleIncomingNewMessages(incomeMessages) {\n    this.setState(state => ({\n      newMessageNum: state.newMessageNum + incomeMessages.length,\n      messages: state.messages.concat(incomeMessages),\n    }));\n  } // This is the method to take care of API call for new message\n\n  handleMyNewMessage(myNewMessage) {\n    this.setState(state => ({\n      newMessageNum: 0,\n      messages: [...state.messages, myNewMessage],\n      // Also need to submit it to the backend here\n    }));\n  }\n```\n\nThe `ChatMessageList` will know about the current message list, and the user ID (so that it can tell between my messages and others):\n```\n<ChatMessageList\n              messages={this.state.messages}\n              userID={this.getCurrentUser().id}\n            />\n```\n\nThe `ChatInputForm` will know about how to handle the user's new message:\n``` \n<ChatInputForm\n                 newMessageHandler={this.handleMyNewMessage}\n                 me={this.getCurrentUser()}\n               />\n```\n\nThe code can be find [here](https://github.com/tantigers/TripTime/tree/develop/src/spa/components/chat).\nThe CSS module is [here](https://github.com/tantigers/TripTime/blob/develop/src/spa/css/chat-box.module.css).\n\n\nIt's fun to create a cutie like this, let's see how it works with WebSocket in the future:)\n\n","tags":["Web Development","React","Javascript","TripTime","CSS","Web Beautify"]},{"title":"Mock time in Jest","url":"/2020/04/24/Mock-time-in-Jest/","content":"\nLast time I wrote about my first time [using jest to test React components](https://niuniuanran.github.io/2020/04/21/Test-a-React-Component-with-Jest/). It looked nice when I tested locally but Travis CI was not happy with my snapshot.\n# Commit the snapshots!\nMy first reaction was checking if I should commit the snapshot files at all, or should I let Travis CI generate its own.\n[Jest documentation](https://jestjs.io/docs/en/snapshot-testing) gives a clear answer: yes.\nShould snapshot files be committed?\n > Yes, all snapshot files should be committed alongside the modules they are covering and their tests. They should be considered part of a test, similar to the value of any other assertion in Jest. In fact, snapshots represent the state of the source modules at any given point in time. In this way, when the source modules are modified, Jest can tell what changed from the previous version. It can also provide a lot of additional context during code review in which reviewers can study your changes better.\n\n# Mock the date\nNow if I look into the Travis CI logs:\n<img src=\"travis.png\" width=\"300px\"/>\nSo it's greeting the `Tester` at different time of the day...\nThere must be a way to mock a stable date for the testing!\n\nSo I found a piece of code from [here](https://github.com/facebook/jest/issues/2234) and added it before I render the user Dashboard:\n```javascript\n  const DATE_TO_USE = new Date(2020, 11, 17, 23, 24, 0);\n  const _Date = Date;\n  global.Date = jest.fn(() => DATE_TO_USE);\n  global.Date.UTC = _Date.UTC;\n  global.Date.parse = _Date.parse;\n  global.Date.now = _Date.now;\n```\nNow I get a stable goodnight.\n\n# So what is `jest.fn`\n> Mock functions are also known as \"spies\", because they let you spy on the behavior of a function that is called indirectly by some other code, rather than just testing the output. You can create a mock function with jest.fn().\n\n[Here's the jest doc on mock-functions](https://jestjs.io/docs/en/mock-functions.html)\n\n\n","tags":["Web Development","Javascript","TripTime","Test Driven Development","Jest"]},{"title":"Architecting for the Cloud: AWS Best Practices (Whitepaper reading notes)","url":"/2020/04/23/Architecting-for-the-Cloud-AWS-Best-Practices-Whitepaper-reading-notes/","content":"[Architecting for the Cloud: AWS Best Practices](https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf) is the whitepaper recommended to read for AWS Solution Architect Associate Certificate Test.\n\n# AWS Cloud Design Principles\n# Scalability\nThere are generally two ways to scale an IT architecture: vertically and horizontally.\n## Scaling Vertically vs Scaling Horizontally\n### Scaling Vertically\nIncrease the specification of an individual resource. For example, upgrading a server with larger hard drive or better I/O capacity.\nNot always cost-effective.\nWill reach the limit.\nNot always highly available.\nEasy to implement and can be sufficient for many user cases especially in the short ter,.\n### Scaling Horizontally\nScaling horizontally takes place through an increase in the number of resources, such as adding more hard drives to a storage array or adding more servers to support an application.\nA great way to build internet-scale applications that leverage the elasticity of cloud computing.\n\nNow let's examine some possible horizontal scaling scenarios.\n## Stateless Applications\nA stateless application is an application that does not need knowledge of previous interactions and does not store session information: given the same input, produces the same response to any end user.\nStateless Applications can be scaled horizontally:\n- Any of the available compute resource (EC2/Lambda) can serve the request.\n- Each resource do not need to be aware of their peers: all that is required is a way to **distribute workload** to them.\n### Distribute Load to Multiple Nodes\nTo distribute the workload to multiple nodes, you can use either a push or pull model.\n#### push model\nWith a push model, you can use **Elastic Load Balancing** to distribute the workload. \n- A  Network Load Balancer operates at level 4 of the open systems interconnection model.\n- WIth container-based services, you can also use Application Load Balancer.\n- You can use Amazon Route 53 to implement a DNS round robin.\nIn this case, DNS responses return an IP address from a list of valid hosts in a round-robin fashion. While easy to use, this approach does not always work with with the elasticity of cloud computing, because even if you can set low time to live (TTL) values for your DNS records, caching DNS resolvers are outside the control of Amazon Route 53 and might not always respect your settings.\n#### pull model\nInstead of a load balancing solution, you can implement a pull model for asynchronous, event-driven workloads.\nIn a pull model, tasks that need to be performed or data that needs to be processed can be stored as messages in a queue using Amazon Simple Queue Service (Amazon SQS) or as a streaming data solution such as Amazon Kinesis. \nMultiple compute resources can then pull and consume those messages, processing them in a distributed fashion.\n\n## Stateless Component\nIn practice, most applications maintain some kind of state information. You can still make a portion of your architecture stateless by not storing anything that needs to persist to more than a single request in the local file system.\n### Using HTTP cookies to store session information\nConsider only storing a unique session identifier in an HTTP cookie, and store more detailed session information on the server side. Most programming platforms provide a native session management mechanism.\n### Server side session storage\nUser session information is often stored on the local file system by default, and result in a stateful architecture.\n- A common solution is to store information in database. **Amazon DynamoDB is a great choice** to store the server side user session information.\n- When larger files is required to be stored, you can place those files in a **shared storage layer**, such as Amazon S3 or Amazon EFS, and you can still avoid the introduction of stateful components, i.e. the workload can be picked up by another EC2 instance when needed.\n### Multi-step workflow\nYou can use **AWS Step Functions** to centrally store execution history and make these workloads stateless.\n\n## Stateful Components\nExamples of inevitably stateful components:\n- database\n- many legacy applications were designed to run on a single server by relying on local compote resources\n- User cases that require client devices to maintain a connection to a specific server for prolonged periods, for example: real-time multiplayer gaming, much easier to achieve a non-distributed implementation where participants are connected to the same server.\nYou might still be able to scale those components horizontally by distributing the load to multiple nodes with **session affinity**: you **bind all the transactions of a session to a specific compute resource**.\n### Session Affinity\nsession affinity(会话保持)是在负载均衡器上的一种机制，在完成负载均衡任务的同时，还负责**一系列相关连的访问请求会分配到一台服务器上**｡\n#### Limitations of Session Affinity\n- Existing sessions won't benefit from the introduction of newly launched compute nodes\n- If a compute node is terminated, users bound to them will lose their session-specific data.\n#### Implement Session Affinity Option 1\nFor HTTP/HTTPS traffic, you can use sticky session feature of an **Application Load Balancer** to bind a user's session to specific instance. \nWith this feature, an Application Load Balancer will try to use the same server for that user for the duration of the session.\n\n## Use client-side load balancing\nIn this model, the clients need a way of discovering valid server endpoints to directly connect to. You can use DNS for that, or you can build a simple discovery API to provide that information to the software running on the client.\nHealth checking mechanism also needs to be implemented on the client side.\n\n## Distributed Processing\nBy dividing a task and its data into many small fragments of work, you can execute them in parallel across a set of compute resources.\n- Offline batch jobs can be horizontally scaled by using distributed data processing engines such as **AWS Batch, AWS Glue, and Apache Hadoop**.\n- On AWS, you can use **Amazon EMR(Amazon Elastic MapReduce)** to run Hadoop workloads on top of a fleet of EC2 instances without the operational complexity. \n- For real-time processing of streaming data, **Amazon Kinesis** partitions data in multiple shards that can then be consumed by multiple Amazon EC2 or AWS Lambda resources to achieve elasticity.\n\n# Disposable Resources instead of Fixed Servers\nWhen designing for AWS, you can take advantage of the dynamically provisioned nature of cloud computing.\nYou can think of servers and other components as temporary resources.\n## Immutable infrastructure pattern \nIt solves the **configuration drift** issue with fixed, long running servers.\n> Configuration drift: Changes and software patches applied through time can result in untested and heterogeneous configurations across different environments\n\nImmutable infrastructure pattern: a server—once launched—is never updated. Instead, when there is a problem or need for an update, the problem server is replaced with a new server that has the latest configuration.\nThis enables resources to always be in a consistent (and tested) state, and makes rollbacks easier to perform. This is more easily supported with stateless architectures.\n\n## Instantiating Compute Resources\nMake the creation and configuration of new compute note/other components an automated and repeatable process, so that you can dispose resources and launch new ones swiftly.\n### Bootstrapping\nExecute automated bootstrapping actions: scripts that install software/copy data to bring that resource to a particular state.\n- Set up EC2 instances with user data scripts and cloud-init directives\n- Script and configuration management tools such as **Chef** or **Puppet**\n- With custom scripts and the AWS APIs, or with AWS CloudFormation support for AWS Lambda-backed custom resources, you can write provisioning logic that acts on almost any AWS resource.\n### Golden Image\nGolden image is a snapshot of a particular state of the resource.\nResources can be launched from a golder image:\n- EC2 instances (AMI)\n- Amazon RDS DB instances (instantiating it from a Amazon RDS snapshot)\n- Amazon Elastic BLock Store (Amazon EBS) volumes (EBS snapshot)\nWhen compared to the bootstrapping approach, a golden image results in faster start times and removes dependencies to configuration services or third-party repositories. **This is important in auto-scaled environments where you want to be able to quickly and reliably launch additional resources as a response to demand changes.**\n\n### Containers\n#### Docker\nDocker allows you to package a piece of software in a Docker image, which is a standarised unit for software development.\nThese services allow you to deploy and manage multiple containers across a cluster of EC2 instances:\n- AWS Elastic Beanstalk\n- Amazon Elastic Container Service (ECS)\n- AWS Fargate\nYou can build **golden Docker images** and use the ECS Container  \n#### Kubernetes and Amazon EKS\nWith Kubernetes and Amazon Elastic Container Service for Kubernetes, you can easily deploy, manage and scale containerized applications.\n\n### Hybrid\nYou can use a combination of Golden Images and Bootstrapping.\nItems that do not change often or that introduce external dependencies will typically be part of your golden image. An example of a good candidate is your web server software that would otherwise have to be downloaded by a third-party repository each time you launch an instance.\nItems that change often or differ between your various environments can be set up dynamically through bootstrapping actions.\n\nElastic Beanstalk follows the Hybrid model. It provides preconfigured run time environments - each initiated from its AMI, but allows you to run bootstrap actions through .ebextensions configuration files, and configure environmental variables to parameterize the environment differences.\n\n## Infrastructure as Code\nApplications of the disposable resources principles are not limited to the individual resource level. AWS assets are programmable, and you can use code to make your whole infrastructure reusable, maintainable, extensible, and testable:\n### AWS CloudFormation\nAWS CloudFormation templates give you an easy way to create and manage a collection of related AWS resources.\n**Your CloudFormation templates can live with your application in your version control repository, which allows you to reuse architectures and reliably clone production environments for testing.**\n\n# Automation\nConsider introducing one or more of these types of automation into your application architecture to ensure more resiliency, scalability, and performance.\n## Serverless Management and Deployment\nDeployment pipeline\n- AWS CodePipeline\n- AWS CodeBuild\n- AWS CodeDeploy\n\n## Infrastructure Management and Deployment\n- AWS Elastic Beanstalk\n- Amazon EC2 auto recovery: create an Amazon CloudWatch alarm that monitors an EC2 instance and automatically recovers it if it becomes impaired. (but you lose your in-memory data)\n- AWS Systems Manager: automatically collect software inventory, apply OS patches, create a system image to configure Windows and Linux operating systems\n- Auto Scaling: maintain application availability and scale your Amazon EC2, Amazon DynamoDB, Amazon ECS, Amazon Elastic Container Service for Kubernetes (Amazon EKS) capacity.\nYou can use Auto Scaling to help make sure that you are running the desired number of **healthy** EC2 instances across multiple Availability Zones.\n\n## Alarms and Events\n- Amazon CloudWatch alarms -> Amazon Simple Notification Service\n- Amazon CloudWatch events: a near real-time stream of system events that describe changes in AWS resources. You can route events to one/more targets, such as\n    - Lambda Functions\n    - Kinesis Streams\n    - SNS topic\n- AWS Lambda scheduled events: configure a Lambda function that execute on a regular schedule.\n- AWS WAF security automations: you can administer AWS Web Application Firewall completely through APIs, which makes security automation easy, enabling rapid rule propagation and fast incident response.\n\n# Loose Coupling\nAs application complexity increases, a desirable attribute of an IT system is that it can be broken into smaller, loosely coupled components.\nReduce dependencies.\nA change or a failure in one component should not cascade to other components.\n## Well-Defined Interfaces\nComponents interact only through specific, technology-agnostic（技术不可知） interfaces, such as RESTful APIs.\n### microservices architecture\nThis granular design pattern of decoupling components with interfaces is commonly referred to as a **microservices architecture**.\n### Amazon API Gateway\nA fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\n- traffic management, \n- authorization and access control, \n- monitoring, and\n- API version management.\n\n## Service Discovery\n**Loose coupling is a crucial element if you want to take advantage of the elasticity of cloud computing, where new resources can be launched or terminated at any point in time.**\nThose compute resources running smaller services need a way to be addressed/discovered.\n### Elastic Load Balancing\neach load balancer gets its own hostname, you can consume a service through a stable endpoint.\n### DNS and private Amazon Route 53 zones\nThe particular load balancer’s endpoint can be abstracted and modified at any time.\n### Amazon Route 53 auto naming\nAuto naming lets you automatically create DNS records based on a configuration you define.\n\n## Asynchronous Integration\nSuitable for any interaction that does not need an immediate response and where an acknowledgement that a request has been registered will suffice.\nIt involves \n- one component that generates events \n- another component that consumes them\n\nThe two components do not integrate through direct point-to-point interaction, but usually through an intermediate durable storage layer, such as:\n - an SQS(Simple Queueing Service) queue \n - a streaming data platform such as Amazon Kinesis\n - Cascading Lambda events\n - AWS Step functions\n - Amazon Simple Workflow Service\n \n> This approach decouples the two components and introduces additional resiliency. So, for example, if a process that is reading messages from the queue fails, messages can still be added to the queue and processed when the system recovers. \n\n\n## Distributed Systems Best Practices\n**build applications that handle component failure in a graceful manner.**\n\nA request that fails can be retried with an exponential backoff and Jitter strategy, or it can be stored in a queue for later processing. For front-end interfaces, it might be possible to provide alternative or cached content instead of failing completely when, for example, your database server becomes unavailable.\n\nThe **Amazon Route 53 DNS failover feature** also gives you the ability to monitor your website and automatically route your visitors to a backup site if your primary site becomes unavailable. You can host your backup site as a static website on Amazon S3 or as a separate dynamic environment.\n\n# Services, Not Servers\n\n## Managed Services\nAWS managed services provide building blocks that developers can consume to power their applications. Examples of managed services that power your applications:\n- SQS for messaging cluster\n- S3 for storage of as much data as you need (5T each individual object)\n- Amazon CloudFront for content delivery\n- ELB for load balancing\n- Amazon DynamoDB for NoSQL databased\n- Amazon CloudSearch for search workloads\n- Amazon Elastic Transcoder for video encoding\n- Amazon Simple Email Service (SES) for sending and receiving emails\n\n## Serverless Architectures \nIt is possible to build both event-driven and synchronous services for mobile, web, analytics, CDN business logic, and IoT without managing any server infrastructure.\n### Create a serverless application \n- By using **Amazon API Gateway**, you can develop virtually infinitely scalable synchronous APIs powered by **AWS Lambda**. \n- When combined with **Amazon S3** for serving static content assets, this pattern can deliver a complete web application.\n### Authentication and Access Control\n- You can use **Amazon Cognito** so that you don't have to manage a back-end solution to handle user authentication, network state, storage, and sync.\n- Amazon Cognito provides temporary AWS credentials to your users, allowing the mobile application running on the device to interact directly with **IAM**-protected AWS services.\n### AWS IoT\n- AWS IoT provides a fully managed device gateway that scales automatically with your usage without any operational overhead.\n### Responsive services at edge locations\n**AWS Lambda@Edge** lets you run Lambda functions at **Amazon CloudFront edge locations** in response to CloudFront events.\n### Data Analytics\n-  **Amazon Athena** is an interactive query service that makes it easy for you to analyze data in Amazon S3 using standard SQL. Serverless, you pay for the queries that you run.\n\n# Databases\nOn AWS, constraints to choose database technologies are removed.\n## Relational Database\n### Scalability \n- Scale vertically by upgrading to a larger Amazon RDS DB instances; also consider using Amazon Aurora, supports higher throughput.\n- For read-heavy applications, you can also scale horizontally by adding **read replicas**.\n> Read Replicas separate database instances that are replicated asynchronously. As a result, they are subject to replication lag and might be missing some of the latest transactions.\n> Application designers need to consider which queries have tolerance to slightly stale data. Those queries can be executed on a read replica, while the remainder should run on the primary node. Read replicas can also not accept any write queries.\n\n- Relational database workloads that need to scale their **write** capacity beyond the constraints of a single DB instance require a different approach called data partitioning or sharding.\n> The application’s data access layer needs to be modified to have awareness of how data is split so that it can direct queries to the right instance. In addition, schema changes must be performed across multiple database schemas, so it is worth investing some effort to automate this process.\n\n### High Availability\n\n- Direct Connection(DX) is highly available when there are two ports open.  \n- ElastiCache is highly available when you group 2 to 6 nodes into a cluster with replicas where 1 to 5 read-only nodes contain replicate data of the group's single read/write primary node.\n- RedShift is highly available when you build multi-region or multi-availability zone (AZ) clusters\n\nFor any production relational database, we recommend using the **Amazon RDS MultiAZ deployment** feature, which creates a synchronously replicated standby instance in a different Availability Zone.\n> Resilient applications can be designed for **Graceful Failure** by offering reduced functionality, such as read-only mode by using read replicas.\n\n### Anti-Patterns\nConsider a NoSQL database if:\n- Your application primarily indexes and queries data with no need for joins or complex transactions\n- If you expect a write throughput beyond the constraints of a single instance\nIf you have large binary files:\n- Consider using Amazon S3 to hold the actual files\n- Only hold the metadata for the files in your database.\n\n## NoSQL Databases\nNoSQL databases trade some of the query and transaction capabilities of relational databases for a more flexible data model that seamlessly scales horizontally.\n> Amazon DynamoDB is a fast and flexible NoSQL database service for applications that need consistent, single-digit, millisecond latency at any scale. It is a fully managed cloud database and supports both **document** and **key-value store** models.\n\n### Scalability\n> NoSQL database engines will typically perform data partitioning and replication to scale both the reads and the writes in a horizontal fashion. They do this transparently, and don’t need the data partitioning logic implemented in the data access layer of your application.\n- Amazon Dynamo DB manages **table partitioning** automatically, adding new partitions as your table grows in size or read-provisioned and write-provisioned capacity changes.\n- Amazon DynamoDB Accelerator (DAX) is a managed, highly available, in-memory cache for DynamoDB to leverage significant performance improvements.\n### High Availability\n- Amazon DynamoDB synchronously replicates data across **three** facilitates in an AWS Region.\n- **Global Tables** are replicated across your selected AWS Regions.\n### Anti-patterns\n- If your schema cannot be denormalized, and your application requires joins or complex transactions, consider a relational database instead.\n- If you have large binary files (audio, video, and image), consider storing the files in Amazon S3 and storing the metadata for the files in your database.\n\n## Data Warehouse\nA data warehouse is a specialized type of **relational database**, which is optimized for analysis and reporting of large amounts of data.\n> It can be used to combine transactional data from disparate sources (such as user behavior in a web application, data from your finance and billing system, or customer relationship management or CRM) to make them available for analysis and decision-making.\n\nOn AWS, you can leverage **Amazon Redshift**, a managed data warehouse service that is designed to operate at less than a tenth the cost of traditional solutions\n\n### Scalability\nAmazon Redshift achieves efficient storage and optimum query performance through a combination of \n- massively parallel processing (MPP), \n- columnar data storage, and\n- targeted data compression encoding schemas.\n\nThe **Amazon Redshift MPP architecture** enables you to increase performance by increasing the number of nodes in your data warehouse cluster. \n**Amazon Redshift Spectrum** enables Amazon Redshift SQL queries against exabytes of data in Amazon S3\n\n### High Availability\n- We recommend that you deploy production workloads in **multi-node clusters**, so that data that is written to a node is automatically replicated to other nodes within the cluster.\n- Data is also continuously backed up to Amazon S3.\n- Amazon Redshift continuously monitors the health of the cluster and automatically rereplicates data from failed drives and replaces nodes as necessary.\n\n### Anti-patterns\nBecause Amazon Redshift is an SQL-based relational database management system (RDBMS), it is compatible with other RDBMS applications and business intelligence tools.\nAlthough Amazon Redshift provides the functionality of a typical RDBMS, including online transaction processing (OLTP) functions, it is not designed for these workloads. If you expect a high concurrency workload that generally involves reading and writing all of the columns for a small number of records at a time, you should instead consider using Amazon RDS or Amazon DynamoDB.\n\n## Search\n> A query is a formal database query, which is addressed in formal terms to a specific data set. Search enables datasets to be queried that are not precisely structured. \n> For this reason, applications that require sophisticated search functionality will typically outgrow the capabilities of relational or NoSQL databases.\n\nOn AWS, you can choose between **Amazon CloudSearch** and **Amazon Elasticsearch Service (Amazon ES)**\n- Amazon CloudSearch is a managed service that requires little configuration and will scale automatically.\n- Amazon Elasticsearch offers an open-source API and gives you more control over the configuration details. Amazon ES has also evolved to become more than just a search solution. Often used as an analytics engine for use cases such as:\n    - log analytics\n    - real-time application monitoring\n    - click stream analytics.\n\n### Scalability \nBoth Amazon CloudSearch and Amazon ES use **data partitioning** and **replication** to scale horizontally.\n\nThe difference is that with **Amazon CloudSearch**, you don’t need to worry about how many partitions and replicas you need because the service automatically handles that.\n### High Availability \nBoth Amazon CloudSearch and Amazon ES include features that store data redundantly across **Availability Zones**.\n\n## Graph Databases\nA graph database uses **graph structures for queries**. \n- A graph is defined as consisting of **edges (relationships)**, which directly relate to nodes (data entities) in the store. \n- The relationships enable data in the store to be linked together directly, which allows the fast retrieval of hierarchical structures in relational systems.\n- Graph databases are purposefully built in user cases like:\n    - social networking\n    - recommendation engines\n    - fraud detection\n**Amazon Neptune** is a fully managed graph database service.\n### Scalability\nAmazon Neptune is a purpose-built, high-performance graph database optimized for processing graph queries.\n### High Availability\nAmazon Neptune has:\n- read replicas\n- point-in-time recovery\n- continuous backup to Amazon S3\n- replication across Availability Zones\n- support for encryption at rest and in transit\n\n\n# Managing Increasing Volumes of Data\n> A data lake is an architectural approach that allows you to store massive amounts of data in a central location so that it's readily available to be categorized, processed, analyzed, and consumed by diverse groups within your organization.\n\n> Since data can be stored as-is, you do not have to convert it to a predefined schema, and you no longer need to know what questions to ask about your data beforehand. This enables you to select the correct technology to meet your specific analytical requirements.\n\n<img src=\"data-lake.png\" width=\"800px\" alt=\"aws data lake solution\"/>\n\n> **AWS Glue** is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. Useful for data lakes where data in the central lake gets extracted, transformed and loaded.\n\n# Removing Single Points of Failure (High Availability)\n> Production systems typically come with defined or implicit objectives for **uptime**. \n> A system is highly available when it can **withstand the failure** of an individual component\n  or multiple components, such as hard disks, servers, and network links. \n\nTo attain high availability:\n- automate recovery\n- reduce disruption at every layer of architecture\n\n## Introducing Redundancy\nRemove single points of failure with redundancy: have multiple resources for the same task.\n### Standby Redundancy\nIn standby redundancy, when a resource fails, functionality is recovered on a secondary resource with the failover process. \nFailover process requires some time. During the failover process the resource remains unavailable. The secondary resource can either be:\n- launched automatically only when needed (reduce cost)\n- running idle (to accelerate failover and minimize disruption)\n\nStandby redundancy is often used for stateful components such as relational databases.\n\n### Active Redundancy\nRequests are distributed to multiple redundant compute resources. When one of them fails, the rest can simply absorb a larger share of the workload. Compared to standby redundancy, active redundancy can achieve better usage and affect a smaller population when there is a failure.\n\n## Detect Failure\nYou should aim to build as much **automation** as possible in both detecting and reacting to failure.\n- You can use services such as **ELB** and **Amazon Route 53** to configure **health checks** and **mask failure by routing traffic to healthy endpoints**.\n- You can replace unhealthy nodes automatically using **Auto Scaling** or by using the **Amazon EC2 auto-recovery feature** or services such as **AWS Elastic Beanstalk**.\n\n## Design Good Health Checks\n- In a typical three-tier application, you configure health checks on **ELB**. Design your health checks with the objective of reliably assessing the health of the back-end nodes.\n    - Not a simple TCP health check: won't be able to tell if the instance itself is healthy but the web server process has crashed. Instead, you should assess whether the web server can return an HTTP 200 response for some simple request. \n    - Not a deep health check (a test that depends on other layers of your application) at this layer. \n    - A layered approach is often the best.\n- A deep health check might be appropriate at the **Amazon Route 53** level.\n    - By running a more holistic check that determines if that environment is able to actually provide the required functionality, you can configure Amazon Route 53 to failover to a static version of your website until your database is up and running again.\n\n## Durable Data Storage\nData replication is the technique that introduces redundant copies of data. It can happen in a few different modes.\n### Synchronous replication \nOnly acknowledges a transaction after it has been durably stored in both the primary location and its replicas. \n- Synchronous replication redundantly stores all updates to your data.\n- For Amazon S3 objects, you can use **versioning** to preserve, retrieve, and restore their versions.\n### Asynchronous Replication\nDecouples the primary node from its replicas at the expense of introducing replication lag:  changes on the primary node are not immediately reflected on its replicas.\n- Asynchronous replicas are used to horizontally scale the system’s read capacity for queries that can tolerate that replication lag. \n-  It can also be used to increase data durability when some loss of recent transactions can be tolerated during a failover.  For example, you can maintain an asynchronous replica of a database in a separate AWS Region as a disaster recovery solution.\n\n### Quorum-based replication\nCombines synchronous and asynchronous replication.\nReplication to multiple nodes can be managed by **defining a minimum number of nodes that must participate in a successful write operation**. \n\n### RPO and RTO\nIt is important to understand where each technology you are using fits in these data storage models. Their behavior during various failover or backup/restore scenarios should align to your **recovery point objective (RPO)** and your **recovery time objective (RTO)**.\n\n## Automated Multi-Data Centre Resilience\n### Infrastructure\nEach AWS Region contains multiple distinct locations, or Availability Zones. Each Availability Zone is engineered to be independent from failures in other Availability Zones. An Availability Zone is a data center, and in some cases, an Availability Zone consists of multiple data centers. Availability Zones within a Region provide inexpensive, low-latency network connectivity to other zones in the same Region. This allows you to replicate your data across data centers in a synchronous manner so that failover can be automated and be transparent for your users.\n### Active Redundancy\n- A fleet of application servers can be distributed across multiple Availability Zones and be attached to ELB. When the EC2 instances of a particular Availability Zone fail their health checks, ELB stops sending traffic to those nodes.\n- AWS Auto Scaling ensures that the correct number of EC2 instances are available to run your application, launching and terminating instances based on demand and defined by your scaling policies.\n### AWS services that are inherently designed according to the multiple Availability Zone (multi-AZ) principle:\n- Amazon RDS\n- Amazon S3 (except for Amazon S3 One Zone-Infrequent Access)\n- Amazon DynamoDB synchronously replicates data across **three** facilitates in an AWS Region.\n\n## Fault Isolation and Traditional Horizontal Scaling\nWhat if every instance is affected?\n> If a particular request happens to trigger a bug that causes the system to fail over, then the caller may trigger a cascading failure by repeatedly trying the same request against all instances. You need to ISOLATE the fault.\n\n### Shuffle Sharding\nFor example, if you have eight instances for your service, you might create four shards of two instances each (two instances for some redundancy within each shard) and distribute each customer to a specific shard. \nYou can thus reduce the impact on customers in direct proportion to the number of shards you have.\n> Route 53 Infima’s Shuffle Sharding takes this pattern of rapidly diminishing likelihood for an increasing number of matches.\n\nYou get many shard combinations if each shard randomly pick a certain \"hand\" of instances. \nThere will be instance overlaps in the shuffled shards, but we can make the client fault tolerance:\nBy having simple retry logic in the client that causes it to try every endpoint in a Shuffle Shard, until one succeeds, we get a dramatic bulkhead effect.\n > [The Bulkhead pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead) is a type of application design that is tolerant of failure. In a bulkhead architecture, elements of an application are isolated into pools so that if one fails, the others will continue to function.\n\n# Optimize for Cost\n## Right Sizing\nChoose the right instance types for:\n- Amazon EC2\n- Amazon RDS\n- Amazon Redshift\n- Amazon ES\nChoose the right storage solution:\n- Select the right Amazon S3 storage class\n- Select the right EBS volume type (magnetic, general purpose SSD, provisioned IOSP SSD) for:\n    - Amazon EC2\n    - Amazon RDS\n    - Amazon ES\nAWS provides tools to help you identify cost-saving opportunities and keep your resources right-sized:\n- Cost Explorer (graphs)\n- AWS Budgets (alerts)\n- AWS Cost and Usage Reports (detailed line items)\n- Map your AWS costs and usage into meaningful categories with [Cost Categories](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html)\n- [Cost Allocation Tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html): After you or AWS applies tags to your AWS resources (such as Amazon EC2 instances or Amazon S3 buckets) and you activate the tags in the Billing and Cost Management console, AWS generates a cost allocation report as a comma-separated value (CSV file) with your usage and costs grouped by your active tags. \n- [AWS Price List API and AWS Price List Service API](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/price-changes.html) lets you query prices of AWS services via JSON(AWS Price List Service API) or HTML(AWS Price List API). You can also subscribe to Amazon Simple Notification Service (Amazon SNS) notifications to get alerts when prices for the services change. \n- Logging Billing and Cost Management API calls with AWS CloudTrail\n\n## Elasticity\n### Auto Scaling\n Plan to implement Auto Scaling for as many Amazon EC2 workloads as possible, so that you horizontally scale up when needed and scale down and automatically reduce your spending when you don’t need that capacity anymore.\n### Automate turning off non-productive workloads\nUsing Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances. \n### Consider which compute workloads you could implement on AWS Lambda\n so that you never pay for idle or redundant resources.\n### Replace EC2 workload with AWS managed services\nWhere possible, replace Amazon EC2 workloads with AWS managed services that either don’t require you to make any capacity decisions. such as:\n- ELB (elastic load balancing)\n- Amazon CloudFront \n- Amazon SQS, (simple queue service)\n- Amazon Kinesis Firehose (reliably load streaming data into data lakes, data stores and analytics tools)\n- AWS Lambda, \n- Amazon SES, (simple email service)\n- Amazon CloudSearch\n- Amazon EFS (elastic file storage)\n\nOr AWS managed services that enable you to easily modify capacity as and when need, such as:\n- Amazon DynamoDB\n- Amazon RDS\n- Amazon ES\n\n## Take Advantage of the Variety of Purchasing Options\n### Reserved Instances\nIdeal for applications with predictable minimum capacity requirements. \nYou can take advantage of tools such as **AWS Trusted Advisor** or **Amazon EC2 usage reports** to identify the compute resources that you use most often and that you should consider reserving.\nOther services have reserved capacity options too:\n- Amazon Redshift,\n- Amazon RDS\n- Amazon DynamoDB\n- Amazon CloudFront\n### Spot Instances\nThe hourly price for a Spot instance (of each instance type in each Availability Zone) is set by Amazon EC2, and adjusted gradually based on the long-term supply of, and demand for, Spot instances.\nYour Spot instance runs whenever capacity is available and the maximum price per hour for your request (bid price) exceeds the **Spot price**. \n\nSee [here](https://niuniuanran.github.io/2020/04/20/AWS-Billing-and-Pricing/#Spot-instance-pricing) for details for spot instance pricing when interrupted.\n\n# Caching\nCaching is a technique that stores previously calculated data for future use.\n## Application Data Caching\nApplications can be designed so that they store and retrieve information from fast, managed, in-memory caches.\n- **Amazon ElasticCache** is a web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud. Supports two in-memory cache engines:\n    - Memcached\n    - Redis\n- **Amazon DynamoDB Accelerator(DAX) is a fully managed, highly available, in-memory cache for DynamoDB for high through-put. \nDAX adds in-memory acceleration to your DynamoDB tables without requiring you to manage cache invalidation, data population, or cluster management.\n## Edge Caching\nCopies of static content (images, CSS files, or streaming pre-recorded video) and dynamic content (responsive HTML, live video) can be cached at an **Amazon CloudFront edge location**, which is a CDN with multiple points of presence around the world.\n\n# Security\n## Use AWS features for Defense in Depth\n- You can build a VPC topology that isolates parts of the infrastructure through the use of subnets, security groups, and routing controls.\n- Services like AWS WAF, a web application firewall, can help protect your web applications from SQL injection and other vulnerabilities in your application code.\n- For access control, you can use IAM to define a granular set of policies and assign them to users, groups, and AWS resources.\n- The AWS Cloud offers many options to protect your data, whether it is in transit or at rest with encryption.\n\n## Share Security Responsibility with AWS\n- AWS is responsible for the security of the underlying cloud infrastructure\n- you are responsible for securing the workloads you deploy in AWS. \nFor example, when you use services such as Amazon RDS and Amazon ElastiCache, security patches are applied automatically to your configuration settings. This not only reduces operational overhead for your team, but it could also reduce your exposure to vulnerabilities.\n\n## Reduce Privileged Access\n\n- In a traditional environment, service accounts are often assigned long-term credentials that are stored in a configuration file. On AWS, you can instead use **IAM roles** to grant permissions to applications running on EC2 instances through the use of short-term credentials, which are automatically distributed and rotated.\n\n- For mobile applications, you can use **Amazon Cognito** to allow client devices to access AWS resources through temporary tokens with fine-grained permissions.\n \n ## Security as Code\n \n- **Golden Environment**: capture security frameworks, regulations, and organizational policies in a template. This template is used by AWS CloudFormation and deploys your resources in alignment with your security policy. You can reuse security best practices among multiple projects, as a part of your continuous integration pipeline.\n\n- For greater control and security, AWS CloudFormation templates can be imported as products into **AWS Service Catalog**. This allows you to centrally manage your resources to support consistent governance, security, and compliance requirements, while enabling your users to quickly deploy only the approved IT services they need.\n\n## Real-time Auditing\nOn AWS, you can implement continuous monitoring and automation of controls to minimize exposure to security risks. Services to provide tis real-time auditing:\n- AWS Config\n- Amazon Inspector\n- AWS Trusted Advisor\n\nWith **AWS Config** rules you also know if a resource was out of compliance even for a brief period of time, making both point-in-time and period-in-time audits very effective.\n\nYou can implement extensive logging for your applications (using **Amazon CloudWatch Logs**) and for the actual AWS API calls by enabling **AWS CloudTrail**\n\nAWS CloudTrail is a web service that records API calls to supported AWS services in your AWS account and delivers a log file to your S3 bucket.\n\nYou can use **AWS Lambda, Amazon EMR (Amazon Elastic MapReduce), Amazon ES, Amazon Athena**, or third-party tools from AWS Marketplace to scan log data to detect events such as unused permissions, privileged account overuse, key usage, anomalous logins, policy violations, and system abuse.\n\n\n\n\n\n\n\n\n","tags":["AWS","AWS SAA Test"]},{"title":"What is ES6 proxy?","url":"/2020/04/21/What-is-ES6-proxy/","content":"I got this question when I was [testing a React component with Jest](https://niuniuanran.github.io/2020/04/21/Test-a-React-Component-with-Jest/), and Jest yelled at me that it could not deal with the imported CSS module (which is managed by webpack). The problem was solved with `object-identity-proxy`.\nHere's the description of `object-identity-proxy` from their [npm page](https://www.npmjs.com/package/identity-obj-proxy):\n> An identity object using ES6 proxies. Useful for testing trivial webpack imports. For instance, you can tell Jest to mock this object as imported CSS modules; then all your className lookups on the imported styles object will be returned as-is.\n\nSo what is an ES6 proxy?\nHere's [what MDN says](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy):\n> The Proxy object is used to define custom behavior for **fundamental operations (e.g. property lookup, assignment, enumeration, function invocation, etc)**.\n\n# Syntax\n```javascript\nconst p = new Proxy(target, handler)\n```\n# Parameters\n`target`:\nA target object to wrap with Proxy. It can be any sort of object, including a native array, a function, or even another proxy.\n`handler`:\nAn object whose properties are functions define the behavior of proxy p when an operation is performed on it.\n\n# Operations that could be trapped with handler\nMDN listed all the handler object methods, i.e. fundamental operations that could be traps by handler:\n- `handler.getPrototypeOf()`\nA trap for Object.getPrototypeOf.\n- `handler.setPrototypeOf()`\nA trap for Object.setPrototypeOf.\n- `handler.isExtensible()`\nA trap for Object.isExtensible.\n- `handler.preventExtensions()`\nA trap for Object.preventExtensions.\n- `handler.getOwnPropertyDescriptor()`\nA trap for Object.getOwnPropertyDescriptor.\n- `handler.defineProperty()`\nA trap for Object.defineProperty.\n- `handler.has()`\nA trap for the in operator.\n- `handler.get()`\nA trap for getting property values.\n- `handler.set()`\nA trap for setting property values.\n- `handler.deleteProperty()`\nA trap for the delete operator.\n- `handler.ownKeys()`\nA trap for Object.getOwnPropertyNames and Object.getOwnPropertySymbols.\n- `handler.apply()`\nA trap for a **function call**.\n- `handler.construct()`\nA trap for the `new` operator.\n\n# Now check the `object-identity-proxy`\nHere's the code I found in `node_modules/identity-obj-proxy/src/index.js`:\n```javascript\nidObj = new Proxy({}, {\n  get: function getter(target, key) {\n    if (key === '__esModule') {\n      return false;\n    }\n    return key;\n  }\n});\n```\nSo this is what they mean by 'returned as-is'! \n\n# Write my first two simple proxies\nNow I'll write two simple proxies to practice.\n## Default Dictionary\n```javascript\nconst myDictionary={dog: \"woof\", cat:\"meow\", duck:\"quack\"};\nconst myDefaultDictionary = new Proxy(myDictionary, {\nget: function getter(target, key) {\n    return key in target?\n            target[key]:\"hello\";\n},\nset: function setter(obj, prop, value) {\n    if(value.length < 4) obj[prop]=value+'...';\n    else obj[prop]=value;\n}\n})\n```\n\nNow I can get and set the animal sounds:\n```\nconsole.log(myDefaultDictionary.cat); //'meow'\nconsole.log(myDefaultDictionary.anran); //'hello'\n\nmyDefaultDictionary.anran = 'hi';\nconsole.log(myDefaultDictionary.anran); //'hi...'\nmyDefaultDictionary['yihao']='yee hee';\nconsole.log(myDefaultDictionary.yihao); //'yee hee'\n```\n\n## Check on argument\n```javascript\nfunction getALuckyNumber(name) {\n    console.log(`${name}'s lucky number is ${Math.floor(Math.random() * 30)}`);\n  }\n  \n  const getLuckyNumberAnranMachine = new Proxy(getALuckyNumber, \n      {apply: function(target, thisArg, argList)\n          {if(argList[0].toLowerCase() === 'anran') console.log(`anran, how many times have I tell you it's 18?`); \n          else target(argList[0]);}}\n  );\n```\nNow I always get a fixed lucky number (and get told off when I ask it):\n```javascript\ngetLuckyNumberAnranMachine(\"yihao\"); // yihao's lucky number is 12\ngetLuckyNumberAnranMachine(\"judy\"); // judy's lucky number is 3\ngetLuckyNumberAnranMachine(\"anran\"); // anran, how many times have I tell you it's 18?\n```\n\n# [A nice extending constructor example by MDN](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy#Extending_constructor)\n\n```javascript\nfunction extend(sup, base) { \n  var descriptor = Object.getOwnPropertyDescriptor(\n    base.prototype, 'constructor'\n  ); \n  base.prototype = Object.create(sup.prototype);\n  var handler = {\n    construct: function(target, args) {\n      var obj = Object.create(base.prototype);\n      this.apply(target, obj, args);\n      return obj;\n    },\n    apply: function(target, that, args) {\n      sup.apply(that, args);\n      base.apply(that, args);\n    }\n  };\n  var proxy = new Proxy(base, handler);\n  descriptor.value = proxy;\n  Object.defineProperty(base.prototype, 'constructor', descriptor);\n  return proxy;\n} \n\nvar Person = function(name) {\n  this.name = name;\n};\n\nvar Boy = extend(Person, function(name, age) {\n  this.age = age;\n});\n\nBoy.prototype.gender = 'M'; \n\nvar Peter = new Boy('Peter', 13);\n\nconsole.log(Peter.gender);  // \"M\"\nconsole.log(Peter.name);    // \"Peter\"\nconsole.log(Peter.age);     // 13\n\n```\n\n# Next Step\nHaven't fully understand the MDN extending constructor example yet:\nWhat is `Object.getOwnPropertyDescriptor`? How to best understand what is `proptype` in Javascript?","tags":["Web Development","Javascript","TripTime"]},{"title":"Test a React Component with Jest","url":"/2020/04/21/Test-a-React-Component-with-Jest/","content":"# Learning the Jest API basics\nI learnt about the Jest API usage from the [Jest API document](https://jestjs.io/docs/en/api). The functions that I used this time are:\n- `describe(name, fn)`\n- `expect(value)`\n- `.toBe(value)`\n- `.not`\n- `.toEqual(value)`\n- `.toMatchSnapshot(propertyMatchers?, hint?)`\n\n# Mocking webpack Modules\nJest can be used in projects that use webpack to manage assets, styles, and compilation. I did not use webpack directly, but the Next.js has built-in webpack that used to import CSS modules. \n\n```\n// __tests__/components/homepage/TripList.test.js\n\nimport TripList from '../../../components/homepage/TripList';\nimport TestRenderer from 'react-test-renderer';\n\ndescribe('Test TripList renders correctly when given no trip', () => {\n  const noTripRenderer = TestRenderer.create(\n    <TripList\n      displayIfNoTrip={testNoTripDisplay}\n      icon={faCode}\n      tripInfoList={[]}\n      title={'Test Trip List'}\n    />,\n  );\n  \n```\nI imported css files in the `TripList.js` file:\n```javascript\nimport styles from '../../css/homepage.module.css';\n```\nBut Jest does not know what to do with non-JS files at this moment, I got this error (I'll just include the whole thing to better explain the context):\n```\n        Test suite failed to run\n   \n       Jest encountered an unexpected token\n   \n       This usually means that you are trying to import a file which Jest cannot parse, e.g. it's not plain JavaScript.\n   \n       By default, if Jest sees a Babel config, it will use that to transform your files, ignoring \"node_modules\".\n   \n       Here's what you can do:\n        • To have some of your \"node_modules\" files transformed, you can specify a custom \"transformIgnorePatterns\" in your config.\n        • If you need a custom transformation specify a \"transform\" option in your config.\n        • If you simply want to mock your non-JS modules (e.g. binary assets) you can stub them out with the \"moduleNameMapper\" config option.\n   \n       You'll find more details and examples of these config options in the docs:\n       https://jestjs.io/docs/en/configuration.html\n   \n       Details:\n   \n       /Users/anran/Desktop/TripTime/src/spa/css/trip-card.module.css:1\n       .card {\n       ^\n   \n       SyntaxError: Unexpected token '.'\n   \n         2 | \n         3 | import React from 'react';\n       > 4 | import styles from '../../css/trip-card.module.css';\n           | ^\n         5 | import PropTypes from 'prop-types';\n         6 | import { FontAwesomeIcon } from '@fortawesome/react-fontawesome';\n         7 | import { faStickyNote, faCommentDots } from '@fortawesome/free-solid-svg-icons';\n   \n         at Runtime._execModule (node_modules/jest-runtime/build/index.js:988:58)\n         at Object.<anonymous> (components/homepage/TripCard.js:4:1)\n```\nBecause this was caused by an imported module, the [Using with webpack guide](https://jestjs.io/docs/en/webpack) gave me a detailed guidance on what to do with non-js imported modules.\n\nI used [`identity-obj-proxy`](https://github.com/keyz/identity-obj-proxy) to mock my CSS modules, and a local `fileMock.js` to define mocking of other non-JS modules:\n```javascript\n// __mocks__/fileMock.js\nmodule.exports = 'test-file-stub';\n```\n\nI went to the `jest.config.js` file, and added the `moduleNameMapper` configuration.\n```javascript\nmodule.exports = {\n  moduleNameMapper: {\n       \"\\\\.(jpg|jpeg|png|gif|eot|otf|webp|svg|ttf|woff|woff2|mp4|webm|wav|mp3|m4a|aac|oga)$\": \"<rootDir>/__tests__/__mocks__/fileMock.js\",\n       \"\\\\.(css|less)$\": \"identity-obj-proxy\"\n     }\n// ... more configs\n\n}\n```\nNow I could test my Next.js, and Jest knows what to do with non-JS modules.\n\n# TestRenderer\n`react-test-renderer` package provides a React renderer that can be used to render React components to pure JavaScript objects, without depending on the DOM or a native mobile environment.\nReact website gives clear guidance on [the usage of TestRenderer](https://reactjs.org/docs/test-renderer.html).\nThe functions I used this time are:\n- `TestRenderer.create()`, used to create the component renderer\n- `testRenderer.toJSON()`, used for snapshot testing\n- `testRenderer.root`, used to get the rendered instance\n- `testInstance.findByType()`, `testInstance.findByProps()`, `testInstance.findAllByType()`, used to find elements in the rendered instance\n\n\n# Snapshot Testing\nI first heard the term snapshot testing from my TripTime teammate and it sounded quite intimidating, but it turned out to be a very handy tool for front-end testing.\n\nThe `expect(value).toMatchSnapshot(propertyMatchers?, hint?)` function generates a snapshot locally when it first runs, and in the future runs will test if the new version matches the existing snapshot.\nThe code looks like this:\n```\ndescribe(\"Test userHomepage\", ()=>{\n    const userHomePageRenderer = TestRenderer.create(\n        <UserHomePage name={'Tester'} />,\n     );\n    test('Check if UserHomePage matches Snapshot', () => {\n        expect(userHomePageRenderer.toJSON()).toMatchSnapshot();\n    })\n//...\n});\n```\n\n# Next Step\nWhat is ES6 proxy?","tags":["Web Development","React","TripTime","Test Driven Development","Jest"]},{"title":"AWS Billing and Pricing","url":"/2020/04/20/AWS-Billing-and-Pricing/","content":"# Organizations, Consolidated Billing\nYou can use **AWS Organizations** to link all your accounts, and then use **Consolidated Billing** to **utilize unused RI price with Instances of the same type in other accounts**.\n\nYou have multiple AWS Accounts and additional unused reserved instances in the main production account.  You have deployed On-Demand instances in the Test & Dev Account.  How can you reduce your total EC2 costs?  \n- Step one, Create AWS Organizations \n- Use Consolidated Billing to link the Accounts.\n\n# EC2 Costs\nDetailed monitoring and use of Elastic Load Balancers will increase your EC2 Instances costs.\n## Spot instance pricing\n[See here for the tables](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html).\nIf you terminate your spot instance, you are charged with the same policy as on-demand (though different rate): Linux, for second; Windows, for full hour even if you use partial hour.\nIf EC2 terminate your spot instance, if it's in spot block, no charge at all; if not in spot block, for Linux you are charged by second, for windows you are not charged for the partial hour.\n## Scheduled Reserved Instances\nScheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a **daily, weekly, or monthly** basis, with a specified start time and duration, for **a one-year term**.\n\n# DynamoDB Pricing\nDynamoDB charges for **reading, writing, and storing data **in your DynamoDB tables, along with any optional features you choose to enable. \nDynamoDB has two capacity modes and those come with specific billing options for processing reads and writes on your tables: on-demand and provisioned. \n## on-demand capacity mode\nWith on-demand capacity mode, DynamoDB charges you for the data reads and writes your application performs on your tables. You do not need to specify how much read and write throughput you expect your application to perform because DynamoDB instantly accommodates your workloads as they ramp up or down.\n\n## provisioned capacity mode\nWith provisioned capacity mode, you specify the number of reads and writes per second that you expect your application to require. \n\n## Pricing Terms\n### Read request unit: RCU\nAPI calls to read data from your table are billed in read request units. DynamoDB read requests can be either strongly consistent, eventually consistent, or transactional. A strongly consistent read request of up to 4 KB requires one read request unit. For items larger than 4 KB, additional read request units are required. For items up to 4 KB in size, an eventually consistent read request requires one-half read request unit, and a transactional read request requires two read request units. For example, a strongly consistent read request of an 8 KB item requires two read request units, an eventually consistent read of an 8 KB item requires one read request unit, and a transactional read of an 8 KB item requires four read request units. See Read Consistency for more details.\n\n### Write request unit: WCU\nAPI calls to write data to your table are billed in write request units. A standard write request unit can write an item up to 1 KB. For items larger than 1 KB, additional write request units are required. A transactional write requires two write request units. For example, a write request of a 1 KB item requires one write request unit, a write request of a 3 KB item requires three write request units, and a transactional write request of a 3 KB item requires six write request units.\n\n### Replicated write request unit: rWCU\nhen using DynamoDB global tables, your data is written automatically to multiple AWS Regions of your choice. Each write occurs in the local Region as well as the replicated Regions.\n\n### Streams read request unit: \nEach GetRecords API call to DynamoDB Streams is a streams read request unit. Each streams read request unit can return up to 1 MB of data.\n\n### Transactional read/write requests: \nIn DynamoDB, a transactional read or write differs from a standard read or write because it guarantees all operations contained in a single transaction set succeed or fail as a set.\n\n\n\n\n\n","tags":["AWS","AWS CP Test"]},{"title":"AWS Security","url":"/2020/04/19/AWS-Security/","content":"\n<img src=\"shared-model.jpg\" width=\"700px\" alt=\"shared responsibility model\"/>\n\n# AWS Compliance Programs\n[AWS Compliance Programs](https://aws.amazon.com/compliance/programs/)\n## ISO/IEC 27001\nISO/IEC 27001:2013 is a security management standard that specifies security management best practices and comprehensive security controls following the ISO/IEC 27002 best practice guidance. \n## HIPAA \nsafeguarding medical information\n## PCI DSS\nhandle credit card information\n## SOC\nAWS System and Organization Controls (SOC) Reports are independent third-party examination reports that demonstrate how AWS achieves key compliance controls and objectives. \n\n# AWS Artifact\nOn-demand access to AWS security and compliance reports.\n**No cost**\n\n# Amazon Inspector\n## Hardening\nThe act of eliminating as many security risks as possible.\n\nAmazon Inspector runs a security benchmark against specific EC2 instances.\n\n# AWS WAF\nWeb application firewall\nWAF can either be attached to either **CloudFront** or an **Application Load Balancer**\nYou can either:\n- Write your own **rules** to ALLOW or DENY traffic based on the **content** of an **HTTP request**.\n- Use a **ruleset** from a trusted AWS Security Partner in the AWS WAF Rules Marketplace.\n\n# AWS Shield\nA managed DDoS protection service\nWhen you route your traffic through **Route53** or **CloudFront** you are using **AWS Shield Standard**\n\n# Penetration Testing\n**All Security Testing must be in line with AWS Security Testing Terms and Conditions.**\n## Permitted Services\nAmazon EC2 instances, NAT Gateways, and Elastic Load Balancers\nAmazon RDS\nAmazon CloudFront\nAmazon Aurora\nAmazon API Gateways\nAWS Lambda and Lambda Edge functions\nAmazon Lightsail resources\nAmazon Elastic Beanstalk environments\n## Prohibited Activities\nDNS zone walking via Amazon Route 53 Hosted Zones\nDenial of Service (DoS), Distributed Denial of Service (DDoS), Simulated DoS, Simulated DDoS\nPort flooding\nProtocol flooding\nRequest flooding (login request flooding, API request flooding)\n\n# Amazon Guard Duty\nthreat detection service. It uses machine learning to analyze the following AWS logs:\n- CloudTrail logs\n- VPC Flow logs\n- DNS logs\n\n# KMS\nThree things to know:\n- KMS is a multi-tenant HSM(hardware security module)\n- Many AWS services are integrated to use KMS to encrypt data with a simple checkbox\n- KMS uses envelope encryption: encrypt your data key with a master key as an additional layer of security.\n\n# Amazon Macie\nMonitors **S3 data access** activities to detect risks of unauthorized access or inadvertent data leaks.\n\n# Security Groups and NACLs\nFirewalls at instance/subnet level.\nSecurity Group, you create Allow rules\nNACLs, you create Allow and Deny rules\n\n# AWS VPN\nlets you establish a secure and **private tunnel** from your network/device to the AWS global network.\n## AWS Site-to-Site VPN\nsecurely connect on-premises network or branch office site to VPC\n## AWS Client VPN\nsecurely connect users to AWS or on-premises networks\n\n\n# IAM Best practices\n## Account Root User Access Key: Lock Away!\nYou use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key.\n## Create individual IAM users\nDon't use your AWS account root user credentials to access AWS, and don't give your credentials to anyone else. Instead, create individual users for anyone who needs access to your AWS account. \n## Grant Least Privilege\n## Use Customer Managed Policies Instead of Inline Policies\n## Configure a Strong Password Policy for Your Users\nIf you allow users to change their own passwords, require that they create strong passwords and that they rotate their passwords periodically.\n## Enable MFA\n## Do Not Share Access Keys\n## Rotate Credentials Regularly\nChange your own passwords and access keys regularly, and make sure that all IAM users in your account do as well. \n## Use Policy Conditions for Extra Security\nThere is a `condition` property in the policy JSON.\nFor example, you can write conditions to specify a range of allowable IP addresses that a request must come from. You can also specify that a request is allowed only within a specified date range or time range. \n## Monitor Activity in Your AWS Account\n- CloudTrail","tags":["AWS","AWS CP Test"]},{"title":"Global Outsourcing Reading Notes - Part One","url":"/2020/04/18/Global-Outsourcing-Reading-Notes-Part-One/","content":"[The Handbook of Global Outsourcing and Offshoring (available through auckland uni library)](https://search-credoreference-com.ezproxy.auckland.ac.nz/content/title/macoaore) is the textbook for Course InfoSys701. I will be reading it and putting notes into this article.\n\n# Key Terminologies in the Sourcing Literature\n## Sourcing\nSourcing is the act through which work is **contracted or delegated** to an external or internal entity that could be physically located anywhere. \n\nIt encompasses various insourcing (keeping work in-house) and outsourcing arrangements such as offshore outsourcing, captive offshoring, nearshoring and onshoring.\n\n## Outsourcing\nOutsourcing is defined as contracting with a **third-party supplier** for the management and completion of a certain amount of work, for a specified length of time, cost and level of service.\n\n## Offshoring\nOffshoring refers to the **relocation** of organisational activities (e.g., information technology, finance and accounting, back office and human resources) to a wholly owned subsidiary or an independent service provider in **another country**. \n\nThis definition illuminates the importance of distinguishing whether the offshored work is performed by the same organisation or by a third party. \n\n- When the work is offshored to a centre owned by the organisation, we refer to a **captive model of service delivery**. \n- When the work is offshored to an independent third party, we refer to an **offshore outsourcing model of service delivery**. \n- And when organisational activities are relocated to a neighbouring country (e.g., US organisations relocating their work to Canada or Mexico), we use the term **nearshoring**.\n\nIn addition, there are various common buzzwords such as **best-sourcing** (or best-shoring, right-shoring and far-shoring (as opposed to nearshoring), usually coined and used by supplier companies. Finally, there is also the **backsourcing** trend, which implies bringing work back in-house.\n\n\n\n# Current situation of outsourcing\nWhile, during 2014, many wondered whether automation and backsourcing (also known as re-shoring) would see the erosion of offshore outsourcing, in practice offshore outsourcing has been growing worldwide. \n\nIn more recent years, clients have pursued a cost-plus agenda when offshore outsourcing, and for large multinationals, offshore outsourcing increasingly has to fit into a larger global sourcing strategy that mitigates risk and links different sourcing options in a coordinated manner. \n\n# Drivers, Benefits and Risks of Global Sourcing\n\n## Outsourcing Client's Motivations\n- The main driver for outsourcing is still cost reduction\n- Access to skills and flexibility in how human capital is utilised\n- Flexibility in how human capital is utilised\n\n<img src=\"drivers.png\" alt=\"\" width=\"450px\"/>\n\n## Drivers for growth of Global Outsourcing\n- Technological advances in the telecommunications industry and the Internet have shrunk space and time and have enabled the coordination of organisational activities at the global level. \n- The supply of skilled yet low-cost labour in countries such as India and the Philippines and subsequently now over 125 further locations; \n- Investments in infrastructure\n- An improved business, economic and political climate in a number of developing countries; \n- The standardisation of IT processes and communication protocols that contribute to the efficiency of inter-organisational activities.\n- Many countries have invested heavily in improving their telecommunications infrastructure, which is essential for electronically transmitted services. \n- Many countries have provided tax advantages to attract offshoring. \n\n## Benefits of Global Outsourcing\nGlobal sourcing may offer several benefits associated with the advantages of outsourcing in general.\n\n- A company may reap significant **cost advantages** through the creation of economies of scale, access to the unique expertise of a third party and the reduction or stabilisation of overhead costs. \n- A company may benefit from outsourcing by **concentrating on core activities**, organisational specialisations, or by focusing on achieving key strategic objectives. \n    - More specifically, a strategy of building core competencies and outsourcing the rest may enable a company to focus its resources on a relatively few knowledge-based core competencies where it can develop best-in-the-world capabilities. \n    - Concentration on a core business may allow a company to exploit distinctive competencies that will lead to a significant competitive advantage.\n- Outsourcing can give the organisation access to the supplier's **capabilities and innovative abilities**, which may be expensive or impossible for the company to develop in-house (Quinn and Hilmer, 1994).\n- A network of suppliers can provide any organisation with the ability to quickly adjust the scale and scope of its production capability upwards or downwards, at a lower cost, in response to changing demand. In this way, outsourcing can provide greater **flexibility** (McCarthy and Anagnostou, 2003). \n- Outsourcing can **decrease the product or process design cycle time** if the client uses multiple best-in-class suppliers that work simultaneously on individual components of the system, as each supplier can contribute greater depth and sophisticated knowledge in specialised areas and thus offer higher quality inputs than any individual supplier or client can (Quinn and Hilmer, 1994).\n    - On this basis, having several offshore centres can provide **around-the-clock workdays**. \n    - In other words, development and production can take place constantly by exploiting the time difference between different countries.\n\n## Disadvantages of Adopting sourcing strategies\nAdopting sourcing strategies poses several other disadvantages. \n- **Loss of critical skills** or overdependence on an outside organisation for carrying out **important business functions** may evolve into significant threats to a company's well-being. \n- **Security and confidentiality of data** can become major issues for many companies. \n- **Losing control over the timing and quality of outputs** since these will be undertaken by an outside supplier: the result may be a poorer quality of the final product or service, and this may sully a company's image.\n- Additional outsourcing risks are associated with **organisational changes**. For example, outsourcing is usually followed by changes in organisational structure with **redundancies and layoffs**. Internal fears and employee resistance.\n- Outsourcing can be associated with problems related to the company's ability to learn because it can **increase insecurity among the workforce and decrease its motivation**, reducing employees’ willingness to question and experiment. \n\nTable 1.1 Offshore outsourcing risks\n<table class=\"entrytable\" id=\"t1.1\">\n<caption>Table 1.1 Offshore outsourcing risks</caption>\n<thead>\n<tr>\n<th><p>Risk category</p></th>\n<th><p>Sample risks</p></th>\n</tr>\n</thead>\n<tfoot>\n<tr>\n<td colspan=\"2\"><p><span class=\"bodyi\">Source:</span> Adapted from <a href=\"/content/entry/macoaore/references/0#bibl133\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><span class=\"crossRef\">Willcocks and Lacity, 2006</span></a>.</p></td>\n</tr>\n</tfoot>\n<tbody>\n<tr>\n<td><p>Business</p></td>\n<td>\n<p>No overall cost savings</p>\n<p>Poor quality</p>\n<p>Late deliverables</p>\n</td>\n</tr>\n<tr>\n<td><p>Legal</p></td>\n<td>\n<p>Inefficient or ineffective judicial system at offshore locale</p>\n<p>Intellectual property rights infringement</p>\n<p>Export restrictions</p>\n<p>Inflexible labour laws</p>\n<p>Difficulty obtaining visas</p>\n<p>Changes in tax laws that could significantly erode savings</p>\n<p>Inflexible contracts</p>\n<p>Breach in security or privacy</p>\n</td>\n</tr>\n<tr>\n<td><p>Political</p></td>\n<td>\n<p>Backlash from internal IT staff</p>\n<p>Perceived as unpatriotic</p>\n<p>Politicians’ threats to tax US companies that source offshore</p>\n<p>Political instability within offshore country</p>\n<p>Political instability between USA and offshore country</p>\n</td>\n</tr>\n<tr>\n<td><p>Workforce</p></td>\n<td>\n<p>Supplier employee turnover</p>\n<p>Supplier employee burnout</p>\n<p>Inexperienced supplier employees</p>\n<p>Poor communication skills of supplier employees</p>\n</td>\n</tr>\n<tr>\n<td><p>Social</p></td>\n<td>\n<p>Cultural differences</p>\n<p>Holiday and religious calendar differences</p>\n</td>\n</tr>\n<tr>\n<td><p>Logistical</p></td>\n<td>\n<p>Time-zone challenges</p>\n<p>Managing remote teams</p>\n<p>Coordination of travel</p>\n</td>\n</tr>\n</tbody>\n</table>\n\n \n# The Future of Outsourcing and Offshoring\n\n## Trend 1: Spending will continue to rise in all global sourcing markets, but BPO will overtake ITO\nBPO is outpacing ITO because many executives recognise that they undermanage their back offices and do not wish to invest in back-office innovations.\n\n## Trend 2: The ITO and BPO outsourcing markets will continue to grow through multisourcing\n\nAlthough ITO and BPO spending is increasing, the average size of individual contracts and the duration of contracts has been decreasing. \n\nMultisourcing is becoming the dominant practice, and its overall growth is driven by client organisations that are signing more contracts with more suppliers. \n\nAlthough multisourcing **helps clients access the best suppliers** and **mitigates the risks of relying on a single supplier**, it also means:\n- increased transaction costs because clients must manage more suppliers. \n- suppliers themselves incur more transaction costs\n    - they must bid more frequently because their contracts are shorter; \n    - they face more competition because smaller deals mean that more suppliers qualify to bid, \n    - they need to attract more customers in order to meet growth targets.\n\n## Trend 3: Global clients will view India primarily as a destination for excellence rather than a way to lower costs\n\nMany US and UK clients initially engaged Indian suppliers to provide technical services such as programming and platform upgrades. As these relationships matured, US clients assigned more challenging work to their Indian suppliers. \n\nIndian suppliers have also been taking actions to move themselves up the BPO value chain into complex BPO work and management consulting activities. \n\n## Trend 4: China's investment in ITO and BPO services signals promise …\n\n## Trend 5: Developing countries beyond India and China will become important players in the global business and IT services market\n\n## Trend 6: Large companies will give application service provision a very strong second look, as it becomes cloud services\n\nMany thought that application service provision (ASP) had died with the dot-com bust. But through cloud computing, small and medium enterprises (SMEs) and large organisations are increasingly buying into the proposition of renting applications software, infrastructure, applications, services and storage over the Internet.\n\n## Trend 7: Outsourcing will help insourcing … to an extent\n\n In-house operations are facing tough competition in nearly every area and can no longer assume they will retain their monopoly status with the organisation. As a result, in-house operations are adopting the techniques of the market.\n \n## Trend 8: Nearshoring will become more prevalent\n\nCompared to offshoring to remote locations, the benefits of nearshoring include lower travel costs, fewer time-zone differences and closer cultural compatibility. \n\n## Trend 9: More companies will sell their captive centres or create virtual captive centres\n\n## Trend 10: SMAC (social media, mobile Internet, business analytics and cloud) impacts outsourcing … in the longer run\n\n- Cloud platform suppliers like Amazon, Google, Microsoft and IBM will move up the value chain with more automated platforms. \n- Software- as-a-service could become a dominant sourcing model in certain areas such as employee performance management, indirect procurement, payroll and benefits administration, as clients move to self-help through managed services.\n- Automation (also known as robotic process automation) will have significant impact on outsourcing industry.    \n    - Generally speaking, work can be classified into four types: routine manual, routine cognitive, non-routine manual and non-routine cognitive. \n    - Of these, it is believed that routine manual work could be automated in the near future and routine cognitive work follows suit. \n    - Non-routine cognitive work is likely to be automated through the application of Big Data and business analytics, and non-routine cognitive work is also somewhat automated with the use of algorithms. \n\nIf this scenario is correct, then client companies will be able to significantly reduce the number of employees, for example, **not by outsourcing but by automating.**\nMeanwhile outsourcing suppliers may try to combat this by offering cheaper automated solutions of their own.\n    \n# Chapter 1 Summary\nIn this chapter, we have explained \n- the key terminology relating to global sourcing \n- an extensive review of past, current and future trends in global sourcing. \n\nIt is clear that more and more firms have introduced business solutions relating to global sourcing to \n- access scarce skills, \n- reduce costs and \n- streamline operations. \n\nFurthermore, interest has been growing rapidly in **outsourcing business processes**. The success rate of outsourcing has been mixed, because outsourcing benefits are not easily won and managements on both client and supplier sides face multiple risks and challenges on the path to successful outcomes. \n\nMeanwhile SMAC and related technologies are going to have a long-term significant impact on outsourcing, which will see outsourcing still grow but make more automated service propositions.\n","tags":["Coursework","Global Outsourcing"]},{"title":"Use CSS Combinator Selectors to do 'all but the last'","url":"/2020/04/17/Use-CSS-Combinator-Selectors-to-do-all-but-the-last/","content":"I've been using ` `(descendent combinator) and `>`(child combinator) heavily, bud did not pay much attention to `~` (general sibling combinator) and `+` (adjacent sibling combinator).\nToday I was making the homepage for TripTime, and I wanted to add a nice dotted gray divider between components:\n\n<img src=\"border.png\" alt=\"border preview\" width=\"800px\"/>\n\nThe `Next.js` code looks like this:\n\n<img src=\"react-code.jpg\" alt=\"react code\" width=\"600px\"/>\n\nThis time instead of messing with `nth-child` pseudo class, I used the adjacent sibling combinator to get a quick solution in choosing all `.component` elements but the last one, and it is as simple as this:\n```css\n.component + .component {\n    border-top: 1px dotted var(--box-shadow-color);\n}\n```\nThis is telling CSS to select all `.component` elements that are immediately after a `.component` element (and share the same parent).\n\n","tags":["TripTime","CSS","Web Beautify"]},{"title":"Cool thing on telnet","url":"/2020/04/17/Cool-thing-on-telnet/","content":"Telnet is an application protocol used on the Internet or local area network to provide a bidirectional interactive text-oriented communication facility using a virtual terminal connection.  (Definition from [Wikipedia](https://en.wikipedia.org/wiki/Telnet))\n\n[This website](https://osxdaily.com/2018/07/18/get-telnet-macos/) gave a fascinating example of using Telnet to watch StarWars, the full movie! \n\n<img src=\"telnet-star-war.gif\" width=\"700px\" alt=\"star wars\">\n\nHow many interesting people are there doing IT!\n\nHistorically, Telnet provided access to a command-line interface on a remote host. However, because of serious security concerns when using Telnet over an open network such as the Internet, its use for this purpose has waned significantly in favor of SSH.\n\n\n\n","tags":["Web Development","Web Protocols"]},{"title":"CSS clip-path","url":"/2020/04/16/CSS-clip-path/","content":"\n> The `clip-path` CSS property property creates a clipping region that sets what part of an element should be shown. Parts that are inside the region are shown, while those outside are hidden.\n\nIn the TripTime project, I made a polygon effect for small screen views of the landing page:\n\n<img src=\"polygon-border.jpg\" alt=\"polygon\" width=\"400px\">\n\nI achieved this by adding a pseudo-element with `border` property:\n\n```css\n    .introRight {\n        position: absolute;\n        bottom: 0;\n        left: 0;\n        width: 100vw;\n        height: 40vh;\n        background-color: rgba(256, 256, 256, 0.95);\n        font-size: 4.5vw\n    }\n\n    .introRight::after {\n        z-index: 2;\n        content: \"\";\n        position: absolute;\n        bottom: 100%;\n        left: 0;\n        width: 0;\n        height: 0;\n        border-bottom: 40vh solid rgba(256, 256, 256, 0.95);\n    }\n\n\n    .introRight::after {\n        border-right: 100vw solid transparent;\n    }\n```\n\nNow I learnt that I could also achieve this shape through:\n\n```css\n.introRight {\n        position: absolute;\n        bottom: 0;\n        left: 0;\n        width: 100vw;\n        height: 80vh;\n        background-color: rgba(256, 256, 256, 0.95);\n        font-size: 4.5vw;\n        clip-path: polygon(0 40vh, 100vw 0vh, 100vw 80vh, 0 80vh);\n    }\n```\n\nWith the `polygon` type, give a bunch of x-y coordinates. These specify position relative to the element itself, starting from the left top corner as (0, 0).\n\nOther options available for `clip-path` can be found [at the MDN page]([https://developer.mozilla.org/en-US/docs/Web/CSS/clip-path) \n\n\n\n","tags":["TripTime","CSS","Web Beautify"]},{"title":"The life of Impressume","url":"/2020/04/16/The-life-of-Impressume/","content":"I tried to build my Resume-making website last semester but never got to finish it. Now I will carry on. This post will record what I have done.\n\n","tags":["Web Development","My Projects"]},{"title":"Immediately-invoked function expression(IIFE)","url":"/2020/04/16/Immediately-invoked-function-expression-IIFE/","content":"\nIIFE is the foundation of modern modularisation in frontend development!\n\n```javascript\nlet module = (function(){\nlet _private = \"hi, anran!\";\nlet fun = ()=>{console.log(_private);}\nreturn {fun}\n})()\n```\nhere `module` is an immediately-invoked function expression. The `_private` variable will not be available outside of the `module`. To log the `_private` message in the console, run:\n```javascript\nmodule.fun();\n```\nYou cannot get the `_private` message by running\n```javascript\nmodule._private\n```\nThis isolated scope decreases the chance of global pollution. However, it could contain functions but not properties. This is when frontend modularisation protocols come in.\n\n# Next Step\n- Get more clear about how ES6 enables modularisation, why it needs to work with webpack to function to its full power.\n\n- [Node.js modules](https://nodejs.org/api/modules.html#modules_modules):\n```javascript\n// square.js\n// Assigning to exports will not modify module, must use module.exports\nmodule.exports = class Square {\n  constructor(width) {\n    this.width = width;\n  }\n\n  area() {\n    return this.width ** 2;\n  }\n};\n```\n\n```javascript\n// bar.js\nconst Square = require('./square.js');\nconst mySquare = new Square(2);\nconsole.log(`The area of mySquare is ${mySquare.area()}`);\n```","tags":["Javascript"]},{"title":"Being Exposed to Webpack","url":"/2020/04/16/Being-Exposed-to-Webpack/","content":"Today I first heard about Webpack. Writing something like `import ../assets/images/dog,png` in a JavaScript file is unheard of before, and it's good to know that with Webpack I can actually import these static resources in this way,\n> At its core, webpack is a static module bundler for modern JavaScript applications. When webpack processes your application, it internally builds a dependency graph which maps every module your project needs and generates one or more bundles. -- [Webpack Core Concepts](https://webpack.js.org/concepts/)\n\nSee also [this article](https://juejin.im/post/5ccf0c5d6fb9a03203619874) about why we need modularization in frontend development.\n\nNext.js uses a preconfigured webpack under the hood that you don't need to mess with. You can also extend the usage of `webpack` by defining a function that extends webpack config inside `next.config.js`. See the [Next.js Custom Webpack Config here](https://nextjs.org/docs/api-reference/next.config.js/custom-webpack-config).\n\nToday I'll just follow the webpack documentation to have a overview of webpack core concepts.\n\n# Core Concepts\n## Entry\nEntry point is the module that webpack starts building its internal dependency graph.\n\n## Output\nOutput is where webpack will save the generated bundles.\n\n## Loaders\nOut of the box, webpack only understands JavaScript and JSON files. \nLoaders allow webpack to process other types of files and convert them into valid modules that can be consumed by your application and added to the dependency graph.\n\nAt a high level, loaders have two properties in your webpack configuration:\nThe test property identifies which file or files should be transformed.\nThe use property indicates which loader should be used to do the transforming.\n\n```\n/* webpack.config.js */\n\nconst path = require('path');\n\nmodule.exports = {\n  output: {\n    filename: 'my-first-webpack.bundle.js'\n  },\n  module: {\n    rules: [\n      { test: /\\.txt$/, use: 'raw-loader' }\n    ]\n  }\n};\n```\n\nThe configuration above has defined a rules property for a single module with two required properties: test and use. This tells webpack's compiler the following:\n> \"Hey webpack compiler, when you come across a path that resolves to a '.txt' file inside of a require()/import statement, **use the raw-loader to transform it** before you add it to the bundle.\"\n\n## Plugins\nPlugins can be leveraged to perform a wider range of tasks like bundle optimization, asset management and injection of environment variables.\n\nIn order to use a plugin, you need to require() it and add it to the plugins array. Most plugins are customizable through options. Since you can use a plugin multiple times in a configuration for different purposes, you need to create an instance of it by calling it with the `new` operator.\n\n## Mode\nBy setting the mode parameter to either development, production or none, you can enable webpack's built-in optimizations that correspond to each environment. The default value is production.\n\n## Browser Compatibility\nwebpack supports all browsers that are ES5-compliant (IE8 and below are not supported). webpack needs Promise for import() and require.ensure(). If you want to support older browsers, you will need to load a polyfill before using these expressions.\n\n## Environment\nwebpack runs on Node.js version 8.x and higher.\n\n# An \"ahead of time compiler\" for the browser\n[A video tutorial by front end center](https://www.youtube.com/watch?v=WQue1AN93YU) presented the network timeline without using webpack:\n\n<img src=\"./timeline.jpg\" alt=\"timeline\" width=\"800px\">\n\nAs can be seen, the HTML is the entry point when we browse to the webpage.\nThe dependency from the html page to asset css/js files and then to `@import` resources can be compounding on big projects.\n\nWebpack follows a **graph of dependencies** regardless of their file type and tries to build a complete picture:\n\n<img src=\"./dependency-graph.jpg\" alt=\"dependency graph\" width=\"800px\">\n\n## Moving assets to webpack\nThe author then demonstrated how he moved the assets to webpack.\n### Step 1: Installing Webpack\n```\nnpm install --save-dev webpack\n```\nThen add an npm script to package.json\n```\n\"scripts\": {\n\"build\": \"webpack\"\n}\n```\nCreate `webpack.config.js`\n```javascript\nmodule.exports = {\n    entry: './src/scripts/main.js',\n    output: {\n    path: __direname + '/build',\n    publicPath: '/build/',\n    filename: 'bundle.js'\n    }\n}\n```\nIf we fire up `npm run build`, now webpack builds up your `bundle.js`.\n\nWhat we will be doing is **move the dependencies that browser needs to deal with, to dependencies that webpack take care of**\n\n### Step 2: Improve performance\nCSS is synchronised, which means we only start loading the resources that CSS refers to after it's fully loaded.\nBy moving the resource dependency into your bundle.js, you allow these resources to load early on.\n\nSo these are for today of me getting to know the basic idea of webpack. \n# Next Step\n- Follow [this video](https://www.youtube.com/watch?v=WQue1AN93YU) to try out how webpack functions.\n- The Network inspection tool looks handy to use when concerned about performances and what requests are being made.\n","tags":["Web Development","Web API"]},{"title":"Threat Modelling","url":"/2020/04/16/Threat Modelling/","content":"\n# STRIDE (Threat Identification)\nThreat | Desired property\n--- | ---\nSpoofing |\tAuthenticity\nTampering|\tIntegrity\nRepudiation| Non-repudiability (Accountability)\nInformation disclosure|Confidentiality\nDenial of Service|\tAvailability\nElevation of Privilege|\tAuthorization\n\n# DREAD (Threat Assessment)\nRating Category | Explanation\n--- | ---\nDamage | how bad would an attack be?\nReproducibility | how easy is it to reproduce the attack?\nExploitability | how much work is it to launch the attack?\nAffected Users | how many people will be impacted?\nDiscoverability |  how easy is it to discover the threat?\n\n\n\n\n\n\n","tags":["Security","Good to know"]},{"title":"Distributed Denial of Service attack","url":"/2020/04/15/Distributed-Denial-of-Service-attack/","content":"# Denial of Service (Dos)\n## What is a denial-of-service attack?\nA denial-of-service (DoS) attack occurs when legitimate users are unable to access information systems, devices, or other network resources due to the actions of a malicious cyber threat actor. \n## What services could be affected?\nEmail, websites, online accounts (e.g., banking), or other services that rely on the affected computer or network. \n## How is Denial-of-service condition accomplished?\nA denial-of-service condition is accomplished by flooding the targeted host or network with traffic until the target cannot respond or simply crashes, preventing access for legitimate users. \n## Common DoS attack\n### Smurf Attack\nThe attacker sends Internet Control Message Protocol broadcast packets to a number of hosts with a spoofed source Internet Protocol (IP) address that belongs to the target machine. The recipients of these spoofed packets will then respond, and the targeted host will be flooded with those responses.\n### SYN flood\n#### Normal three-way handshake\nNormally when a client attempts to start a TCP connection to a server, the client and server exchange a series of messages which normally runs like this:\n- The client requests a connection by sending a SYN (synchronize) message to the server.\n- The server acknowledges this request by sending SYN-ACK back to the client.\nThe client responds with an ACK, and the connection is established.\n- This is called the TCP three-way handshake, and is the foundation for every connection established using the TCP protocol.\n#### SYN flood attack sends SYN only\nA SYN flood attack works by not responding to the server with the expected ACK code. \nThe malicious client can either simply **not send the expected ACK**, or by **spoofing the source IP address in the SYN**, causing the server to send the SYN-ACK to a falsified IP address – which will not send an ACK because it \"knows\" that it never sent a SYN.\n#### Consequences\nThe server will wait for the acknowledgement for some time, as simple network congestion could also be the cause of the missing ACK. However, in an attack, the half-open connections created by the malicious client bind resources on the server and may eventually exceed the resources available on the server. At that point, the server cannot connect to any clients, whether legitimate or otherwise. This effectively denies service to legitimate clients. Some systems may also malfunction or crash when other operating system functions are starved of resources in this way.\n\n# Distributed DoS (DDoS)\nIn a DDoS attack, the incoming traffic flooding the victim originates from many\ndifferent sources. This effectively makes it impossible to stop the attack simply by\nblocking a single source.","tags":["Security","Good to know"]},{"title":"CSS Scroll Snap","url":"/2020/04/14/CSS-Scroll-Snap/","content":"Today I used the Scroll Snap module of CSS when developing the landing page of TripTime. It's a handy eye candy:)\n\nThe full reference for the CSS Scroll Snap can be find [here](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Scroll_Snap).\n\nI created a `snap-scroll.module.css` file to contain the snap-scroll-related CSS rules, for vertical scroll snap:\n```\n/* TripTime/src/spa/css/snap-scroll.module.css */\n.container {\n       overflow: auto;\n       outline: 1px dashed lightgray;\n       scroll-snap-type: y mandatory;\n   }\n.slide {\n       height: calc(100vh - 3em); /* 3em is the height of top bar */\n       width: 100vw;\n       border-bottom: 1px var(--box-shadow-color) solid;\n       box-sizing: border-box;\n       scroll-snap-align: center;\n   }\n```\n# Parent property: scroll-snap-type\n`scroll-snap-type: mandatory` means as long as you scroll the container, the viewport of the scroll container will move on to the next snap point.\nBy contrast, if you use `proximity`, the scroll container will rest at the closest snap point, and you will get sent back to your current snap point if you scroll but don't scroll close enough to the next scroll point.\n\n# Children property: scroll-snap-align\nThe value could be `none`, `start`, `end` or `center`.\nWith `none`, you are saying the current element does not contain a snap point.\nWith other options, you add a snap position in the corresponding axis, and the scroll container could rest on the specified point.\n\n# The child doesn't have to take up the whole container viewport\n\n<img src=\"snap-scroll.gif\" width=\"700px\" alt=\"snap scroll example\">\n\nThe site info component does not take up the whole space, but it can also be a snap point by having the `scroll-snap-align` rule:\n\n```\n .info-container {\n    height: 12vh;\n    scroll-snap-align: start;\n    }\n```\nIf this is the last child of the scroll container, when the scroll container reaches the end, it will rest at the position where the `info-container` takes up the bottom space. \n\n\n","tags":["CSS","Web Beautify"]},{"title":"Book Notes: Pro Git","url":"/2020/04/14/Book-Notes-Pro-Git/","content":"Today I started reading the book [Pro Git](https://git-scm.com/book/en/v2) written by Scott Chacon and Ben Straub.\nI'll put my notes here as I go.\n\n# Git in the Command Line \nHere I'll put the git command line that appeared in the book for future indexing.\n## Chapter 1\n`\n$ git config --list --show-origin\n$ git config --global user.name \"John Doe\"\n$ git config --global user.email johndoe@example.com\n$ git config --global user.name \"John Doe\"\n$ git config user.name\n`\n`\n$ git help <verb>\n$ git <verb> --help\n$ man git-<verb>\n$ git add -h\n`\n\n","tags":["Book Notes","git"]},{"title":"Getting to know about WebSocket","url":"/2020/04/13/Getting-to-know-about-WebSocket/","content":"> The WebSocket API is an advanced technology that makes it possible to open a two-way interactive communication session between the user's browser and a server. With this API, you can send messages to a server and receive event-driven responses without having to poll the server for a reply. [-- introduction from MDN](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API)\n\n# WebSocket vs HTTP\n<img src=\"idea-of-websocket.JPG\" alt=\"websocket vs http\"/>\n\n# The WebSocket Object\nThe WebSocket object provides the API for creating and managing a WebSocket connection to a server, as well as for sending and receiving data on the connection.\n\nTo construct a WebSocket, use the WebSocket() constructor.\n\n```javascript\n// Create WebSocket connection.\nconst socket = new WebSocket('ws://localhost:8080');\n\n// Connection opened\nsocket.addEventListener('open', function (event) {\n    socket.send('Hello Server!');\n});\n\n// Listen for messages\nsocket.addEventListener('message', function (event) {\n    console.log('Message from server ', event.data);\n});\n```\n\n# WebSocket Events\n- open\n- message\n- error\n- close\n","tags":["Web API"]},{"title":"AWS Loggings","url":"/2020/04/13/AWS-Loggings/","content":"Content from [this AWS page](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users). Very helpful summary for CP Test.\n\nLogging features are available in the following AWS services:\n# Amazon CloudFront \nLogs user requests that CloudFront receives. \n\n# AWS CloudTrail \nLogs AWS **API** calls and related events made by or on behalf of an AWS account. \n\n# Amazon CloudWatch\nMonitors your AWS Cloud resources and the applications you run on AWS. You can set alarms in CloudWatch based on metrics that you define. \n\n# AWS Config\nProvides detailed historical information about the configuration of your AWS resources, including your IAM users, groups, roles, and policies. \nFor example, you can use AWS Config to determine the permissions that belonged to a user or group at a specific time. \n\n# Amazon Simple Storage Service (Amazon S3)\nLogs access requests to your Amazon S3 buckets.","tags":["AWS","AWS CP Test"]},{"title":"AWS Storage notes","url":"/2020/04/13/AWS-Storage-notes/","content":"# Block Storage vs Object Storage\nBlock Storage can change one block (piece of the file) that contains the character.\nWith object Storage, you need to re-upload the whole file for any updates.\nBlock Storage are typically faster and use less bandwidth, but can cost more than object level storage.\n\n# Amazon EBS (Elastic Block Storage)\nAmazon EBS offers block-level storage for your EC2 instance - directly attached to EC2 instances, lower latency, good choice for running a database in EC2 instance.\n\nEBS volumes are automatically replicated within **the same Availability Zone**.\n**EBS's scope is a single Availability Zone**\n\n## EBS Volume types \n Amazon EBS provides three volume types: General Purpose SSD, Provisioned IOPS SSD, and magnetic.\n- Solid State Drives (SSD)\n    - General Purpose\n    - Provisioned IOPS\n- Hard Disk Drives (HDD/ Magnetic)\n    - Throughput Optimized\n    - Cold\n> Only SSD can be used for boot volume of EC2 instances\n> **Provisioned IOPS SSD** backed Amazon EBS volumes gives you the best performance.\n- Lower cost HDD option can be used for additional storage other than boot volumes.\n\n## Encryption\nEncrypted Amazon EBS volumes\nNo additional cost\n\n## Snapshots\nEBS backup: EBS snapshots, stored in Amazon S3.\nTo provide a even higher level of data durability with EBS, you can use **Point-in-time snapshots** of your volumes. \nYou can recreate a new volume from a snapshot anytime.\nYou can also share snapshots/copy snapshots to different Regions to provide even greater Disaster Recovery protection.\n\n## Elasticity\nYou can change storage type and size without stopping your instances.\n\n## EBS Cost\n### Volumes\nAll volume types are charged by the amount that is provisioned per month until you release the storage.\nEBS volumes persist independently from the instance.\n### I/O\n- I/O is included in the price of General Purpose SSD volumes. \n- For Amazon EBS magnetic volumes, I/O is charged by the number of requests that you make to your volume.\n- With Provisioned IOPS SSD volumes, you are also charged by the amount you provision in IOPS (multiplied by the percentage of days that you provision for the month).\n### Snapshots\nAdded cost of snapshots to Amazon S3 is per GB-month of data stored\n### Data transfer\n- Inbound data transfer is free\n- Outbound data transfer acoss Region incurs cost.\n\n# Amazon S3\n## Objects\nAmazon S3 is object-level storage.  Objects can be almost any data file, such as images, videos, or server logs. Objects can be up to 5 TB in size. \n \n## Durability\nAmazon S3 Standard offers 99.99% availability.\nAmazon S3 offers 11 9’s of durability for Standard Storage Class as well as for their Infrequent Access and Glacier Class.\n\nOne Zone IA has reduced availability (99.95%) and reduced durability (could get destroyed). Good for secondary backup.\n\n\n## Access control\nYou get fine-grained control over who can access your data by using **AWS Identity and Access Management (IAM) policies**, **Amazon S3 bucket policies**, and even **per-object access control lists(ACL)**.\n\nBy default, none of your data is shared publicly. \nYou can also encrypt your data in transit and choose to enable server-side encryption on your objects.\n\n## S3 Storage Classes\n\n<img src=\"S3-classes.png\" width=\"600px\"/>\n\n### S3 Standard\nAmazon S3 Standard is designed for high durability, availability, and performance object storage for frequently accessed data. \n###  S3 Intelligent-Tiering \nOptimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.\nThere are no retrieval fees when you use the Amazon S3 Intelligent-Tiering storage class, and no additional fees when objects are moved between access tiers. \nIt works well for long-lived data with access patterns that are unknown or unpredictable \n### Amazon S3 Standard-Infrequent Access (Amazon S3 Standard-IA) \nFor data that is accessed less frequently, but requires rapid access when needed.\nThis combination of low cost and high performance makes Amazon S3 Standard-IA good for long-term storage and backups, and as a data store for disaster recovery files.\n### Amazon S3 One Zone-Infrequent Access (Amazon S3 One Zone-IA) \nAmazon S3 One Zone-IA stores data in a single Availability Zone and it costs less than Amazon S3 Standard-IA. \nAmazon S3 One Zone-IA works well for customers who want a lower-cost option for infrequently accessed data, but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. \nA good choice for storing secondary backup copies of on-premises data or easily re-creatable data. You can also use it as cost-effective storage for data that is replicated from another AWS Region by using Amazon S3 Cross-Region Replication.\n### Amazon S3 Glacier\nA secure, durable, and low-cost storage class for **data archiving**.\nYou can upload objects directly to Amazon S3 Glacier, or use **Amazon S3 lifecycle policies** to transfer data between any of the Amazon S3 storage classes for active data (Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA) and Amazon S3 Glacier\n### Amazon S3 Glacier Deep Archive\nIt supports long-term retention and digital preservation for data that might be accessed once or twice in a year. \nThe **lowest-cost storage class** for Amazon S3. \nAll objects that are stored in Amazon S3 Glacier Deep Archive are replicated and stored across at least three geographically dispersed Availability Zones, and these objects can be restored within 12 hours.\n\n## Amazon S3 Bucket\n- Amazon S3 stores data inside buckets. Buckets are logical containers for objects.\n- Buckets are essentially the prefix for a set of files, and must be uniquely named across all of Amazon S3 globally. \n- You can control access for each bucket—who can create, delete, and list objects in the bucket. \n- You can also view access logs for the bucket and its objects.\n- YOU can choose the geographical region where Amazon S3 stores the bucket and its contents.\n- Because buckets can be accessed using path-style and virtual-hosted–style URLs, we recommend that you create buckets with **DNS-compliant bucket names**.\n\n### [Bucket Naming rules](https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html)\n- Bucket names must be a series of one or more labels. Adjacent labels are separated by a single period (.). Bucket names can contain lowercase letters, numbers, and hyphens. Each label must start and end with a lowercase letter or a number.\n- Bucket names must not be formatted as an IP address (for example, 192.168.5.4). \n- Bucket names must start with a lowercase letter or number.\n- Bucket names must not contain uppercase characters or underscores.\n- Bucket names should be between 3 and 63 characters long.\n\n### Bucket URLs\nAmazon S3 supports both virtual-hosted–style and path-style URLs to access a bucket. \n#### path-style URL endpoint\nhttps://s3.ap-northeast-1.amazonaws.com/bucket-name\n#### virtual hosted-style URL endpoint\nhttps://bucket-name.s3-ap-northeast-1.amazonaws.com\n  \n### Hosting static website on S3\nYou can host static websites on S3 but are not able to host any dynamic sites designed with server side scripting.\nFor hosting a static website, the minimum two files you need is `index.html` and `error.html`\n\n### S3 Transfer Acceleration \nS3 Transfer Acceleration uses CloudFront’s globally distributed edge locations to provide fast file transfers over long distances. \nYou might want to use Transfer Acceleration on a bucket for various reasons, including the following: \n- You have customers that upload to a centralized bucket from all over the world. \n- You transfer gigabytes to terabytes of data on a regular basis across continents. \n- You underutilize the available bandwidth over the Internet when uploading to Amazon S3.\n## Scaling\nAmazon S3 automatically manages the storage behind your bucket while your data grows. You can get started immediately, and your data storage will grow with your application needs.\nAmazon S3 also scales to handle a high volume of requests. You do not need to provision the **storage or throughput**, and you are billed only for what you use.\n## Pricing\nYou pay for:\n- Storage, gigabytes per month; \n- transfer OUT to other Regions; \n- PUT, COPY, POST, LIST, and GET requests.\n\nYou don't pay for:\n- transfers IN to Amazon S3 \n- transfers OUT to Amazon CloudFront edge locations or EC2 instances within that same Region\n\n### Factors influencing S3 pricing\n- Storage class type\n- Amount of storage\n- Requests\n- Data transfer\n\n# Amazon Elastic File System\nFile storage for use with AWS services and on-premises resources. \n\n# Amazon S3 Glacier\n## Three retrieval options:\n- expedited: 1-5 minutes\n- standard: 3-5 hours\n- bulk: 5-12 hours\n## Archive\nArchive: Any object such as a photo, video, file, or document that you store in Amazon S3 Glacier. \nArchive is the base unit of storage in Amazon S3 Glacier. Each archive has its own unique ID and can also have a description.\n## Vault\nA container for storing archives. When you create a vault, you specify the vault name and the **region** in which you would like to locate the vault.\n## Vault Access Policy\n One vault access policy can be created for each vault to manage access permissions for that vault. You can also use a vault lock policy to make sure a vault cannot be altered. \n Each vault can have one vault access policy and one vault lock policy that is attached to it.\n## Using S3 Glacier\n### AWS Management Console. \nOnly a few operations—such as creating and deleting vaults, and creating and managing archive policies—are available in the console.\n### Amazon S3 Glacier REST APIs\n### AWS Java or .NET SDKs\n### AWS CLI\n### Amazon S3 with lifecycle policies\n## Pricing\nWhile both services have per-request charges, Amazon S3 charges for PUT, COPY, POST, LIST, GET operations. In contrast, Amazon S3 Glacier charges for UPLOAD and retrieval operations.\n\n Because Amazon S3 Glacier was designed for less-frequent access to data, it costs less for storage but more for each RETRIEVAL REQUEST than Amazon S3.\n## Amazon S3 lifecycle policies\nYou should automate the lifecycle of the data that you store in Amazon S3. By using lifecycle policies, you can cycle data at regular intervals between different Amazon S3 storage types. This automation reduces your overall cost, because you pay less for data as it becomes less important with time.\n\n## Server-side Encryption\nServer-side encryption is focused on protecting data at rest.\n- With both Amazon S3 and Amazon S3 Glacier, you can securely transfer your data over HTTPS. \n- Any data that is archived in Amazon S3 Glacier is encrypted by default. \n- With Amazon S3, your application must initiate server-side encryption. You can accomplish server-side encryption in Amazon S3 in several ways.\n### Amazon S3-managed encryption keys (SSE-S3) \n### Customer-provided Encryption Keys (SSE-C) \n### AWS Key Management Service (AWS KMS)\n\n# EFS vs EBS vs S3\nAmazon EBS delivers high-availability block-level storage volumes for Amazon Elastic Compute Cloud (EC2) instances. It stores data on a file system which is retained after the EC2 instance is shut down. \nAmazon EFS offers scalable file storage, also optimized for EC2. It can be used as a common data source for any application or workload that runs on numerous instances. Using an EFS file system, you may configure instances to mount the file system. \nThe main differences between EBS and EFS is that **EBS is only accessible from a single EC2 instance in your particular AWS region**, while EFS allows you to **mount the file system across multiple regions and instances**.\nFinally, Amazon S3 is an object store good at storing vast numbers of backups or user files. Unlike EBS or EFS, **S3 is not limited to EC2**. Files stored within an S3 bucket can be accessed programmatically or directly from services such as **AWS CloudFront**. This is why many websites use it to hold their content and media files, which may be served efficiently from AWS CloudFront.\n\n# Storage Options for EC2\n## Two Direct options (Can be used for root volume):\n### Amazon Elastic Block Store (EBS)\nDurable block storage service that is designed to be used with Amazon EC2 for both throughput-and transaction-intensive workloads.\n### Amazon EC2 Instance Store\n**Temporary** block-level storage for your instance. This storage is located on disks that are physically attached to the host computer.\nInstance Store works well when you must temporarily store information that changes frequently, such as buffers, caches, scratch data, and other temporary content. You can also use Instance Store for data that is replicated across a fleet of instances, such as a load balanced pool of web servers. \n**If the instances are stopped—either because of user error or a malfunction—the data on the instance store will be deleted.**\n**Rebooting does not erase data in instance storage**\n## Other options(not for root volume):\n### <u>Mount</u> an Amazon Elastic File System (EFS) file system\nAmazon EFS is designed to provide massively parallel shared access to thousands of Amazon EC2 instances, enabling your applications to achieve high levels of aggregate throughput and IOPS with consistent low latencies.\n### <u>Connect</u> to Amazon Simple Storage Service(Amazon S3)\nAn object storage service\n\n\n- A vault in S3 Glacier is the container for storage archieves\n- EBS is used when requiring a encryption solution\n\n","tags":["AWS","AWS CP Test"]},{"title":"AWS naming and URL patterns","url":"/2020/04/13/AWS-naming-and-URL-patterns/","content":"# IAM User \n## login URL\nhttps://AWSAccountID.signin.aws.amazon.com/console\n## Naming\nIAM User names should contain alphanumeric characters, or any of the following: _+=,.@-\n\n# S3 Bucket\n## path-style URL endpoint\nhttps://s3.ap-northeast-1.amazonaws.com/bucket-name\n## virtual hosted-style URL endpoint\nhttps://bucket-name.s3-ap-northeast-1.amazonaws.com\n## Bucket naming\nno underscore(_)\ncan have dot(.) or hyphen(-)\n\n\n","tags":["AWS","AWS CP Test"]},{"title":"AWS Services Tested","url":"/2020/04/12/AWS-Services-Tested/","content":"\n# Free services\n## Services that are free to use but can provision AWS services that cost money:\n- CloudFormation\n- Elastic Beanstalk\n- Auto Scaling\n- Opsworks\n- Amplify\n- AppSync\n- CodeStar\n## Free\n- Organisations and Consolidated Billing\n- AWS Cost Explorer\n- Amazon VPC\n- IAM\n- AWS Artifact\n\n# Services using Edge Location\n## CloudFront\n## Route 53\nRequesting going to either CloudFront or Route 53 will be routed to the nearest edge location automatically.\n\nCloudFront Origin can be an **S3 Bucket, EC2 or ELB**\n\n## S3 Transfer Acceleration\nAs the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\n## API Gateway\nAPI Gateway endpoint traffic also use the AWS Edge Network.\n\n# AWS Service Catalog\nAWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage commonly deployed IT services, and helps you achieve consistent governance and meet your compliance requirements, while enabling users to quickly deploy only the approved IT services they need.\n# Amazon Athena\nAmazon Athena is an interactive **query service** that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.\n# AWS Config\nAWS Config is a managed service that provides AWS resource inventory information and enables you to record configuration change history to enable security and governance requirements. With AWS Config, you can discover both existing and deleted resources at any point in time.\nAWS Config is a service that enables you to assess, audit, and evaluate the **configurations** of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. \n\n# Rekognition\nAmazon Rekognition makes it easy to add **image and video analysis** to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content. \n\n# Macie\nA machine learning-powered security service to discover, classify, and protect **sensitive data**.\n\n# Snowball: Physical solution.\nSnowball offers a petabyte-scale **data transport solution** that uses devices designed to be secure to transfer large amounts of data **into and out of the AWS Cloud**. \nThey send you the physical device!\n\n# LightSail\nAmazon Lightsail is the easiest way to get started with AWS if you just need **virtual private servers**. Lightsail includes everything you need to launch your project quickly – a virtual machine, SSD-based storage, data transfer, DNS management, and a static IP – for a low, predictable price.\n\n# Amazon WorkSpaces \nAmazon Workspaces is a managed, secure, **cloud desktop service**. You can use Amazon WorkSpaces to provision either Windows or Linux desktops.\n\n# AWS Inspector \nAWS Inspector is an agent you install on your EC2 Instance and run analysis to identify vulnerabilities\n\n# AWS CloudFormation\nCloudFormation enables you to build a template of your infrastructure as code which you can configure to your exact specification that can be then be used to deploy servers that meet the configuration and security requirements for the business.\nYou can use **JSON** and **YAML** formats to design your CloudFormation templates\nAWS CloudFormation itself is **completely free**; when CloudFormation is used to launch EC2 Instances from the template, those EC2 Instances will be chargeable - Similar to free Elastic Beanstalk service.\n\n# Amazon Simple Queue Service (SQS)\nAmazon Simple Queue Service (SQS) is a fully managed **message queuing service** that enables you to decouple and scale microservices, distributed systems, and serverless applications. \n\n# Amazon API Gateway\nAmazon API Gateway is a fully managed service that makes it easy for developers to **create, publish, maintain, monitor, and secure APIs** at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can **create RESTful APIs and WebSocket APIs** that enable real-time two-way communication applications. \n\n# Route 53\nAmazon Route 53 is a highly available and scalable cloud **Domain Name System (DNS)** web service. \nDNS service is like the phone book of the Internet. Route 53 routes end users (who provide the domain name) to Internet applications.\n\n# Amazon Kinesis \nAmazon Kinesis makes it easy to collect, process, and analyze real-time, **streaming data** so you can get timely insights and react quickly to new information. \n## Kinesis Video Streams\nCapture, process, and store video streams\n## Kinesis Data Streams\nCapture, process, and store data streams\n## Kinesis Data Firehose\nLoad data streams into AWS data stores\n## Kinesis Data Analytics\nAnalyze data streams with **SQL** or **Java**\n\n# AWS WAF（Web Application Firewall)\nAWS WAF is a web application firewall that helps protect your web applications or APIs **against common web exploits** that may affect availability, compromise security, or consume excessive resources. \n\n# Consolidated Billing\nConsolidated Billing enables you to link your accounts and take advantage of volume price discounts\n\n# Amazon SNS\nAmazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed **pub/sub messaging service** that enables you to decouple microservices, distributed systems, and serverless applications.\nAmazon SNS and Amazon CloudWatch are integrated so users can collect, view, and analyze metrics for every active SNS. Once users have configured CloudWatch for Amazon SNS, they can gain better insight into the performance of their Amazon SNS topics, push notifications, and SMS deliveries.\nAmazon CloudWatch gives visibility into your filtering activity, and AWS CloudFormation enables you to deploy subscription filter policies in an automated and secure manner.\n\n# AWS Landing Zone\nAWS Landing Zone is a solution that helps customers more quickly set up a secure, multi-account AWS environment based on AWS best practices.\nThis solution can help save time by automating the set-up of an environment for running secure and scalable workloads while implementing an initial security baseline through the creation of core accounts and resources. It also provides a baseline environment to get started with a multi-account architecture, identity and access management, governance, data security, network design, and logging.\n\n# AWS Resource groups\nYou can use resource groups to organize your AWS resources. You create groups with tags as your grouping criteria.\n\n# AWS Quick Starts\nQuick Starts are built by AWS solutions architects and partners to help you deploy popular technologies on AWS, based on AWS best practices for security and high availability. \nEach Quick Start includes **AWS CloudFormation templates** that automate the deployment and a guide that discusses the architecture and provides step-by-step deployment instructions.\n\n# AWS Cost and Usage Report\n**spread sheet** for you to analyse your cost and usage\n- places the reports into S3\n- Uses Athena to turn report into queryable database\n- Use QuickSight to visualise as graphs\n# AWS Cost Explorer\nAWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time.\n\n# Amazon QuickSight\nAmazon QuickSight is a fast, cloud-powered **business intelligence (BI) service** that makes it easy for you to deliver insights to everyone in your organization.\n It generates create and publish interactive dashboards.\n \n# AWS Lake Formation\n AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis.\n\n# Amazon Simple Workflow\nAmazon Simple Workflow (Amazon SWF) helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the cloud. \n\n# AWS Database Services\n## DynamoDB\nNoSQL key/value database\n## DocumentDB\nNoSQL Document database, MongoDB compatible\n## RDS\nRelational Database Service that supports multiple engines:\nMySQL, Postgres, Aurora, Oracle, Microsoft SQL Server, Maria DB\n### Aurora\nMySQL(5* faster) and PostgreSQL(3* faster) database, fully managed\nBetter performance and redundancy. More expensive than other RDS options.\nAmazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous **backup to Amazon S3**, and **replication across three Availability Zones (AZs)**.\n### Aurora Serverless\nOnly runs when you need it, like AWS Lambda\n## Neptune\nManaged Graph Database\n## Redshift\nColumnar database, **perabyte warehouse**\n## ElastiCache\nRedis or Memcached database\nAmazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud.\n \n# Provisioning Services\n## Elastic Beanstalk\nDeploying and scaling web applications and services\n## OpsWorks\nConfiguration management service. provides managed instances of Chef and Puppet.\n## CloudFormation\nStructured as JSON or YAML code.\nMost flexible provision tools listed here.\n## AWS QuickStart\npre-make packages that can launch and configure your services required to deploy a workload on AWS.\n## AWS Marketplace\nA digital catalogue of thousands of software listings from independent vendors. Find, buy, test and deploy software.\nYou can also buy managed EC2 instances here.\n\n# AWS Fargate\nAWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\nYou pay for the time it runs, just like Lambda.\n\n# AWS Batch\nAWS Batch plans, schedules, and executes your batch computing workloads across the full range of AWS compute services and features, such as Amazon EC2 and Spot Instances.\nThere is no additional charge for AWS Batch. You only pay for the AWS resources (e.g. EC2 instances) you create to store and run your batch jobs.\n\n# AWS Glue\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. \n\n# AWS Storage Gateway\nAWS Storage Gateway is a **hybrid cloud storage service** that gives you on-premises access to virtually unlimited cloud storage. \n## Tape Gateway\nThe Tape Gateway configuration is a cloud-based virtual tape library (VTL) that serves as a drop-in replacement for tape backup systems.\n## Volume Gateway\nWith a Volume Gateway configuration, you can take snapshots of your local volumes and store those snapshots in Amazon EBS. \n## File Gateway\nThe File Gateway helps you manage hybrid file and object workloads that run across both your organization and the AWS Cloud.\n\n# Business Centric Services\n## Amazon Connect\nCall centre\n## WorkSpaces\nVirtual remote desktop\n## Chime\nOnline conference\n## WorkDocs\ncontent creation and collaboration\n## WorkMail\nbusiness email\n## Simple Email Service (SES)\nmarketing, notification, emails\n## Pinpoint\nmarketing campaign management system, sending targeted email, SMS, push notifications and voice messages\n## QuickSight\nBusiness Intelligence service, visualisation\n\n# Enterprise Integration: Hybrid\n## Direct connect\nDedicated Gigabyte network connection from your premises to AWS.\n## VPN\nsecure connection to your AWS network\n## Storage Gateway\nHybrid storage service that allows your on-premises applications to use AWS cloud storage. \n## AWS Managed Microsoft AD (Active Directory)\nAWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud.\n\n# AMAZON MSK (Managed Streaming for Apache Kafka)\nAmazon MSK is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Apache Kafka is an open-source platform for building real-time streaming data pipelines and applications. \n\n# Amazon Sumerian\nAmazon Sumerian lets you create and run virtual reality (VR), augmented reality (AR), and 3D applications quickly and easily without requiring any specialized programming or 3D graphics expertise.\n\n# AWS Elemental MediaConnect\nAWS Elemental MediaConnect is a high-quality transport service for live video.\n# AWS Elemental MediaConvert\nAWS Elemental MediaConvert is a file-based video transcoding service with broadcast-grade features. \n\n# SNS vs SQS\nboth Connect APPs via Messages\n## SNS\npub/sub messaging model. \nsend messages to subscribers of topics while http, email, sqs, sms.\nGood for webhooks, internal emails, triggering lambda functions.\n## SQS\nqueues up messages. Applications pull queue using AWS SDK.\nGood for delayed tasks, queueing up emails.\n\n# Inspector vs Trusted Advisor\nboth security tools that perform autids\n## Inspector\naudits a single EC2 instances, generate a report.\n## Trusted Advisor\ndoes not generate a PDF report. Gives you a holistic review across multiple services on best practices.\n\n# ALB, NLB, CLB\n## Application Load Balancer\nLevel 7 requests\nHTTP/HTTPS traffic\nCan attach WAF.\nIf you are running web application, this is what you want to use.\n### Listener\nBefore you start using your Application Load Balancer, you must add one or more listeners. A listener is a process that checks for connection requests, using the protocol and port that you configure. You need to configure Listeners to specific ports that you will accept incoming traffic on and the ports you will use to forward traffic onto the EC2 Instances\n\n## Network Load Balancer\nLayer 4 IP protocol data\nTCP and TLS traffic where extreme performance is required.\nOptimized for sudden and volatile traffic patterns while using a single statif IP address per Availability zone.\n## Classic load balancer\nold. Layer 4 and 7.\n\n\n# SNS, SES\n## Simple Notification service: Practical and internal\nplain text email.\npub/sub model\nSNS is generally used for sending plain text emails triggered via **other AWS services**, for example, billing alarms.\nLots of AWS services trigger SNS for notifications.\n## SES: professional, marketing\nhtml emails\ncan receive inbound emails\ncan create Email Templates\nMonitor your email reputation\nCustom domain name email\n\n# Storage Gateway vs AWS Site-to-Site VPN\n## Storage Gateway\nAWS Storage Gateway is a hybrid cloud **storage service** that gives you on-premises access to virtually unlimited cloud storage.\n## AWS Site-to-Site VPN\nAWS Site-to-Site VPN is a **network service** that enables you to connect your on-premises datacenter to your AWS VPC\nSite-to-Site VPN supports Internet Protocol security (IPsec) VPN connections.\n\n# AWS CloudHSM\nAWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. \n\nWith CloudHSM, you can manage your own encryption keys using **FIPS 140-2 Level 3 validated HSMs**. \n\nCloudHSM protects your keys with exclusive, single-tenant access to tamper-resistant HSMs in your own Amazon Virtual Private Cloud (VPC).\n\n\n# Amazon RDS Read Replicas\nAmazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for **read-heavy database workloads**.\n\n# AWS IAM Identify Federation\nFederation enables you to manage access to your AWS resources centrally. With federation, you can use single sign-on (SSO) to access your AWS accounts using credentials from your corporate directory. Federation uses open standards, such as Security Assertion Markup Language 2.0 (SAML), to exchange identity and security information between an identity provider (IdP) and an application.\n\n# AWS Cognito\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily.\nWith Amazon Cognito, your users can sign in through social identity providers such as Google, Facebook, and Amazon, and through enterprise identity providers such as Microsoft Active Directory via SAML.\n\n# AWS X-RAY\nAWS X-Ray enables you to analyze and debug distributed applications in and understand how your application and its underlying services are performing. You can then identify and troubleshoot the root cause of performance issues and errors.\n\n# AWS DeepLens\nAWS DeepLens is a wireless-enabled video camera and development platform integrated with the AWS Cloud. It lets you use the latest Artificial Intelligence (AI) tools and technology to develop computer vision applications based on a deep learning model.\n\n# AWS Managed Services (AMS)\nAWS Managed Services (AMS) operates AWS on your behalf, providing a secure and compliant AWS Landing Zone, a proven enterprise operating model, on-going cost optimization, and day-to-day infrastructure management.\nAWS Managed Services follows **ITIL**, a popular IT service management framework used by many Enterprises.\nAMS operates AWS infrastructure on behalf of enterprise customers and partners\n> ITIL, formerly an acronym for Information Technology Infrastructure Library, is a set of detailed practices for IT service management that focuses on aligning IT services with the needs of business\n\n# Global Tables: Multi-Region Replication with DynamoDB\nAmazon DynamoDB global tables provide a fully managed solution for deploying a multiregion, multi-master database, without having to build and maintain your own replication solution. \nWhen using Global Tables, you are charged based on the resources associated with each replica table. Write capacity for Global Tables is represented by replicated write capacity units (rWCUs) instead of standard write capacity units (WCUs)\n\n# Amazon EMR (Elastic MapReducer)\nAmazon EMR is the industry leading cloud-native big data platform for processing vast amounts of data quickly and cost-effectively at scale.\nManaged Hadoop.\n# AWS Infrastructure Event Management\nAWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events.\n\nWith Infrastructure Event Management, you get strategic planning assistance before your event, as well as real-time support during these moments that matter most for your business.\n","tags":["AWS","AWS CP Test"]},{"title":"AWS Support notes","url":"/2020/04/11/AWS-Support-notes/","content":"<img src=\"support-plans.jpg\" width=\"700px\" alt=\"plan summary\"/>\n\n# AWS Trusted Advisor\nAWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices. AWS scans your infrastructure, compares it to AWS best practices in five categories, and provide recommended actions.\n\n<img src=\"AWS-trusted-advisor.png\" alt=\"AWS trusted advisor\" width=\"600px\">\n\n## Five categories of best practices:\n1. Performance\n2. Security\n3. Fault Tolerance\n4. Cost Optimization\n5. Service Limits\n\n## Core Checks available to all customers\nAll AWS customers get access to the seven core Trusted Advisor checks to help increase the security and performance of the AWS environment. Checks include:\n- Security\n    - S3 Bucket Permissions\n    - Security Groups - Specific Ports Unrestricted\n    - IAM Use\n    - MFA on Root Account\n    - EBS Public Snapshots\n    - RDS Public Snapshots\n- Service Limits\n\n## Full Trusted Advisor Benefits\n**Business Support** and **Enterprise Support** customers get access to the full set of Trusted Advisor checks and recommendations. These help optimize your entire AWS infrastructure, to increase security and performance, reduce your overall costs, and monitor service limits. \nAdditional benefits include:\n- Notifications: Stay up-to-date with your AWS resource deployment with weekly updates, plus create alerts and automate actions with Amazon CloudWatch.\n- Programmatic access: Retrieve and refresh Trusted Advisor results programmatically using AWS Support API.\n\n# AWS Concierge\nYour AWS Concierge is a senior customer service agent who is assigned to your account when you subscribe to an **Enterprise** or qualified Reseller Support plan. This Concierge agent is your primary point of contact for **billing or account inquiries**; when you don’t know whom to call, they will find the right people to help. \nThe best way to contact the AWS Concierge is through the AWS Support Center.\n\n# AWS Support Plans\n[Comparison the four support plans](https://aws.amazon.com/premiumsupport/plans/)\n\n Plan | Trusted Advisor | Response Times | Programmatic Case Management | Technical Account Management | Account Assistance | Telephone support                                    \n--- | --- | --- | --- | --- | --- | ---\nBasic Support | 7 core checks | N/A | No | No |No | No\nDeveloper Support | 7 core checks | General Guidance <24 hours, System impaired < 12 hours | No | No | No | No\nBusiness Support | full checks | General Guidance < 24 hours, System impaired < 12 hours, Production System Impaired < 4 hours, Production system down < 1 hour | AWS Support API | No | No | Yes\nEnterprise Support | full checks | General guidance < 24 hours, System impaired < 12 hours, Production System Impaired < 4 hours, Production System down < 1 hour, Business-critical system down < 15 minutes | AWS Support API | Designated Technical Account Manager (TAM) to proactively monitor your environment and assist with optimization. | Concierge Support Team | Yes","tags":["AWS","AWS CP Test"]},{"title":"Numbers to remember for AWS CP test","url":"/2020/04/10/Numbers-to-remember-for-AWS-CP-test/","content":"The numbers presented below are all the \"default maximum\" unless otherwise noted. Some limits can be increased on demand.\n\n# 3008\n- [The maximum memory allocation for a single Lambda function: 3,008 MB(hard limit)]\n\n# 1000\n- [Maximum concurrent Lambda function execution: 1000 (soft limit)]\n\n# 250\n- [Maximum deployment package size of a Lambda function: 250MB unzipped, including layers]\n\n# 200\n- [200 Subnets per VPC](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html)\n- [200 Network ACLs per VPC](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html) You can associate one network ACL to one or more subnets in a VPC.\n- [200 Route tables per VPC](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html) The main route table counts toward this quota.\n\n# 60\n- [You can have 60 inbound and 60 outbound rules per security group (making a total of 120 rules). ](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html)\n\n# 50\n- [50 routes per route table (non-propagated routes)](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html)\n\n# 40 \n- [Maximum size of an individual S3 Glacier Archive is 40 TB](https://aws.amazon.com/glacier/faqs/)\n\n# 35\n- [Retention period for Amazon RDS automated backups can be between 0 and 35 days. Setting the backup retention period to 0 disables automated backups](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html)\n\n# 20\n- [Number of open invitations you can add in a 24-hour period in AWS Organizations](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_reference_limits.html)\n- [default maximum 20 Rules per network ACL](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html) This is the one-way quota for a single network ACL, where the quota for ingress rules is 20, and the quota for egress rules is 20. This quota includes both IPv4 and IPv6 rules, and includes the default deny rules (rule number 32767 for IPv4 and 32768 for IPv6, or an asterisk * in the Amazon VPC console).\n\n# 15\n- [Lambda function maximum timeout: 15minutes(hard limit)](https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/)\n\n# 25\n- [Objects stored in Amazon S3 Glacier Deep Archive can be restored within 12 hours] (https://aws.amazon.com/blogs/aws/new-amazon-s3-storage-class-glacier-deep-archive/)\n\n# 10\n- [AWS Direct Connect provides 1 Gbps and 10 Gbps connections](https://aws.amazon.com/directconnect/)\n\n# 7\n- [All AWS customers get access to the 7 core **Trusted Advisor checks** to help increase the security and performance of the AWS environment, in 2 of the 5 aspects: security and service limits](https://aws.amazon.com/premiumsupport/technology/trusted-advisor/)\n\n# 5\n- [For Business/Enterprise support, AWS Trusted Advisor analyzes your AWS environment and provides best practice recommendations in 5 categories: security, service limits, cost optimization, performance, fault tolerance](https://aws.amazon.com/premiumsupport/technology/trusted-advisor/)\n- [Default maximum Elastic IP addresses per region per AWS account](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html)\n- [Maximum number of Security Group per instance](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)\n- [By default, Amazon EC2 provides Basic Monitoring, which sends metric data to CloudWatch in 5-minute periods.](https://aws.amazon.com/blogs/aws/amazon-cloudwatch-basic-monitoring-for-ec2-at-no-charge/). To get metric data every 1 minute, pay for Detailed Monitoring.\n- [Number of member accounts you can create concurrently with AWS Organizations](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_reference_limits.html)\n- [By default, maximum 5 VPCs per region per account](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html)\n- [5 Internet gateways per Region](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html)  This quota is directly correlated with the quota on VPCs per Region. To increase this quota, increase the quota on VPCs per Region. Only one internet gateway can be attached to a VPC at a time.\n- [5 NAT gateways per Availability Zone](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html) A NAT gateway in the pending, active, or deleting state counts against your quota.\n- [Maximum item(individual object) size in Amazon S3 is 5TB](https://aws.amazon.com/s3/faqs/)\n\n# 5-12\n- [Bulk retrievals are S3 Glacier’s lowest-cost retrieval option, which you can use to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5–12 hours.](https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html)\n\n# 3-5\n- [Standard retrievals of Glacier archive typically complete within 3–5 hours.](https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html)\n\n# 3\n- [Amazon CloudWatch – Basic Monitoring for EC2 at No Charge monitors three metrics: CPU load, disk I/O, and network I/O](https://aws.amazon.com/blogs/aws/amazon-cloudwatch-basic-monitoring-for-ec2-at-no-charge/)\n- [A CloudWatch Alarm is always in one of 3 states: OK, ALARM, or INSUFFICIENT_DATA](https://aws.amazon.com/blogs/aws/amazon-cloudwatch-alarms/)\n- [EC2 Reserved Instances: 1yr or 3yr terms](https://aws.amazon.com/ec2/pricing/reserved-instances/)\n- [With Amazon S3 Glacier Deep Archive storage class, data is stored across 3 or more AWS Availability Zones](https://aws.amazon.com/blogs/aws/new-amazon-s3-storage-class-glacier-deep-archive/)\n- [3 CloudWatch metric alarm states: **OK, ALARM, INSUFFICIENT_DATA**](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)\n- [Amazon DynamoDB synchronously replicates data across 3 facilitates in an AWS Region](https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf)\n\n# 2\n- In most cases, each region has at least 2 Availability Zones.\n\n# 1\n- [EC2 Scheduled Reserved Instances: 1yr term](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html): Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. \n- [You can only have 1 Internet Gateway per VPC](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html)\n- Each S3 Glacier vault can have one vault access policy and one vault lock policy that is attached to it.\n- [Amazon EBS volumes have built-in redundancy within 1 Availability Zone - it is automatically replicated within its Availability Zone to prevent data loss due to failure of any single hardware component.](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html)","tags":["AWS","AWS CP Test"]},{"title":"Building a VPC in AWS from scratch","url":"/2020/04/09/Building-a-VPC-in-AWS-from-scratch/","content":"Today I will build a VPC with two public and two private subnets, and host web and database servers inside them, without using the wizard.\n\n# Create a VPC\n[Here's the aws VPC document](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html). \nGo to the VPC service page, create a VPC called AnranVPC, and specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block: 10.0.0.0/16.\n<img src=\"create-vpc.gif\" alt=\"create-vpc\" width=\"800px\">\nThere is a default Route Table and a default Network ACL (access control list) attached to my VPC already.\n<img src=\"vpc-defaults.gif\" alt=\"vpc-defaults\" width=\"800px\">\nThe default Network ACL allows all traffic.\n> A network ACL contains a numbered list of rules. We evaluate the rules in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. \n\nThe default Route Table directs network traffic with the IP address within the range 10.0.0.0/16 to local nodes.\n\n# Create Subnets\n> A VPC spans all of the Availability Zones in the Region. You can create subnets in multiple AZs in your VPC to enhance availability and redundancy.\n\n<img src=\"create-subnets.gif\" alt=\"create-subnets\" width=\"800px\">\n\nI created 2 public subnets in 2 availability zones, and 2 private ones in 2 availability zones.\n<img src=\"subnets.jpg\" alt=\"create-subnets\" width=\"800px\">\n\nEach subnet has 251 availability IPv4 addresses, because although there are 8 free digits for each subnets, there are five reserved IP addresses:\n- 10.0.0.0: Network address.\n- 10.0.0.1: Reserved by AWS for the VPC router.\n- 10.0.0.2: Reserved by AWS. The IP address of the DNS server is the base of the VPC network range plus two. For VPCs with multiple CIDR blocks, the IP address of the DNS server is located in the primary CIDR. We also reserve the base of each subnet range plus two for all CIDR blocks in the VPC. For more information, see Amazon DNS Server.\n- 10.0.0.3: Reserved by AWS for future use.\n- 10.0.0.255: Network broadcast address. We do not support broadcast in a VPC, therefore we reserve this address.\n\nBy default, each subnet is attached with the VPC default Network ACL and the VPC default route table.\n<img src=\"subnet-default.gif\" alt=\"subnet-default\" width=\"800px\">\n\nHere I did not modify the Network ACL and leave it as default (allows all inbound/outbound traffic), but there are things good to know about Network ACL:\n- Each network ACL includes a default rule whose rule number is an asterisk. This rule ensures that if a packet doesn't match any of the other rules, it's denied. You can't modify or remove this rule.\n- You can create a custom network ACL for your VPC. By default, a network ACL that you create blocks all inbound and outbound traffic until you add rules, and is not associated with a subnet until you explicitly associate it with one.\n- A network ACL contains a numbered list of rules. We evaluate the rules in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. \n- A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. (while security group only allows traffic)\n- You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC. \n- Network ACL acts at subnet level, and one Network ACL can be attached to one or more subnets. Each subnet has to have one Network ACL, and by default is attached to the Default VPC Network ACL that allows all traffic.\n\n# Create Gateways\n<img src=\"create-gateways.gif\" alt=\"create-gateways\" width=\"800px\">\n\nI create an Internet Gateway, and give it an identifiable name.\nWe allocate an Elastic IP address, create a NAT gateway, put it in a public subnet, and associate the Elastic IP address with it.\n**The Internet Gateway has to be created first before the NAT gateway!**\nI created the NAT gateway first and put it into the PublicSubnet1 of AnranVPC, but it failed and was deleted automatically, because AnranVPC did not have an Internet Gateway attached at that time.\n\n# Create and Associate Route tables\n> A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same subnet route table. \n\n<img src=\"create-public-route-table.gif\" alt=\"create-public-route-table\" width=\"800px\">\n\nThe Internet Gateway needs to be attached to the AnranVPC before the route table can link to it.\n\nOnce the route table linking the Internet traffic to the Internet Gateway is created, both public subnets can be associated with it.\n\n<img src=\"create-private-route-table.gif\" alt=\"create-private-route-table\" width=\"800px\">\n\nThe private route table direct Internet-bound traffic to the NAT Gateway. \n\n<img src=\"append-private-route-table.gif\" alt=\"associate-private-route-table\" width=\"800px\">\n\nNow I associate the private route table with the two private subnets.\n\n# Create EC2 instances\n<img src=\"create-EC2-instance.gif\" alt=\"create-EC2-instance\" width=\"800px\">\n\nI created four EC2 instances, one web server in each public subnet, and one database server in each private subnet.\n\n## Append Security Groups\n> A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups.\n> You can specify allow rules, but not deny rules.\n> Traffic need to be explicitly allowed by the security group. \n\nMy web servers allow SSH and HTTP/HTTPS inbound traffic from the Internet.\nMy database servers allow all TCP inbound traffic from within AnranVPC.\n\n# Associating an Elastic IP with web servers\n\n<img src=\"give-web-server-elastic-IP.gif\" alt=\"associate-elastic-IP\" width=\"800px\">\n\nI allocated two Elastic IP addresses and associated them with the two web servers.\n\n# SSH into my web servers\nThe public IPv4 address (as assigned by Elastic IP) of WebServer1 is now 3.234.133.181.\nNow I can SSH into WebServer1 with the saved key file.\n\n<img src=\"ssh-to-web-server-1.gif\" alt=\"ssh-to-web-server-1\" width=\"800px\">\n\nThe first attempt failed because of the access permission of my private key file webserver.pem is too open. \n[The chmod command](https://www.linuxtopia.org/online_books/introduction_to_linux/linux_The_chmod_command.html) changes the access right of files.\n`chmod 400 webserver.pem` sets permissions so that, User / owner can read, can't write and can't execute.\nAfter this command, I can now ssh to WebServer1 with my key file.\n\n<img src=\"set-web-server.gif\" alt=\"set-web-server\" width=\"800px\">\n\nNow I am in the shell of my web server 1 virtual machine.\nI can go to the root shell with :\n```\nsudo su -\n```\nAnd then I ran these commands:\n```\n# Install Apache Web Server and PHP\nyum install -y httpd mysql php\n# Download Lab files\nwget https://aws-tc-largeobjects.s3.amazonaws.com/AWS-TC-AcademyACF/acf-lab3-vpc/lab-app.zip\nunzip lab-app.zip -d /var/www/html/\n# Turn on web server\nchkconfig httpd on\nservice httpd start\n```\nNow if I go to http://3.234.133.181/ in my browser, I can see the web page:\n<img src=\"webpage.png\" alt=\"webpage\" width=\"800px\">\n\nAlthough I cannot access database servers in the private subnets directly from my local PC, I can SSH into my web server and then SSH into my database server from there.\nTo do this, I need to create a copy of the database key file on my web server machine. I can open the dbserver.pem from Atom and copied the key string, and executed the following command on my web server bash:\n```\n[ec2-user@ip-10-0-0-214 ~]$ mkdir key\n[ec2-user@ip-10-0-0-214 ~]$ cd key\n[ec2-user@ip-10-0-0-214 key]$ touch webserver2.pem\n[ec2-user@ip-10-0-0-214 key]$ echo \"-----BEGIN RSA PRIVATE KEY-----(a long block of key string here) -----END RSA PRIVATE KEY-----\" >> webserver2.pem\n[ec2-user@ip-10-0-0-214 key]$ chmod 400 webserver2.pem\n[ec2-user@ip-10-0-0-214 key]$ ssh -i webserver2.pem ec2-user@10.0.2.197\nLast login: Thu Apr  9 00:49:04 2020 from 10.0.0.214\n\n       __|  __|_  )\n       _|  (     /   Amazon Linux 2 AMI\n      ___|\\___|___|\n\nhttps://aws.amazon.com/amazon-linux-2/\n1 package(s) needed for security, out of 7 available\nRun \"sudo yum update\" to apply all updates.\n[ec2-user@ip-10-0-2-197 ~]$ \n```\nSo these are the steps of creating the VPC, subnets, security groups, route tables, Network ACLs from scratch.\n\n# Next Steps\nHost my app onto AWS","tags":["AWS"]},{"title":"Digital Forensic","url":"/2020/04/08/Digital-Forensic/","content":"# Definition of forensic and digital forensics\n## Forensic Science\nTwo main groups of people allowed to speak to a judge: \n- witnesses, allowed only to present what they saw/heard, but no interpretation allowed.\n- experts, allowed to present information/facts/documents, and provide interpretation of those facts.\nForensic Science aims to collect evidence and make an interpretation of the facts, and then present a report to the court.\n> Forensic Science is the application of science to criminal and civil laws, mainly—on the criminal side—during criminal investigation, as governed by the legal standards of admissible evidence and criminal procedure. \n> Forensic scientists can work in both criminal and civil cases;  can work for either the prosecution or the defence\n\n## Digital Forensic\nDigital Forensic is a branch of Forensic Science that encompasses the recovery and investigation of material found in digital devices, often in relation to computer crime. \n### Why we need digital forensic?\n- Cyber crime and cyber enabled crime is flourishing\n- The integrity of electronic system is vulnerable\n- Professional & business reputations are exposed\n- Computer Crime defects our trust of using digital devices, hence interoperability（流通性） and transaction is harmed\n\n### Knowledge field of digital forensics\nIT Security: Security for protection - develop tools and procedures to protect the information\nDigital Forensics: Prosecution or Defense - discover all the facts related to computer crime\nOverlap of these two fields: System hardening(make system more secure and possible for digital forensics scientists to find solid evidence.)\n\n### Definition\nLive Forensics, Network Forensics, Computer Forensics, Mobile Forensics, Database Forensics\n> Scientific tasks, techniques, and practices used in the investigation of stored or transmitted binary information or data for legal purposes. ISO/IEC 30121\n\n> “the process of identifying, preserving, analysing and presenting digital evidence in a manner that is legally acceptable”\n\n{%asset_img digital-forensic-standards.png \"digital-forensic-standards\"%}\n\n### Steps in a Digital Forensic Examination\n- Acquire evidence\n- Analyse evidence\n- Produce report(must have a specific standard to be accepted in the court)\n    -Provide ‘Expert’ consultation and testimony\n\n# Acquisition\nBefore starting a search, we must get a search warrant & search order.\nEvidence collection\n- Collection & digital search on site\n    - Overt: inform the owner about the search\n    - Covert: without informing the owner that the search has been conducted (does not often happens)\n- Bring the device to the lab to do the search\n\n## Search Warrants\n### Who may issue a search warrant?\n- Judge of high court, can be asked by two groups of people:\n    - law enforcement departments, e.g. police based on a suspicion of crime. \n    - private organization/person: \"someone committed a crime against us (e.g. issued a false documentation and harmed our benefit) and we believe there is some evidence in their device that support our claim.\" This is called a \"search order\"\n- National security organisation, they can do search without getting a permission (but must apply for a warrant after the search)\n### High priorities in issuing warrants:\n- Scope of the warrant: the data may not even in the country if the data is saved in cloud services, the warrant might not be able to be issued because of country boundary.\n- Legal privilege issues: Search must be committed only to search for related evidence related to the crime and cannot do anything else. e.g. if an organization is accused for financial crime, but the forensic specialist find evidence of illegal immigration, forensic specialist should not report anything about the immigration fraud without a relevant warrant.\n- Impact on the running of the business: if forensic specialist take away their device, there might be big influence on the organisation. We should be aware of this and minimise the impact.\n\nIf device is located away from boundary, a search warrant cannot be issued.\nSearch warrant must identify where the device is located, and the specialist is only allowed to search for the device on the site outlined by the warrant.\n\n\n\n## What are we acquiring?\nDigital information that is accessible from:\n- hard drives: \n        - the most popular type of hard drive used to come with a high-speed rotating disk and a magnetic head. Don't have a short access time. \n        - now there is a dramatic shrink in size of hard drives. Electronic hard drives have faster access time.\n- usb sticks\n- mobile phones\n- cameras\n- CDs, DVDs, Tapes ... etc\n\n### A Copy and a Clone/Forensic Image\nIf someone clicks \"delete\" on a file, the link/reference to the file is deleted, but the information is still on the hard disk.\nIf you order a usual \"Copy\" you only get referenced files. But if you do a \"Clone\", you can possibly get access to deleted files that could provide valuable information.\n**A forensic image must have a digital signature: Only digitally signed image can be used in court as trusted digital evidence that has not been tampered with/changed in the process of copying and evaluation.**\n\n### Acquisition process\n- Digitally photograph evidence items: demonstrating the evidence comes from specific device in specific location. \n- Formally document evidence items\n**make sure the chain of command is not broken, the devices are never left unattended from the beginning of the search.**\n- Acquire evidence item onto ‘sterile drive’\n**sterile drive was never written into. need device to block changes to the sterile drive**\n- Verify ‘digital signature’ of acquisition \n\n### Evidence case file\nCopy all images to single (sterile) case drive.\n\n### Specialised Hardware/Software\n- Two tools used:\n    - Encase\n    - FTK Imager\n- Write-blocking hardware to ensure that the subject item is not written to \n- Blocking software to stop writing into the original software\n#### Write Blocker\nA write blocker is any tool that permits read-only access to data storage devices without compromising the integrity of the data. NIST‘s general write blocking requirements hold that: \n- The tool shall not allow a protected drive to be changed\n- The tool shall not prevent obtaining any information from or about any drive\n- The tool shall not prevent any operations to a drive that is not protected.\n There are physical and logical write blockers that do the same task but in a slightly different way.\n\nAcquisition hardware: a (write-blocking) device that connects the investigated device and the sterile drive.\n\n### Computer Forensic Software\nTwo different approaches to searching\n- Encase: Just search\n- FTK: Generation of index before search\n    - before the search, it tries to index any combination of terms used in the device. The generation of the index takes long time, but then search is extremely quick.\nDepending on you need, you may use one of or both software tools.\n\n# Analysis (Search)\nWhat we are looking for:\n- documents\n- emails\n- pictures\n- who was on the system and when \n- who copied what off the system and when\n- who deleted what and when\n- who created/modified what and when\n- was the system time messed with\n\n# Report Production\nPaper or DVD? Many judges don't understand the capability of modern devices.\nA matter of scale: if we have 200 gigabytes of data, presenting them on paper could stack to 50 times height of sky tower. \n\n## Admissibility\nThe deliverable in digital forensic is the report.\nThe report must be produced in a compliant fashion.\n**Admissibility** is the acceptance of the report as evidence.\nThe NZ Evidence Act 2006 normalized that\nConsider also “Rules of the Court”\n\n# Digital Forensic Issues\n## Expert Witness\nJudicature Act 1908 N0 69 (as at date): Expert Witness Code of Conduct\n- Expert witness duty to the Court:\n    - An expert witness has an overriding duty to assist the Court impartially on relevant matters within the expert’s areas expertise.\n    - An expert witness is not an advocate for the party who engages the witness.\n- Code of Conduct:\n    - If an expert witness believes that his or her evidence or any part of it may be incomplete or inaccurate without some qualification, that qualification must be stated in his or her evidence.\n    - If an expert witness believes that his or her opinion is not a concluded opinion because of insufficient research or data or for any other reason, this must be stated in his or her evidence.\nAn expert witness, according to high court rules:\n- Must have read and agreed to the High Court code of conduct for Expert Witnesses\n- May give an ‘OPINION’ in evidence\n- Is there to assist the Court. Does not act as an advocate for prosecution or defence\n\n## Search Warrants: very modern problem\nTraditionally search warrants are issued to enter premises.\nWith the ICT architecture and cloud services, the data can be stored far away in location. \n\n## Safe guards the court is looking for:\n- Independent Digital Forensics Consultant\n- Personal undertakings to the court regarding actions\n- To be aware of the impact on the business\n- To hold information remote from plaintiff \n- As an expert witness to act on behalf of the court (as per the undertakings re an expert witness)\n\n## Qualifications, Training, Experience & Ethics\nMust have:\n- Formal Qualifications (degrees, usually degree in business or CS)\n- Training certificates Encase & FTK\n- Wide & detailed experience in IT industry\n- Legal procedure understanding\n- Business experience\n- Personal integrity\n- Credibility as an Expert witness (Professional Indemnity insurance) \n\n### Indemnity Insurance\nYou may make a mistake that result in you being charged by the party harmed with your opinion about the case. If you are a professional in this field, it is almost obligatory to have the indemnity insurance to cover this risk.\n\n## Practical issues during search\nDon't just look for the obvious digital storage devices\nDevices could be disguised/ very small: PC in a cardboard box, credit card camera, watch, USB drive in a barbie\n\n### Future trends\n- Larger Storage devices\n- More Information\n- More (and more) mobile data\n- The Cloud \n- Evolving legal issues – Case Laws\n\n","tags":["Security","Good to know"]},{"title":"Version Control Through and Through","url":"/2020/04/07/Version-Control-Through-and-Through/","content":"This blog is based on course materials by Dr. Gerald Weber. I find it hard to focus on learning these concepts, so I decide drive myself with some output.\n# Dimensions of Version Control\n## VC system\nVersion Control System has two spaces:\n- Product space, for the code files.\n- Meta space, for the version histories.\n### Central Version Control Systems vs. Distributed Version Control Systems: \nDVCS allows work offline and enables different workflows.\n## Best Practices\n### Coding Style\nNeed to decide on a code style among the team. Use style checker if possible/ configure your IDE.\nSide note: [prettier](https://www.npmjs.com/package/prettier) is an opinionated code formatter that enforces a consistent style by parsing your code and re-printing it with its own rules. You can have your own [configurations](https://prettier.io/docs/en/configuration.html).\n### Commits\n#### Single Logical Changes\nSo that changes can be better adopted/cherry picked, and easier to review.\n#### Don't break the build\nOnly commit changes that preserve system integrity. \nAvoid commits that fixes other commits, e.g. \"Add changes missing from previous commit\": **Rework history instead.** \nSee the [fixup](https://stackoverflow.com/questions/3103589/how-can-i-easily-fixup-a-past-commit) commit argument to fix previous commits:\n```\n$ git add ...                           # Stage a fix\n$ git commit --fixup=a0b1c2d3           # Perform the commit to fix broken a0b1c2d3\n$ git rebase -i --autosquash a0b1c2d3~1 # Now merge fixup commit into broken commit\n```\n#### Avoid Regressions\nrun unit tests before committing\nkeep test suite up-to-date\ncommunicate about manual testing\n#### Non critical cleanup\nKeep noncritical cleanup/reformatting separate from functional changes.\n#### Refactorings in single commit\nSimilarly, refactoring does not expect to change functions, but influences a lot of code. Communicate about refactoring so that everyone has the up-to-date version.\n#### Merge global changes early\nAll branches might need to consolidate and prepare for merging the global cleanup/refactoring changes. Merge them early to avoid hard conflicts.\n#### Communicate with other developers\nRegular review and cleanup activities.\nDiscuss and agree on a design.\nCheck if someone else is working on the same part to avoid conflicts.\n#### Fix hostpots for conflicts\nIf some part of the code often cause merge conflicts, it might indicate important design flaw/issue: not decoupled with other modules enough, too much dependencies, etc.\n\n### Merge Commit\n**Merge should only contain changes from the three-way\nmerge**\nA merge commit should only contain changes that fix the commit. Don't add new things while resolving the conflict.\n\n### What files to include in the repository\nUsually commit only source files, not generated files.\nDo include platform independent build instructions in the repository too, but not IDE specific files or local configs.\nNo files that can be generated by the build system.\nRead more about the [.gitignore](https://help.github.com/en/github/using-git/ignoring-files) file. You can set **global** .gitignore file for your machine and keep the project .gitignore file specific to the project itself. Go to [github/gitignore](https://github.com/github/gitignore) and [gitignore.io](https://www.gitignore.io/) to download/generate your .gitignore file.\n\n### Writing a good commit message\nThe seven rules of a great Git commit message, from [this website](https://chris.beams.io/posts/git-commit/)\n1. Separate subject from body with a blank line (**The block line separating the subject and the body is critical: various tools like 'log' and 'rebase' can get confused if you run the two together.**)\n2. Limit the subject line to 50 characters\n3. Capitalize the subject line\n4. Do not end the subject line with a period\n5. Use the imperative mood in the subject line\n   (A properly formed subject line will complete the sentence:\n   If applied, this commit will <u>your subject line here</u>)\n6. Wrap the body at 72 characters\n7. Use the body to explain **what and why** vs. how. The diff will explain the how.\n\nIf you use an issue tracker, put references to them at the bottom, like this:\n```\nResolves: #123\nSee also: #456, #789\n```\n\n### Git Commands\n[git cherry-pick](https://www.atlassian.com/git/tutorials/cherry-pick)\ngit cherry-pick is a powerful command that enables arbitrary Git commits to be picked by reference and appended to the current working HEAD. \n[git rebase](https://www.atlassian.com/git/tutorials/rewriting-history/git-rebase): \nRebase is one of two Git utilities that specializes in integrating changes from one branch onto another. The other change integration utility is git merge . Merge is always a forward moving change record. Alternatively, rebase has powerful history rewriting features.\n[git blame](https://www.atlassian.com/git/tutorials/inspecting-a-repository/git-blame)\nThe high-level function of git blame is the display of author metadata attached to specific committed lines in a file. \nWhen you are interested in finding the origin for lines 40-60 for file foo, you can use the -L option like so (they mean the same thing — both ask for 21 lines starting at line 40):\n```\ngit blame -L 40,60 foo\ngit blame -L 40,+21 foo\n```\n\n### Git Workflows\nSee a great article discussing git workflows [here](https://www.atlassian.com/git/tutorials/comparing-workflows).\n- Centralised workflow\n- Featured branches/ Topic branches. Discussed [here](https://nvie.com/posts/a-successful-git-branching-model/). \n- Forking workflow\n- Gitflow workflow\n\nGuidelines:\n- Short-lived branches\n- Minimize and simplify reverts (it’s beneficial to have a workflow that allows for easy reverts that will not disrupt the flow for other team members.)\n- Match a release schedule\n\n\n# Diffs in Version Control\nIdea: represent change as operations on the file.\n\nDiffs/patches are important for developers:\n   - to review their own changes\n   - to review changes/patches made by other people\n   - to apply the patch to other versions of the code\n   \n## Unified Diff Format\nOperation model:\n- only insert and deletion\n- changes are applied in parallel\nSee [here](https://www.gnu.org/software/diffutils/manual/html_node/Example-Unified.html#Example-Unified) for an example of unified diff format.\n\n<img src=\"UnifiedDiffFormat.jpg\" width=\"500px\" title=\"Unified Diff format\" alt=\"Unified Diff format\">\n\n## Computing Diff\n`diff[v->w]`, or `diff(v,w)` is a diff transforming v into w.\n`udiff[v->w]`, or `udiff(v,w)` is a unified diff transforming v into w.\n\n## Line Based Diffs\nOperate on a string of lines\n- A \"string of lines\" means, that the alphabet is the set of all possible individual lines.\n- The actual diff algorithm is therefore a diff of strings over\n  any character set C.\n### Edit Script \nFinding the smallest edit script between two versions:\n[Largest common subsequence (LCS)](https://medium.com/@marvinraval99/longest-common-subsequence-lcs-using-dynamic-programming-2d77e6d9d683)\n\nThe size of the smallest insert/delet edit script is called the\n[Levenshtein distance](https://web.stanford.edu/class/cs124/lec/med.pdf). \n\n### Edit Graph\nDirected Acyclic Graph (DAG)\n- top left corner: origin\n- bottom right corner: sink\n- every path from origin to sink represents an edit script\n- Vertices:\n    * column labels after 0 given by source string\n    * row labels after 0 given by target string\n- Edges:\n    * vertical downward move: edit: insert column label of\n      endpoint of the edge.\n    * horizontal move right: edit: delete row label of endpoint\n    * diagonal move: keep, no edit, no change; requires matching column/row labels on endpoint. Diagonal move is available when the column label and row label at the endpoint are the same.\n- Paths:\n    * The path to upper right corner, then down to sink: first delete whole source string, then insert whole target string.\n    * every path from origin to bottom right corner represents\n      an edit script.\n      \n### Summary: Computing optimal line-based diffs\nEquivalent to:\n- finding the largest common subsequence (LCS)\n- finding the shortest edit path (D-path)\n- finding the minimum cost path  \nGreedy Principle\n- its enough to search on the furthest reaching D-paths to\nfind optimal solution\n- Runtime O((N + M)D), or with input length n as O(nD)   \n\n## Merge\n### Three-way merge\nTake parent into account, and do the following three steps:\n- For both Dev A and B, do diff relative to parent.\n- apply both patches in parallel\n- if both patches change the same line -> conflict\n\nIf an automatic merge is not possible (because of conflicts),\nconflicts can be solved using a merge tool.\n - [npm-merge-driver](https://www.npmjs.com/package/npm-merge-driver) can be used  for automatic merging of lockfiles - you can install it globally into your machine:\n```\n$ npx npm-merge-driver install --global\n```\n- Fork provides graphical three-way merge tool.\n- [meld](http://meldmerge.org/) is a visual diff and merge tool targeted at developers: haven't found out about it yet.\n\n## Summary\nDiffs are fundamental for dealing with different versions\nof a file.\n- create diffs\n- apply diffs\n- 3 way merge to incorporate concurrent changes of the\nsame file\nDiffs are computed as edit scripts.\n- Our standard diffs are insert/delete scripts on lines.\n- We are usually interested in the smallest edit script; it\nalso identifies the largest common subsequence.\n- Can be computed in O(nD) time, in practice often even\nfaster. n is the length of input (M+N, M and N are length of start and end versions), and D is the number of differences.\n\n<img src=\"MyersDiff.png\" width=\"500px\" title=\"MyersDiff Pseudo Code, page by Dr Gerald Weber\" alt=\"myers diff\">\n\n# Conflicts and Edits\nWe now look at edit scripts with more operations: move, copy, cut/paste, and replaceAll.\n## Understanding different conflicts\n- Textual Conflicts: Edits in the same line, detected by merge tools\n- Semantic Conflicts: logical conflicts (e.g. method signature change)\nBoth textual and semantic conflicts must be resolved by user.\n\n## parallel vs sequential in edit scripts\nThe comma is an operator, requires that for (s,r) the edit scripts s and r can be merged: edit scripts can be executed on the same source.\nFor a general edit script we also define sequential execution of edits, denoted with semicolon: (s;r)\nIn (s;r) the edit r is executed on the result of s:\n```\nd(2,1), i(4,’hi’) = d(2,1); i(3,’hi’)\n```\n\n### Properties of parallel and sequential operators\n- The comma (s,r) is commutative and associative.\n    - `(d(2,1),i(4,'hi')) = (i(4, 'hi'), d(2,1))`\n    - `(d(2,1), i(4,'hi')), d(7,1) = d(2,1)(i(4,'hi'), d(7,1))`\n    - Therefore, we use sorted order wherever possible.\n- The semicolon (s;r) is associative.\n    - `(d(2,1);i(3,'hi'));d(7,1) = d(2,1);(i(3,'hi'); d(7,1)) `\n-We can compose more complex edit scripts.\n    - `(d(2,1), i(3,'hi'),i(5,'.'));d(7,1) = d(2,1), i(3,'hi')\n    \n> **edits that are combined with the parallel comma operator may be undefined: this models a textual conflict.**\n\n## More edit operations for edit scripts\n- Operations beyond insert/delete:\n    - move, copy/cut + paste, replaceAll\n- Important question:\n    - How do merges work with new operations?\nFinding strictly minimal edit script can become more complicated.\n**Operation replaceAll cannot be reconstructed with certainty from change file.**\nBut: With new operations, \n- Some concrete merges may become easier.\n- We may understand semantic conflicts better.\n\n### idxcv scripts\ndelete `d(pos, count)`, insert `i(posB, string)`,\ncut `x(pos, count, X)`, the buffer is X\ncopy `c(pos, count, X)`\npaste `v(posB, X)`\ncut/paste operations can be combined with the parallel operator:\n    `x(a,b,X),v(d,X) = v(d,X),x(a,b,X)`\nIdxcv scripts can be applied to a text:\n`(x(1,1,X), v(6,X))('!hello') = 'hello!'`\nQuestion: What is the result of the following expression?\n```\nx(7,1,X);v(6,X); d(2,1); v(1,X)(’hello u’) =?\n```\n```\n1 2 3 4 5 6 7\nh e l l o _ u\n```\nAfter x(7,1,X);v(6,x), no change really happens:\nNote that posB of x or c starts right at that position, while for v and i the content is added right **behind** the given start position\n```\n1 2 3 4 5 6 7\nh e l l o _ u\nBuffer: u\n```\nAfter d(2,1):\n```\n1 2 3 4 5 6 7\nh l l o _ u \nBuffer: u\n```\nAfter v(1,X):\n```\n1 2 3 4 5 6 7\nh u l l o _ u\nBuffer: u\n```\n\n### Edit scripts and 3-way merge\nWe have said that a 3-way merge combines work done on two branches in parallel.\n- The work in the two branches since the common ancestors can be represented as two idxcv scripts, lets name them p and q. The ancestor s is in our model simply a string.\n- Then the result of the 3-way merge is (p,q)(s).\n- We can now look at p and q and see if there is a conflict.\n- If branch p has three individual commits, then it can be written as p = `(p1; p2; p3)` separated by the semicolon operator. The 3-way merge is now `((p1; p2; p3), q)(s)` and this may help us understand at which stage a possible conflict occurred.\n\n**Question: should the following edit script result in a conflict? If not, what's the result?**\n```\n((x(1,5,X), v(10,X)), (d(2,1); i(1,’u’)))(’hello_hi!_’)\n```\nNo. can be merged without conflict.\n```\n1 2 3 4 5 6 7 8 9 10\nh e l l o _ h i ! _\n```\nAfter `(x(1,5,X), v(10,X))`:\n```\n1 2 3 4 5 6 7 8 9 10\n_ h i ! _ h e l l o \n```\nIn parallel, after `((d(2,1); i(1,’u’)))`:\n```\n1 2 3 4 5 6 7 8 9 10\nh u l l o _ h i ! _\n```\nMerge the above two versions together:\n```\n1 2 3 4 5 6 7 8 9 10\n_ h i ! _ h u l l o \n```\n### Logical positions in a string\n- The above merge works because we adopt the view that each character in the string has an identity.\n- This identity is not affected by change of its index\n- Standard model is **text as an OO(doubly) linked list.**\n- The logical positions that we are interested in our operations are based on a character's context:\n    - its position before a character\n    - its position after a character\n\n### Logical change with fuzzy patching\n#### Fuzzy patching patterns\n**fuzzy patching operators: patterns instead of numbers.**\n`x(’the >’, ’<fox’,X), v(’r the >’, X)( ’the lazy fox jumps\n over the red cat’)` \n = `the fox jumps over the lazy red cat`\n- patterns: ad-hoc(as necessary) identifiers of logical positions:\n    - prefix: pattern for position after a character: ’the >\n    - suffix: pattern for position before a character: ’<fox”\n    - fuzzy patching operators: patterns instead of numbers.\n    - i(), v() have a single pattern as position, d(), x(), c() have\n      two patterns as positions.\n#### How fuzzy patching simplifies rewrites\n- Often no rewriting needed if we switch from parallel to sequential operation:\n`x(’the >’, ’<fox’,X), v(’r the >’, X)`=`x(’the >’, ’<fox’,X); v(’r the >’, X)`\n- But fuzzy patching needs space for a **unique** pattern.\n**\u0001If the pattern is disturbed, we obviously have to repair it, so we need rewrites for such cases.**\n `x(’Hi>’, ’<, H’,X), v(’Jo,>’, X)(’Hi Jo, Hello, great’)`\n =\n `x(’Hi>’, ’<, H’,X); v(’Hi,>’, X)(’Hi Jo, Hello, great’)`\n =\n `'Hi, Jo Hello, great'`\n = `i('Hi,>', ' Jo')('Hi, Hello, great')`\n \n ### POSIX ENF convention\n We might want to identify the ends of the input in the pattern. We agree to use the same signs as in POSIX regular expressions:\n - The `ˆ` matches the start of the string.\n - The `$` matches the end of the string.\n```\n(x(’ˆthe >’, ’<fox j’,X), v(’r the >’, X))( ’the lazy fox and the quick fox jumped over the red cat’)\n```\nThe result is:\n'the fox jumped over the lazy fox and the quick red cat’\n\n### Textual conflict for logical positions\n- Clear case of textual conflict: Two parallel inserts/pastes at the same logical position, which means:\n    - both at the same before-position or at the same after-position.\n    - Reason it is a conflict: because of parallelism, not clear which one goes first.\n    - This conflict case is the equivalent of the textual conflicts in udiff-based diff tools.\n- By contrast, two parallel inserts between the same characters, but one at the before-position and one at the after-position, create no conflict: result is clear.\n    - This case cannot be expressed in udiff tools.   \n    \n### Merge with nested changes\n- If a change in one version fully nests within a cut of a move of a parallel version, there is no conflict.\n`((x(’r the >’, ’<cat’,X); v(’ˆthe >’,X)),d(’pretty >’, ’<red’))(’the fox jumps over the pretty much red cat’)`\n= 'the pretty red fox jumps over the cat'\n- What would be the outcome for ins/del edit script?\n    - an insert of ’the pretty much red fox’ plus a textual conflict at the delete position: Has the word \"much\" been deleted when we insert \"the pretty ... red\" part before the fox?\n- what about d() in parallel with surrounding d()? Isn’t this simply a d()?\n    -Problem: We don’t know whether the surrounding d() is an x() for a later v()! If so, then the insert/delete merge would give the wrong result.\n**Therefore, with the rich operation model, we can perform more automatic merges.**\n#### Nested changes explained with commutativity\n- If a change in one version fully nests within a cut of a move of a parallel version, there is no conflict:\n- If they nest properly, they do not interfere. In other words, they commute. Execution in each order yields the same result.\n- The commutativity expresses the fact that the logical position for the inner change is available before and after the change.\n\n## Summary\n- Logical positions refer to identities of characters in the text.\n- Logical positions can be given with patterns for **fuzzy patching**.\n- Rewriting of operations is often not necessary when switching from comma to semicolon.\n- Textual conflicts can be specified in the extended model.\n- Logical positions explain why more merges are possible in our extended operation model, while the udiff(insert/delete) model might cause conflicts in the same situations.\n\n# Refactorings and merge\n## Refactoring with ra()\nScenario: One developer renames a public function in all appearances, and the other developer, in parallel, makes a call to the function with the old function name. The udiff-based 3-way merge tool does not produce a textual conflict for just these changes, but this conflict may be trapped in typed languages at compile time, or generally at unit test time.\n  \nWe extend the idxcv scripts with a replaceAll operation ra() intended to be unsupervised => idxcvra\n\n> Syntax: `ra(oldstring, newstring)` replaces all occurrences of string oldstring with newstring.\n\n### ra() intended to be unsupervised.\nReplaces all old string with new string\n\n## Merging idxcvra scripts\n### Example 1\n`(ra(’copy’, ’scan’), i(0, ’0. copy attachments, ’))(’1. copy letter, 2. copy invoice’)`\nBecause ra() is unsupervised, safe merge result option is\n’0. scan attachments, 1. scan letter, 2. scan invoice’\n\n`i(0, ’0. copy attachments, ’))` is based on text **before** ra() and should be considered to intend to refer to the function named m before the rename.\n\n**what if the 3-way merge is performed by an i()/d() based merge tool?**\nThe effect of ra() gets translated into two i()/d() operation pairs, for the existing matches only. Overall result: ’0. copy attachments, 1. scan letter, 2. scan invoice’ -> A likely semantic conflict.\n\n### Example 2\n```\u0010\n(（ra('m(', 'p('); i(0, 'm(s){p(\"out: \" + s);}')),\ni('<$', 'm(\"hi\");'))('m(s){println(s);}m(\"hello\");')\n```\nBecause the programmer A writing `i('<$', 'm(\"hi\");'))` would not know about the parallel branch `i(0, 'm(s){p(\"out: \" + s);}'))`, we have to assume that A is not aware of the refactoring and is trying to call the old function `m`.\nTherefore the result is\n```\n('m(s){p(\"out: \" + s);}p(s){println(s);}p(\"hello\");'p(\"hi\");)\n```\n## Summary\n- One classic semantic conflict: conflict between a refactoring (a rename) and a new function call to the function under the old name.\n- If we see replaceAll as a distinct operation ra(), we can avoid the conflict and produce a safe merge result.\n- In determining the merge result we use the semantics of\n  ra() as **unsupervised** replaceAll.\n\n# Next Steps\n- Learn more about greedy algorithms\n- Learn more about git in command lines and in graphical tools.\n\n\n\n \n \n \n\n\n\n\n\u0001\n\n\n\n\n\n\n  \n\n\n\n\n\n\n","tags":["git","basic concepts"]},{"title":"What is RESTful API","url":"/2020/04/07/What-is-RESTful-API/","content":"# What is API\nAPI (Application Programming Interface),\nAPI is the contract/standard provided by one piece of software (A) to another (B), so that B can create a structured request to A and get a structured response.\n# What is RESTful API\nRepresentational State Transfer\nRESTful APIs refers to web services that conform to the REST architectural style.\nREST relies on a **stateless, client-server protocol**, almost always HTTP.\nTreats objects on the server side as **resources that can be created or destroyed**. We can create server object with a POST request, get it with a GET request, or delete it with a DELETE request etc. \n## REST style constraints\n### Stateless\nStateless protocol cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client.\nIn computing, a stateless protocol is a communications protocol in which no session information is retained by the receiver, usually a server. A stateless protocol does not require the server to retain session information or status about each communicating partner for the duration of multiple requests. In contrast, a protocol that requires keeping of the internal state on the server is known as a stateful protocol. A TCP connection-oriented session is a stateful connection because both systems maintain information about the session itself during its life.\nAn example of a stateless protocol is HTTP, meaning that **each request message can be understood in isolation.**\nContrast this with a traditional FTP server that conducts an interactive session with the user. During the session, a user is provided a means to be authenticated and set various variables (working directory, transfer mode), all stored on the server as part of the user's state.\n### Client-server\nSeparation of concerns is the principle behind the client-server constraints. By separating the user interface concerns from the data storage concerns, we improve the portability of the user interface across multiple platforms and improve scalability by simplifying the server components.\n### Cacheable\nIn order to improve network efficiency, cache constraints are added to the REST style.\nCache constraints require that the data within a response to a request be implicitly or explicitly labeled as cacheable or non-cacheable. If a response is cacheable, then a client cache is given the right to reuse that response data for later, equivalent requests.\nThe advantage of adding cache constraints is that they have the potential to partially or completely eliminate some interactions, improving efficiency, scalability, and user-perceived performance by reducing the average latency of a series of interactions.\n### Uniform Interface\nThe central feature that distinguishes the REST architectural style from other network-based styles is its emphasis on a uniform interface between components.\n\n## Related Concepts\n### What is an endpoint\nAn endpoint is a URI/URL where api/service can be accessed by a client application.\n### HTTP methods\nGET POST PUT DELETE\nHEAD OPTIONS(returns the allowed HTTP methods) PATCH\n### Response implements Body\nResponse implements Body, so it also has the following methods available to it:\n- `Body.arrayBuffer()`\n- `Body.blob()`\n- `Body.formData()`\n- `Body.json()`: It returns a promise that resolves with the result of parsing the body text as JSON.\n- `Body.text()`: It returns a promise that resolves with a USVString (text).\n### Response.status\nThe status read-only property of the Response interface contains the status code of the response (e.g., 200 for a success). It is an unsigned short.\nHTTP response status codes indicate whether a specific HTTP request has been successfully completed. Responses are grouped in five classes:\n- Informational responses (100–199),\n- Successful responses (200–299),\n- Redirects (300–399),\n- Client errors (400–499),\n- Server errors (500–599).\nSee this [MDN page](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) to learn more about HTTP response status codes.\nLook at these two api handlers:\n```\nexport default async function login(req, res) {\n  try {\n    await auth0.handleLogin(req, res);\n  } catch (error) {\n    res.status(error.status || 400).end(error.message);\n  }\n}\n```\n```\nexport default async function me(req, res) {\n  try {\n    await auth0.handleProfile(req, res, {});\n  } catch (error) {\n    res.status(error.status || 500).end(error.message);\n  }\n}\n```\nIf the error contains status information, we set the response status to the error status;\nOtherwise, \nIf auth0 did not handleLogin properly, set to 400 Bad Request (The server could not understand the request due to invalid syntax.)\nIf auth0 did not get the user profile properly, set to 500 Internal Server Error (The server has encountered a situation it doesn't know how to handle.)\n\n# Next Steps\nRe-visit the [Promise](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise) object, how to make a function return a promise, and how it resolves.\n> As the Promise.prototype.then() and Promise.prototype.catch() methods return promises, they can be chained.\n\n\n\n\n\n","tags":["Web API"]},{"title":"My Next.js Auth0 implementation","url":"/2020/04/06/My-Next-js-Auth0-implementation/","content":"It has been a one-week challenge for me to create a neat implementation of auth0 authentication in our Next.js project.The @auth0/nextjs-auth0 SDK was published in September 2019, and there have been multiple nice tutorials online that runs through the work from the beginning, but they all happily finish after implementing one Profile page and do not offer further suggestions on good practices when multiple pages need the user authentication information.\nAfter many trial-and-errors, I am now (hopefully) settled with an implementation utilising api endpoints and _app.js.\nHere's a review of what I have done.\n<hr>\n\n# Auth0 Registration\nI first went to auth0.com and signed up for an account, which comes with an auth0 domain.\nI then created a Regular Web App.\nThe fields that are required in the Settings are:\nAllowed Callback URLs, Allowed Web Origins, and Allowed Logout URLs. \n\nFor local development and testing, I set:\n- Allowed Callback URLs: http://localhost:3000/api/callback\n- Allowed Web Origins: http://localhost:3000 \n- Allowed Logout URLs: http://localhost:3000\n\nOnce the application gets deployed, the deployed URLs can be added to corresponding fields, separated with other URLs by a comma(,).\n\nSave the changes to settings, and now scroll up to note down three pieces of basic information:\n- Domain (something.auth0.com),\n- Client ID\n- Client Secret\nThey will be needed for the auth0 configuration.\n\n# nextjs-auth0 Configuration\nThe installation and configuration of @auth0/nextjs-auth0 are fully explained on their [github page](https://github.com/auth0/nextjs-auth0). The only thing to be noted is that, while they put the clientId, clientSecret etc. straight into the /utils/auth0.js file, We should actually put them into the .env file for the security of credentials and for the flexibility among multiple deployments. I also learnt from my team mate that although the .env file is never committed, a .env.example file could be shared through git to present the required .env variables. [dotenv module](https://www.npmjs.com/package/dotenv) is installed to load environment variables from a .env file into process.env.\n\n# Creating API Routes\nI copied and pasted the code from [the @auth0/nextjs-auth0 github page](https://github.com/auth0/nextjs-auth0) for these api endpoints:\n- login, to handle user log in. This will redirect user to auth0.\n- callback, this gets called once user comes back from auth0. It will create a cookie that contains the user session, encrypted with the CookieSecret provided in the configuration step.\n- logout, to handle user log out. This removes the cookie.\n- me, to return the user profile to the client.\n\n# Accessing user info from client\n## Attempt 1: MyApp.getInitialProps\nAccording to this tutorial, the Profile page accesses the user session in the getInitialProps function, which will set the `user` to Profile's `props`:\n```\nProfile.getInitialProps = async ({ req, res }) => {\n  if (typeof window === 'undefined') {\n    const { user } = await auth0.getSession(req);\n    if (!user) {\n      res.writeHead(302, {\n        Location: '/api/login'\n      });\n      res.end();\n      return;\n    }\n\n    return { user }\n  }\n}\n```\nI went to learn more about `getInitialProps` from the [next.js documentation](https://nextjs.org/docs/api-reference/data-fetching/getInitialProps), and learnt that:\n> `getInitialProps` enables server-side rendering in a page and allows you to do initial data population, it means sending the page with the data already populated from the server.\n> `getInitialProps` is an async function that can be added to any page as a static method. \n> `getInitialProps` will disable [Automatic Static Optimization](https://nextjs.org/docs/advanced-features/automatic-static-optimization), which means with the blocking data requirement, Next.js will render the page with `getInitialProps` on-demand, per-request (meaning Server-Side Rendering), instead of pre-rendering the page to static HTML.\n> `getInitialProps` can not be used in children components, *only in the default export of every page*.\n\nTherefore, for the current project implementation, there are two issues that stop me from using `getInitialProps` for each of my pages:\n1. `getInitialProps` could only be called by a page component, and therefore components like `TopBar`, and page components that are exported in wrappers like `withLayout(Map)` could not use `getInitialProps` to fetch the data.\n2. There will be many pages in our website protected by authentication, and including this piece of logic in every pages will not look good (In the end I still included the same logic in each page, but not as naked as this piece).\n\nWith these two problems in mind, my first idea was to make the user property accessible to all pages through customising `_app.js`. I migrated the `Profile.getInitialProps` to `MyApp.getInitialProps`, and pass the user property to the Layout and Component:\n```\nconst { Component, pageProps, user } = this.props;\n    return (\n      <>\n        <Head>\n          <title>TripTime: Time for our next Adventure</title>\n          <meta\n            name='viewport'\n            content='initial-scale=1.0, width=device-width'\n          />\n        </Head>\n        <MainLayout user={user}>\n          <Component {...pageProps} user={user} />\n        </MainLayout>\n      </>\n```\nFor some time I thought this was a perfect solution, until I found out that I had to refresh each page to have the current login status. It looks like when I was browsing through Next.js `Link`s, `MyApp.getInitialProps` does not get invoked again until I refresh the page. So this is how the first attempt failed.\n\n## Attempt 2: React Hooks stuff\nI will not go through this implementation in detail as it is largely based on [this tutorial](https://dev.to/codemochi/how-to-add-auth0-to-nextjs-the-ultimate-guide-4ipn). Up to this moment I don't know what `React.useEffect`, `React.useContext` and `React.useState` do. I'll come back to learn more about that later.\n\n## Attempt 3: componentDidMount\nWhen I was talking about the first attempt with my team mates, they suggested that if I did not pass the `user` object, but instead pass a `getUser` function to be called by components, the user information will be forced to be loaded for each component. Based on this idea, here's the new `\\pages\\_app.js`:\n```\n   import '../css/global.css';\n   import React from 'react';\n   import App from 'next/app';\n   import MainLayout from '../components/layout/MainLayout';\n   import Head from 'next/head';\n   import fetch from 'isomorphic-unfetch';\n   \n   export default class MyApp extends App {\n     setUser() {\n       return async component => {\n         fetch('/api/me')\n           .then(response => (response.ok ? response.json() : null))\n           .then(user =>\n             component.setState(() => {\n               return { user: user };\n             }),\n           );\n       };\n     }\n   \n     render() {\n       const { Component, pageProps } = this.props;\n       return (\n         <>\n           <Head>\n             <title>TripTime: Time for our next Adventure</title>\n             <meta\n               name='viewport'\n               content='initial-scale=1.0, width=device-width'\n             />\n           </Head>\n           <MainLayout setUser={this.setUser()}>\n             <Component {...pageProps} setUser={this.setUser()} />\n           </MainLayout>\n         </>\n       );\n     }\n   }\n```\nAs can be seen, _app.js defines a function `setUser` that returns an `async` function. This `async` function takes in a component, fetches the user information, and sets the user information into the component's state. The `setUser` function is passed on to both `MainLayout` and `Component` as a property.\n\nNow where do I call the `setUser` method?\n\nAccording to [react documentation](), `render` is not the place to call `setState`:\n> The render() function should be pure, meaning that it does not modify component state, it returns the same result each time it’s invoked, and it does not directly interact with the browser.\n\nNot `constructor` either:\n> Typically, in React constructors are only used for two purposes:\n  Initializing local state by assigning an object to this.state.\n  Binding event handler methods to an instance.\n> You should not call setState() in the constructor(). Instead, if your component needs to use local state, assign the initial state to this.state directly in the constructor:\n\nDown on the React Component lifecycle, now I have `componentDidMount`:\n> componentDidMount() is invoked immediately after a component is mounted (inserted into the tree). Initialization that requires DOM nodes should go here. \n> You may call setState() immediately in componentDidMount(). It will trigger an extra rendering, but it will happen before the browser updates the screen. This guarantees that even though the render() will be called twice in this case, the user won’t see the intermediate state.\n\nSo here's my third version of implementation, using `Dashboard` as an example:\n```\n   import React from 'react';\n   import TripSummary from '../components/dashboard/TripSummary';\n   import trip from '../dummy-data/trip';\n   import TripTeamLayout from '../components/layout/TripTeamLayout';\n   import PropTypes from 'prop-types';\n   \n   export default class Dashboard extends React.Component {\n     constructor(props) {\n       super(props);\n       this.state = { user: null };\n     }\n   \n     componentDidMount() {\n       this.props.setUser(this);\n     }\n   \n     render() {\n       return (\n         <TripTeamLayout user={this.state.user}>\n           <TripSummary trip={trip} user={this.state.user} />\n         </TripTeamLayout>\n       );\n     }\n   }\n   \n   Dashboard.propTypes = {\n     setUser: PropTypes.func,\n   };\n```\n\n# My next steps\n- Passing the function around gives me a weird happy thrill. JavaScript is a fun language and I'll learn more about it!\n- What on hell is React Hook and what did the second version do with it remains a mystery to me at this moment. I'll go further into that as soon as possible.\n- The login handler of auth0 seems to provide the option to store state, so that user can come back after they login, instead of being redirected to the profile page all the time. It's an extra nice thing to do when the main features of the website are achieved.\n\n\n**Coding on!**\n\n\n","tags":["Web Development","React","TripTime","Next.js","auth0"]}]